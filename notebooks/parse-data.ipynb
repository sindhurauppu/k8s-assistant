{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9702baf0-4408-4331-8325-9615984e1426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingested 159 documents into ../data_gloss/k8s_docs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "def parse_k8s_doc(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a Kubernetes doc file into a JSON-friendly dict.\n",
    "    YAML front matter is optional.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    metadata = {}\n",
    "    body = content\n",
    "\n",
    "    # Detect YAML front matter\n",
    "    parts = re.split(r\"^---\\s*$\", content, flags=re.MULTILINE)\n",
    "    if len(parts) >= 3:\n",
    "        yaml_block = parts[1]\n",
    "        body = parts[2]\n",
    "        try:\n",
    "            metadata = yaml.safe_load(yaml_block) or {}\n",
    "        except Exception:\n",
    "            metadata = {}\n",
    "\n",
    "    # Clean markdown body (remove Hugo shortcodes like {{< ... >}})\n",
    "    cleaned_body = re.sub(r\"\\{\\{<.*?>\\}\\}\", \"\", body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{%.*?%\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<\\s*/.*?>\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = cleaned_body.strip()\n",
    "\n",
    "    # Build structured JSON\n",
    "    doc_json = {\n",
    "        \"title\": metadata.get(\"title\") or os.path.basename(file_path),\n",
    "        \"reviewers\": metadata.get(\"reviewers\", []),\n",
    "        \"api_metadata\": metadata.get(\"api_metadata\", []),\n",
    "        \"feature\": metadata.get(\"feature\", {}),\n",
    "        \"description\": metadata.get(\"description\", \"\"),\n",
    "        \"content_type\": metadata.get(\"content_type\", \"\"),\n",
    "        \"weight\": metadata.get(\"weight\", \"\"),\n",
    "        \"body\": cleaned_body,\n",
    "        \"source_file\": os.path.basename(file_path),\n",
    "    }\n",
    "\n",
    "    return doc_json\n",
    "\n",
    "\n",
    "def ingest_directory(input_dir: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Ingest all Kubernetes doc files in a directory and write them into one JSON file.\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith((\".md\", \".markdown\", \".txt\")):\n",
    "                try:\n",
    "                    doc = parse_k8s_doc(os.path.join(root, file))\n",
    "                    all_docs.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Skipping {file}: {e}\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_docs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Ingested {len(all_docs)} documents into {output_file}\")\n",
    "\n",
    "\n",
    "def running_in_jupyter() -> bool:\n",
    "    \"\"\"Check if the script is running inside a Jupyter notebook.\"\"\"\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if \"IPKernelApp\" in get_ipython().config:\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if running_in_jupyter():\n",
    "        # Default paths for notebook runs\n",
    "        input_dir = \"../../glossary/\"\n",
    "        output_file = \"../data_gloss/k8s_docs.json\"\n",
    "        ingest_directory(input_dir, output_file)\n",
    "    else:\n",
    "        import argparse\n",
    "\n",
    "        parser = argparse.ArgumentParser(description=\"Ingest Kubernetes docs into JSON\")\n",
    "        parser.add_argument(\"--input_dir\", required=True, help=\"../../glossary/\")\n",
    "        parser.add_argument(\"--output_file\", required=True, help=\"/Users/sindhu/Documents/llm-zoomcamp/KubeRAG/k8s-assistant/data_gloss/\")\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        ingest_directory(args.input_dir, args.output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5d87c35-963e-43ee-8e3e-dbd419eb7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed 159 docs into ../data_gloss/k8s_docs_parsed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure text splitter (approx. ~800 chars per chunk, with overlap for context)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def transform_doc(doc: dict) -> dict:\n",
    "    \"\"\"Clean up one doc: drop unused fields and chunk body.\"\"\"\n",
    "    body = doc.get(\"body\", \"\")\n",
    "\n",
    "    # Clean out Hugo/shortcode tags (defensive, in case not cleaned before)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<.*?>\\}\\}\", \"\", body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{%.*?%\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<\\s*/.*?>\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = cleaned_body.strip()\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = splitter.split_text(cleaned_body) if cleaned_body else []\n",
    "\n",
    "    return {\n",
    "        \"title\": doc.get(\"title\", \"\"),\n",
    "        \"content_type\": doc.get(\"content_type\", \"\"),\n",
    "        \"source_file\": doc.get(\"source_file\", \"\"),\n",
    "        \"weight\": doc.get(\"weight\", \"\"),\n",
    "        \"chunks\": chunks\n",
    "    }\n",
    "\n",
    "def transform_json(input_file: str, output_file: str):\n",
    "    \"\"\"Read JSON, transform all docs, and save new JSON.\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    transformed = [transform_doc(doc) for doc in docs]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transformed, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Transformed {len(transformed)} docs into {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Default paths for notebook runs\n",
    "    input_dir = \"../data_gloss/k8s_docs.json\"\n",
    "    output_file = \"../data_gloss/k8s_docs_parsed.json\"\n",
    "    transform_json(input_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e734b1ca-4fd7-4d77-be07-7d923854e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated JSON saved to ../data_sample/k8s_docs_cleaned2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "input_file = Path(\"../data_sample/k8s_docs_parsed.json\")    # your input JSON file (list of dicts)\n",
    "output_file = Path(\"../data_sample/k8s_docs_cleaned2.json\") # output file\n",
    "\n",
    "# Read the input list of dicts\n",
    "with input_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert 'chunks' from list to string for each dict\n",
    "for item in data:\n",
    "    if \"chunks\" in item and isinstance(item[\"chunks\"], list):\n",
    "        item[\"chunks\"] = \"\\n\\n\".join(item[\"chunks\"])\n",
    "\n",
    "# Write the updated list back as a single JSON array\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Updated JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab67413-5597-401c-b8cb-5e9cc1c0f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "MAX_CHUNK_SIZE = 800  # configurable\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    \"\"\"Replace escaped \\n with real newlines.\"\"\"\n",
    "    text = text.replace(\"\\\\n\\\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\\\n\", \"\\n\")\n",
    "    return text\n",
    "\n",
    "def smart_chunk(text: str, max_size: int = MAX_CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Group text into semantically meaningful chunks:\n",
    "    - Headings, lists, and code blocks stay grouped.\n",
    "    - Wrap long sections if too big.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks, current_chunk = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Start a new chunk if line is a heading\n",
    "        if re.match(r\"^#{1,6}\\s\", stripped) or re.match(r\"^[A-Z].*$\", stripped) and len(stripped.split()) < 10:\n",
    "            if current_chunk:\n",
    "                chunks.append(\"\\n\".join(current_chunk).strip())\n",
    "                current_chunk = []\n",
    "            current_chunk.append(stripped)\n",
    "\n",
    "        # Lists and numbered items stay with their block\n",
    "        elif re.match(r\"^(\\d+\\.|\\-)\\s\", stripped):\n",
    "            current_chunk.append(stripped)\n",
    "\n",
    "        # Code block or indented text\n",
    "        elif stripped.startswith(\"```\") or line.startswith(\"    \") or line.startswith(\"\\t\"):\n",
    "            current_chunk.append(line)\n",
    "\n",
    "        # Normal text\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk).strip())\n",
    "\n",
    "    # Now enforce max_size\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > max_size:\n",
    "            wrapped = textwrap.wrap(chunk, width=max_size, break_long_words=False, break_on_hyphens=False)\n",
    "            final_chunks.extend(wrapped)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def transform_records(input_file: str, output_file: str):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    for record in records:\n",
    "        raw_text = record.get(\"chunks\", \"\")\n",
    "        processed_text = process_text(raw_text)\n",
    "        split_chunks = smart_chunk(processed_text, MAX_CHUNK_SIZE)\n",
    "\n",
    "        record[\"chunks\"] = [\n",
    "            {\"id\": f\"{record['source_file']}_{i}\", \"text\": chunk}\n",
    "            for i, chunk in enumerate(split_chunks)\n",
    "        ]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transform_records(\"../data_sample/k8s_docs_cleaned2.json\", \"../data_sample/k8s_docs_chunked.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8208893-ecb3-4cb1-a993-99a6db181f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_sample/k8s_docs_chunked.json', 'rt') as f_in:\n",
    "    docss = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e75fb1af-d106-476a-8d71-f0cd5b0f2ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e28daf3c-1263-4a51-8ee3-f4aea904f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "i = 0\n",
    "for record in docss:\n",
    "    for chunk in record[\"chunks\"]:\n",
    "        docs.append({\n",
    "            \"id\": i,\n",
    "            \"title\": record[\"title\"],\n",
    "            \"content_type\": record[\"content_type\"],\n",
    "            \"source_file\": record[\"source_file\"],\n",
    "            \"weight\": record[\"weight\"],\n",
    "            \"chunk_id\": chunk[\"id\"],\n",
    "            \"chunk_text\": chunk[\"text\"]\n",
    "        })\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e907ee26-589c-4cd0-965c-e6439291047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10097"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5db72324-54b1-4c5e-b1d2-27c70bd17e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data_sample/k8s_docs_chunked_complete.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67234c-84e3-4e39-a932-ba2fa8bc51d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
