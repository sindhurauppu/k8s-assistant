{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9702baf0-4408-4331-8325-9615984e1426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingested 159 documents into ../data/k8s_docs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "def parse_k8s_doc(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a Kubernetes doc file into a JSON-friendly dict.\n",
    "    YAML front matter is optional.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    metadata = {}\n",
    "    body = content\n",
    "\n",
    "    # Detect YAML front matter\n",
    "    parts = re.split(r\"^---\\s*$\", content, flags=re.MULTILINE)\n",
    "    if len(parts) >= 3:\n",
    "        yaml_block = parts[1]\n",
    "        body = parts[2]\n",
    "        try:\n",
    "            metadata = yaml.safe_load(yaml_block) or {}\n",
    "        except Exception:\n",
    "            metadata = {}\n",
    "\n",
    "    # Clean markdown body (remove Hugo shortcodes like {{< ... >}})\n",
    "    cleaned_body = re.sub(r\"\\{\\{<.*?>\\}\\}\", \"\", body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{%.*?%\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<\\s*/.*?>\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = cleaned_body.strip()\n",
    "\n",
    "    # Build structured JSON\n",
    "    doc_json = {\n",
    "        \"title\": metadata.get(\"title\") or os.path.basename(file_path),\n",
    "        \"text\": cleaned_body,\n",
    "        \"source_file\": os.path.basename(file_path),\n",
    "    }\n",
    "\n",
    "    return doc_json\n",
    "\n",
    "\n",
    "def ingest_directory(input_dir: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Ingest all Kubernetes doc files in a directory and write them into one JSON file.\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith((\".md\", \".markdown\", \".txt\")):\n",
    "                try:\n",
    "                    doc = parse_k8s_doc(os.path.join(root, file))\n",
    "                    all_docs.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Skipping {file}: {e}\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_docs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Ingested {len(all_docs)} documents into {output_file}\")\n",
    "\n",
    "\n",
    "def running_in_jupyter() -> bool:\n",
    "    \"\"\"Check if the script is running inside a Jupyter notebook.\"\"\"\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if \"IPKernelApp\" in get_ipython().config:\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if running_in_jupyter():\n",
    "        # Default paths for notebook runs\n",
    "        input_dir = \"../../glossary/\"\n",
    "        output_file = \"../data/k8s_docs.json\"\n",
    "        ingest_directory(input_dir, output_file)\n",
    "    else:\n",
    "        import argparse\n",
    "\n",
    "        parser = argparse.ArgumentParser(description=\"Ingest Kubernetes docs into JSON\")\n",
    "        parser.add_argument(\"--input_dir\", required=True, help=\"../../glossary/\")\n",
    "        parser.add_argument(\"--output_file\", required=True, help=\"/Users/sindhu/Documents/llm-zoomcamp/KubeRAG/k8s-assistant/data/\")\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        ingest_directory(args.input_dir, args.output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d87c35-963e-43ee-8e3e-dbd419eb7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed 159 docs into ../data/k8s_docs_parsed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure text splitter (approx. ~800 chars per chunk, with overlap for context)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def transform_doc(doc: dict) -> dict:\n",
    "    \"\"\"Clean up one doc: drop unused fields and chunk body.\"\"\"\n",
    "    body = doc.get(\"body\", \"\")\n",
    "\n",
    "    # Clean out Hugo/shortcode tags (defensive, in case not cleaned before)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<.*?>\\}\\}\", \"\", body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{%.*?%\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = re.sub(r\"\\{\\{<\\s*/.*?>\\}\\}\", \"\", cleaned_body)\n",
    "    cleaned_body = cleaned_body.strip()\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = splitter.split_text(cleaned_body) if cleaned_body else []\n",
    "\n",
    "    return {\n",
    "        \"title\": doc.get(\"title\", \"\"),\n",
    "        \"content_type\": doc.get(\"content_type\", \"\"),\n",
    "        \"source_file\": doc.get(\"source_file\", \"\"),\n",
    "        \"weight\": doc.get(\"weight\", \"\"),\n",
    "        \"chunks\": chunks\n",
    "    }\n",
    "\n",
    "def transform_json(input_file: str, output_file: str):\n",
    "    \"\"\"Read JSON, transform all docs, and save new JSON.\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    transformed = [transform_doc(doc) for doc in docs]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transformed, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Transformed {len(transformed)} docs into {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Default paths for notebook runs\n",
    "    input_dir = \"../data/k8s_docs.json\"\n",
    "    output_file = \"../data/k8s_docs_parsed.json\"\n",
    "    transform_json(input_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320b334b-6689-444c-9f60-31e3d63bc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "def parse_markdown_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Extract YAML frontmatter between ---\n",
    "    frontmatter_match = re.search(r\"---\\n(.*?)\\n---\", content, re.DOTALL)\n",
    "    if not frontmatter_match:\n",
    "        return None\n",
    "    frontmatter = yaml.safe_load(frontmatter_match.group(1))\n",
    "\n",
    "    # Extract synopsis section (everything after \"## synopsis\" until next \"##\")\n",
    "    synopsis_match = re.search(\n",
    "        r\"##.*?synopsis.*?\\n(.*?)(?=\\n##|\\Z)\", content, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    synopsis = synopsis_match.group(1).strip() if synopsis_match else \"\"\n",
    "\n",
    "    # Extract examples section (everything after \"## examples\" until next \"##\")\n",
    "    examples_match = re.search(\n",
    "        r\"##.*?examples.*?\\n(.*?)(?=\\n##|\\Z)\", content, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    examples = examples_match.group(1).strip() if examples_match else \"\"\n",
    "\n",
    "    # Combine the useful content\n",
    "    combined_text = synopsis + \"\\n\\n\" + examples if examples else synopsis\n",
    "\n",
    "    return {\n",
    "        \"title\": frontmatter.get(\"title\"),\n",
    "        \"text\": combined_text.strip(),\n",
    "        \"source_file\": os.path.basename(file_path),\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_directory_to_json(input_dir, output_file):\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(input_dir):  # walk recursively\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                doc = parse_markdown_file(file_path)\n",
    "                if doc:\n",
    "                    docs.append(doc)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "convert_directory_to_json(\"../../kubectl/\", \"../data/kubernetes_docs.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466e271-1095-4d0b-9ef6-d94e763a7a1f",
   "metadata": {},
   "source": [
    "Concatenate 2 jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c8db95-c22f-48e1-bd9b-f4fb9bbcf510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def concat_json_files(file1, file2, output_file):\n",
    "    with open(file1, \"r\", encoding=\"utf-8\") as f1, open(file2, \"r\", encoding=\"utf-8\") as f2:\n",
    "        data1 = json.load(f1)\n",
    "        data2 = json.load(f2)\n",
    "\n",
    "    # Concatenate lists\n",
    "    combined = data1 + data2\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(combined, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Example usage:\n",
    "concat_json_files(\"../data/k8s_docs.json\", \"../data/kubernetes_docs.json\", \"../data/kubernetes_docs_combined.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf438f-9481-41d3-b21e-a6fc2e48a578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
