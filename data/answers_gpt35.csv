0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943
Labels in Kubernetes are tags used to attach identifying attributes to objects that are meaningful and relevant to users. They are key/value pairs that help organize objects and select subsets of objects within a Kubernetes cluster.,"In Kubernetes, a label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each. Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app. If --overwrite is true, then existing labels can be overwritten; otherwise, attempting to overwrite a label will result in an error. If --resource-version is specified, then updates will use this resource version; otherwise, the existing resource-version will be used.",Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.,"Labels are meaningful and relevant to users because they are key/value pairs that tag objects with identifying attributes, allowing users to organize and select subsets of objects efficiently.",Labels can be attached to Kubernetes objects such as Pods and Services.,"A check that runs periodically on a container in a pod is a probe. Its purpose is to define the container's state and health, informing the container's lifecycle.","A check that Kubernetes periodically performs against a container, known as a probe, defines the container's state and health, influencing the container's lifecycle.","To learn more, read [container probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).","The primary purpose of a LimitRange in Kubernetes is to constrain resource consumption per container or pod, specified for a particular namespace. It either limits the quantity of resources that can be created for a particular resource type or the amount of resources that may be requested or consumed by individual containers or Pods within a namespace.",A LimitRange either limits the quantity of resources that can be created (for a particular resource type) or the amount of resources that may be requested/consumed by individual containers or Pods within a namespace.,"A LimitRange can either limit the quantity of resources that can be created for a particular resource type, or the amount of resources that may be requested or consumed by individual containers or Pods within a namespace.","A LimitRange either limits the quantity of resources that can be created (for a particular resource type), or the amount of resources that may be requested/consumed by individual containers or Pods within a namespace.","Device classes defined by device drivers and cluster admins in Kubernetes are used to categorize devices within the cluster for use with dynamic resource allocation (DRA). They allow devices to be claimed and used in workloads by creating specific claims. These classes help in requesting and sharing resources among Pods, and Kubernetes allocates matching devices to specific claims, placing the corresponding Pods on nodes that can access the allocated devices.","With dynamic resource allocation (DRA), Kubernetes allows device drivers and cluster admins to define device classes that can be claimed in workloads. Kubernetes allocates matching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices. When a ResourceClaimTemplate is used, Kubernetes automatically creates ResourceClaim objects and binds them to specific Pods. When the Pod terminates, Kubernetes deletes the corresponding ResourceClaim.","To manage resource sharing among Pods, hardware accelerators can be utilized through Dynamic Resource Allocation (DRA). With DRA, device drivers and cluster admins define device classes that can be claimed in workloads. Kubernetes allocates matching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices.","A platform developer can add new functionalities to their Kubernetes instance by using:

- [Custom Resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/): These allow developers to define their own resource types.
- [Extend the Kubernetes API with the aggregation layer](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/): This enables the integration of additional APIs with Kubernetes.

Some platform developers contribute their extensions to the Kubernetes community, while others develop closed-source or site-specific extensions.",Platform developers may develop extensions which are contributed to the Kubernetes community.,"Yes, platform developers can create closed-source commercial or site-specific extensions that are not shared publicly.",A reviewer is a person who reviews code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.,Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.,A reviewer's responsibility is limited to specific parts of the codebase.,Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.,"Reviewers contribute to the software development process by reviewing code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles, ensuring the integrity and quality of the code being contributed.","CustomResourceDefinitions let you extend the Kubernetes API for your environment if the built-in capabilities can't meet your needs. They define a new custom API to add to your Kubernetes API server, without building a complete custom server.",CustomResourceDefinitions let you extend the Kubernetes API for your environment if the built-in functionality can't meet your needs.,"No, CustomResourceDefinitions do not require building a new custom server to extend the Kubernetes API. They allow you to add new custom APIs to your Kubernetes API server without building a complete custom server.","In the Kubernetes Community, the term *upstream* often refers to the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, community members may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.","In the Kubernetes Community, the term *upstream* refers to the core Kubernetes codebase, which other code or third-party tools rely upon. In GitHub or git, *upstream* refers to the source repo from which a repo was forked.","In the Kubernetes community, a community member might want to move a feature upstream to ensure that it becomes part of the core Kubernetes codebase instead of remaining in a plugin or third-party tool. This can increase the feature's utilization and support, as upstream features are maintained as part of the core project and can be relied upon by the broader ecosystem.","In **GitHub** or **git** terminology, the convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","In the Kubernetes Community, when referring to other code or tools relying on the core Kubernetes codebase, they use the term *downstream*. This means the ecosystem, code, or third-party tools that depend on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to enhance their functionality.","A former extension API enabled applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider. It provided a way to list, provision, and bind with external services without needing detailed knowledge about how those services would be created or managed.","A former extension API, known as the Service Catalog, enabled applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider. It provided a way to list, provision, and bind with external services without needing detailed knowledge about how those services would be created or managed.","A Kubernetes cluster could connect with external managed software offerings, such as a datastore service offered by a cloud provider, using the Service Catalog extension API.","Admission controllers are pieces of code that intercept requests to the Kubernetes API server prior to the persistence of the object. They can be ""validating,"" ""mutating,"" or both. Mutating admission controllers may modify the objects they admit, while validating ones may not. Any admission controller has the ability to reject the request.","Mutating admission controllers may modify the objects they admit, while validating admission controllers may not. Both types can intercept requests to the Kubernetes API server and can reject requests.","In Kubernetes, an admission controller can be either ""validating,"" ""mutating,"" or both. Any admission controller may reject a request. Mutating controllers may modify the objects they admit, while validating controllers may not. Thus, an admission controller can indeed modify a request and also has the capability to reject it.",A mutating admission controller has the ability to modify objects before they are stored.,"No, validating admission controllers may not change the objects they process in Kubernetes. They may only reject the request.","Examples of day-to-day operations involved in managing a Kubernetes cluster include:

- Deploying new Nodes to scale the cluster
- Performing software upgrades
- Implementing security controls
- Adding or removing storage
- Configuring cluster networking
- Managing cluster-wide observability
- Responding to events","To scale a cluster by adding resources, you can deploy new nodes to increase the capacity of the cluster. This operation involves adjusting the number of nodes based on your workload demands. Additionally, managing storage and configuring cluster networking are aspects of scaling that help accommodate the increased number of nodes and workloads.

Examples of cluster operations work include deploying new nodes, adding or removing storage, and managing cluster-wide observability.",Examples of activities involved in configuring cluster networking include deploying new Nodes to scale the cluster and configuring cluster networking itself.,"Implementing security controls is important in Kubernetes cluster management because it helps protect the cluster from potential threats and vulnerabilities. Security controls ensure that only authorized users and applications can access resources within the cluster, maintain data integrity, and safeguard the cluster's operation. By using security best practices, you can minimize the risk of unauthorized access, data breaches, and other security incidents that could impact the stability and reliability of the cluster.","Managing cluster-wide observability in Kubernetes involves configuring and monitoring the cluster to ensure that its performance, health, and resource utilization are visible and can be tracked effectively. This includes setting up logging, monitoring, tracing systems, and dashboards to provide insights into the system's operation and to help diagnose any issues that may arise within the cluster. These observability tools assist in maintaining the overall performance and reliability of the Kubernetes environment.","Claims storage in Kubernetes is defined so that the storage can be mounted as a volume in a Pod. It specifies the amount of storage, how the storage will be accessed (read-only, read-write, and/or exclusive), and how it is reclaimed (retained, recycled, or deleted). The details of the storage itself are described in the PersistentVolume object.","A volume in Kubernetes is a directory containing data, accessible to the containers in a Pod. A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.

For more information on storage, you can refer to the Kubernetes [storage documentation](/docs/concepts/storage/).","In Kubernetes, the reclamation of storage that is no longer needed is managed through the Persistent Volume Claim (PVC). Reclamation can be handled in different ways:

- **Retained**: The storage remains in its current state for manual reclamation by an administrator.
- **Recycled**: The storage can be scrubbed clean and made available for new claims.
- **Deleted**: The storage is deleted automatically and cannot be reclaimed in its current form.","To describe the specifics of storage in Kubernetes, you would typically refer to the object's spec, which defines how each object, like Pods or Services, should be configured and its desired state. For objects that involve storage, such as PersistentVolume or PersistentVolumeClaim, the spec would include settings specific to storage configurations.","In Kubernetes, a Persistent Volume Claim (PVC) claims storage defined in a Persistent Volume, so that the storage can be mounted as a volume in a Pod. The PVC specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive), and how it is reclaimed (retained, recycled, or deleted). The storage's details are described in the Persistent Volume object. This allows the volume to become a directory containing data, accessible to the container in a Pod.","Someone can contribute to the Kubernetes project or community by donating code, documentation, or their time. Contributions can include creating pull requests (PRs), reporting issues, providing feedback, participating in community discussions, or organizing community events.","Contributing to the Kubernetes project does not only involve donating code. Contributions include pull requests (PRs), issues, feedback, participation, or organizing community events.","Contributions to the Kubernetes community include donating documentation, time, feedback, participation in events, organizing community events, issues, and participating in SIGs.","Yes, organizing events can be considered a form of contribution to the Kubernetes project. Contributions include participation or organizing community events among other activities.","Someone can provide feedback as a contribution to the Kubernetes project by participating in discussions, reporting issues, or giving feedback on PRs or community events.","An endpoint of a Kubernetes Service is one of the Pods (or external servers) that implements the Service. For Services with selectors, the EndpointSlice controller will automatically create one or more EndpointSlices giving the IP addresses of the selected endpoint Pods. EndpointSlices can also be created manually to indicate the endpoints of Services that have no selector specified.","When a Service has a selector, the EndpointSlice controller automatically creates one or more EndpointSlices that provide the IP addresses of the selected endpoint Pods.",EndpointSlices can be created manually to indicate the endpoints of Services that have no selector specified.,"Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. Operating such a system becomes a lot more straightforward because administrators can make assumptions about it. After all, they know no one made mistakes or changes they forgot to communicate. Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed to create infrastructure is stored in version control (such as Git). This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","Immutable infrastructure helps in identifying and mitigating security risks by preventing or identifying unauthorized changes. Since the infrastructure cannot be changed once deployed, it is easier to detect anomalies and unauthorized modifications. This makes it straightforward for administrators to manage the system because they know no unauthorized changes went unnoticed or uncommunicated, hence reducing potential security vulnerabilities.","Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place. By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks.","Containers are an example of immutable infrastructure because persistent changes to containers can only be made by creating a new version of the container or recreating the existing container from its image. This means that once a container is deployed, it cannot be altered, thereby ensuring that any changes require creating a new version or instance of the container. This immutability helps in maintaining security and predictability in the infrastructure.","Immutable infrastructure and infrastructure as code are closely related as they both emphasize consistency and control in managing infrastructure. Immutable infrastructure involves deploying infrastructure components, such as virtual machines or containers, that cannot be altered once they are deployed. Instead of making changes directly to running systems, updates are implemented by deploying new versions.

Infrastructure as code complements immutable infrastructure by allowing all the automation necessary to create and manage this immutable infrastructure to be stored in version control systems like Git. This approach means that any changes to the infrastructure are documented as code, providing a clear and traceable audit log of changes, enhancing accountability and reducing the risk of unauthorized changes. Together, these practices ensure a high level of consistency and reliability in managing infrastructure.","A ConfigMap in Kubernetes can be consumed in the following ways:

- As environment variables
- As command-line arguments
- As configuration files in a volume","A ConfigMap allows you to decouple environment-specific configuration from your application, so that your applications are easily portable.",An API object used to store non-confidential data in key-value pairs.,Extensions are software components that extend and deeply integrate with Kubernetes to support new types of hardware.,Many Kubernetes users might not need to install extensions themselves because these clusters often come with extensions pre-installed.,"Most Kubernetes users will not need to create their own extensions, as many cluster administrators use a hosted or distribution instance of Kubernetes that comes with extensions pre-installed.",Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.,"If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.",You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).,HostAliases is a mapping between the IP address and hostname to be injected into a Pod's hosts file. This is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. It is only valid for non-hostNetwork Pods.,"HostAliases are applied to a Pod's hosts file if they are specified, and this is only valid for non-hostNetwork Pods.","No, HostAliases is only valid for non-hostNetwork Pods.",[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.,"No, it is not mandatory to specify HostAliases for a Pod. HostAliases is an optional list of hostnames and IP addresses that can be injected into the Pod's hosts file if specified, and it is only valid for non-hostNetwork Pods.",Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.,Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.,"For information on Kubernetes and CNI, see [**Network Plugins**](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).",The primary purpose of a sidecar container in Kubernetes is to provide a Pod-local service to the main app container. Sidecar containers are typically started before any app containers run and continue running after Pod startup.,"Sidecar containers are like regular app containers, but with a different purpose: the sidecar provides a Pod-local service to the main app container. Unlike app containers, sidecar containers continue running after Pod startup.",Read [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) for more information.,"DeviceClasses play a key role in dynamic resource allocation within a Kubernetes cluster by defining a category of devices that can be used with dynamic resource allocation (DRA). Administrators or device owners use DeviceClasses to specify a set of devices that can be claimed and utilized in workloads. These claims filter for specific device parameters within a DeviceClass, allowing Kubernetes to allocate matching devices to specific claims and place the corresponding Pods on nodes that can access the allocated devices.",Administrators or device owners are responsible for defining DeviceClasses in a Kubernetes environment.,Devices are claimed in a workload using DeviceClasses by creating ResourceClaims that filter for specific device parameters in a DeviceClass.,"For more information, see [Dynamic Resource Allocation](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass).","The API server is a component of the Kubernetes that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is kube-apiserver. Kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.",The API server is designed to scale horizontally by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.,"The API server is a component of the Kubernetes that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/). kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.",The API server can be scaled horizontally by deploying multiple instances of `kube-apiserver`. Traffic can be managed by balancing it between these instances.,The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.,"A Kubernetes volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.","A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.",See [storage](/docs/concepts/storage/) for more information.,ResourceClaims in Kubernetes describe the resources that a workload needs and are used in dynamic resource allocation (DRA) to provide Pods with access to a specific resource. ResourceClaims can be created by workload operators or generated by Kubernetes based on a template.,"ResourceClaims facilitate dynamic resource allocation (DRA) for Pods by describing the resources that a workload needs. Kubernetes can either have these ResourceClaims created by workload operators or generate them based on requirements. Through DRA, ResourceClaims provide Pods with access to specific resources, and Kubernetes allocates matching devices to specific claims. This ensures that the corresponding Pods are placed on nodes that can access the allocated devices, thereby sharing resources effectively among Pods.",ResourceClaims can be created by workload operators or generated by Kubernetes.,"Node-pressure eviction is the process by which the kubelet proactively terminates pods to reclaim resources on nodes. The kubelet monitors resources like CPU, memory, disk space, and filesystem inodes on your cluster's nodes. When one or more of these resources reach specific consumption levels, the kubelet can proactively fail one or more pods on the node to reclaim resources and prevent starvation. Node-pressure eviction is not the same as API-initiated eviction.","The kubelet monitors resources like CPU, memory, disk space, and filesystem inodes on your cluster's nodes to trigger node-pressure eviction.","Node-pressure eviction and API-initiated eviction differ in their initiation and purpose:

- **Node-pressure eviction** is initiated by the kubelet when resources like CPU, memory, disk space, or filesystem inodes on a node reach specific consumption levels. The kubelet proactively terminates pods to reclaim resources and prevent resource starvation on the node.

- **API-initiated eviction** is triggered by creating an `Eviction` object via the Eviction API, often through a kube-apiserver client like the `kubectl drain` command. This process respects configured `PodDisruptionBudgets` and `terminationGracePeriodSeconds`, and is primarily used for graceful pod termination based on administrative actions rather than resource pressure.

Node-pressure eviction is automatic and based on resource constraints, whereas API-initiated eviction is a controlled administrative action.","Node-pressure eviction is the action that the kubelet takes when resource consumption reaches critical levels on a node. The kubelet monitors resources such as CPU, memory, disk space, and filesystem inodes. When these resources reach specific consumption levels, the kubelet can proactively fail one or more pods on the node to reclaim resources and prevent starvation. This process is known as node-pressure eviction.","Node-pressure eviction is important for preventing resource starvation in a Kubernetes cluster because it proactively terminates pods to reclaim resources on nodes. The kubelet monitors resources like CPU, memory, disk space, and filesystem inodes on your cluster's nodes. When one or more of these resources reach specific consumption levels, the kubelet can proactively fail one or more pods on the node to reclaim resources and prevent starvation.","Preemption logic in Kubernetes helps a pending Pod to find a suitable node by evicting low priority Pods existing on that Node. If a Pod cannot be scheduled, the scheduler tries to preempt lower priority Pods to make scheduling of the pending Pod possible.","Preemption logic in Kubernetes helps a pending Pod to find a suitable place by evicting low priority Pods existing on that Node.

If a Pod cannot be scheduled, the scheduler tries to preempt lower priority Pods to make scheduling of the pending Pod possible.","If a Pod cannot be scheduled, the scheduler tries to preempt lower priority Pods to make scheduling of the pending Pod possible.","A lightweight and portable executable image that contains software and all of its dependencies.

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling. The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","Containers simplify application deployment across various environments by decoupling applications from the underlying host infrastructure. This makes it easier to deploy applications in different cloud or OS environments and simplifies scaling. The process of bundling applications and their dependencies into a container image, known as containerization, allows for lightweight and portable executable images that ensure consistent and reliable deployments.","The term used for applications running inside containers is ""containerized applications.""",The process of containerization involves bundling applications and their dependencies into a container image.,"Containers are considered lightweight and portable because they are executable images that contain software and all of its dependencies. This allows applications to be decoupled from the underlying host infrastructure, making deployment easier across different cloud or OS environments, and facilitating easier scaling.",Every Kubernetes cluster has at least one worker node.,"In Kubernetes, worker nodes host the components of the application workload. They are responsible for executing the containerized applications, running as Pods on the node. Each worker node is managed by the control plane, which oversees the nodes and the Pods in the cluster. This setup allows the application workloads to be distributed across multiple nodes, providing fault-tolerance and high availability. A worker node can be a virtual machine or a physical machine and contains the necessary local daemons or services to run these workloads.","In Kubernetes, controllers, which are part of the control plane, contribute to fault-tolerance and high availability by:

1. Watching the state of the cluster and making necessary changes to maintain the desired state, ensuring applications run smoothly.
2. Managing replicas to maintain multiple identical instances of a pod, which provides high availability and fault tolerance.
3. Running control loops within the control plane to manage deployments, daemonsets, namespaces, and persistent volumes, which helps in self-healing and efficient load balancing within the cluster.",The aggregation layer lets you install additional Kubernetes-style APIs in your cluster.,"The aggregation layer lets you install additional Kubernetes-style APIs in your cluster. When you've configured the cluster to support additional APIs, you can add `APIService` objects to ""claim"" a URL path in the Kubernetes API.","An `APIService` object is used to ""claim"" a URL path in the Kubernetes API when you have configured the aggregation layer to support additional APIs.",The legacy term 'hosting the' refers to its use as a synonym for hosting the control plane. It is still used by some provisioning tools and managed services for control placement.,The legacy term is still being used by some provisioning tools and managed services to signify hosting with `kubernetes.io/role` and control placement.,"The 'kubernetes.io/role' label is used in conjunction with the legacy term ""Master"" by some provisioning tools and managed services to control placement of resources in a Kubernetes cluster.","A person involved in developing Kubernetes typically undertakes roles such as Platform Developer, Application Developer, Code Contributor, Contributor, and Cluster Operator.","No, contributing code is not the only way a developer can participate in the Kubernetes community. A contributor can donate documentation or their time to help the Kubernetes project or community in various ways, such as providing feedback, participating in organizing community events, or creating and managing issues and pull requests.","Someone who donates code, documentation, or their time to help the Kubernetes project or community. Contributions include pull requests (PRs), issues, feedback, participation, or organizing community events.","To reduce complexity, separate processes are compiled into a single binary and run in a single process.","Running multiple processes as a single process in the control plane component reduces complexity. It simplifies the deployment and management of these components by compiling them into a single binary and running them as a single process. This approach can make it easier to manage lifecycle operations, such as starting, stopping, and monitoring the component.","Running processes as a single binary affects the complexity of Kubernetes components by reducing it. Although each process is logically separate, compiling them into a single binary and running them as a single process simplifies the overall architecture and management of the components.",CRI-O is a tool that lets you use OCI container runtimes with Kubernetes CRI. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running containers and to fetch OCI container images from remote registries.,CRI-O enables Kubernetes to interact with container runtimes by implementing the Kubernetes CRI (Container Runtime Interface) to allow the use of OCI-compliant (Open Container Initiative) container runtimes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running containers and to fetch OCI container images from remote registries.,"Yes, CRI-O can work with container images from remote sources. It allows Kubernetes to fetch OCI container images from remote registries.",The container runtimes must adhere to the Open Container Initiative (OCI) runtime spec for compatibility with CRI-O.,"Someone might choose CRI-O when deploying Kubernetes because it allows Kubernetes to use any OCI-compliant runtime as the container runtime for running containers. CRI-O enables fetching OCI container images from remote registries, providing flexibility and compatibility with existing OCI standards.","To manage sensitive information using Kubernetes Secrets, you can store data such as passwords, OAuth tokens, and SSH keys.",Secret values in Kubernetes are encoded as base64 strings and are stored unencrypted by default.,"A Pod can access a Secret in Kubernetes in a variety of ways:

- By mounting the Secret as a volume.
- By exposing the Secret as environment variables.

These methods allow you to refer to the Secret in your Pod configuration according to your application needs.","Secrets are designed for storing sensitive information such as passwords, OAuth tokens, and SSH keys, while ConfigMaps are used for non-confidential data. Secrets give you more control over how sensitive information is used and help reduce the risk of accidental exposure. Unlike ConfigMaps, Secrets are encoded as base64 strings and can be configured to be encrypted at rest.","To enhance the security of stored Secrets in Kubernetes, configure them to be encrypted at rest. By default, Secret values are stored unencrypted, but you can set up encryption at rest to protect their confidentiality.","The Kubernetes downward API allows containers to consume information about themselves or their context in a Kubernetes cluster. Applications in containers can have access to that information, without the application needing to act as a client of the Kubernetes API.","It is sometimes useful for a container to have information about itself, without needing to make changes to the container code that directly couple it to Kubernetes. The Kubernetes downward API allows containers to consume information about themselves or their context in a Kubernetes cluster. Applications in containers can have access to that information, without the application needing to act as a client of the Kubernetes API.","Kubernetes provides two methods to expose Pod and container field values to containers:

- Using environment variables
- Using a `downwardAPI` volume","The Kubernetes downward API allows containers to consume information about themselves or their context in a Kubernetes cluster. Applications in containers can have access to that information without needing to act as a client of the Kubernetes API. There are two ways to expose Pod and container fields to a running container:

- using environment variables
- using a `downwardAPI` volume

Together, these two ways of exposing Pod and container fields are called the _downward API_.","It is beneficial for applications in containers to access field values without acting as a Kubernetes API client because this allows the applications to have information about themselves or their context in a Kubernetes cluster without needing to make changes to the container code that directly couple it to Kubernetes. This can simplify the application code and reduce dependencies on the Kubernetes API, allowing it to be more portable and easier to manage.",The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and fosters a community around projects that orchestrate containers as part of a microservices architecture. Kubernetes is a CNCF project.,Kubernetes is a CNCF project.,The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and fosters a community around projects that orchestrate containers as part of a microservices architecture. Its mission is to make cloud native computing ubiquitous.,The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).,The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and fosters a community around projects that orchestrate containers as part of a microservices architecture. Kubernetes is a CNCF project.,"A person responsible for the high-level design of an application.

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","An architect ensures the scalability and maintainability of an application by focusing on the high-level design that allows the application to interact effectively with its surrounding components. This includes designing the application to integrate with databases, logging infrastructure, and other microservices in a way that supports scalability and ease of maintenance.","Databases, logging infrastructure, and other microservices are examples of surrounding components an application must interact with.","An effective interaction between an app's implementation and its surrounding components is important because it ensures that the app remains scalable and maintainable. Surrounding components include databases, logging infrastructure, and other microservices that are essential for the app's functionality and performance.","An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","AWS (Amazon Web Services) is currently officially supported by kOps. DigitalOcean, GCE (Google Cloud Engine), and OpenStack are in beta support, and Azure is in alpha support.","`kOps` handles system failures within a Kubernetes cluster using its self-healing capabilities, where everything runs in Auto-Scaling Groups. This ensures high availability and resilience by automatically adjusting the infrastructure to maintain the desired state even if failures occur.",`kOps` uses DNS to identify clusters.,"kOps supports the following operating systems for its Kubernetes installations:

- Amazon Linux
- Debian
- Flatcar
- RHEL
- Rocky
- Ubuntu","`kOps` can set up the cloud infrastructure necessary for a Kubernetes cluster by not only helping you create, destroy, upgrade, and maintain a production-grade, highly available Kubernetes cluster, but also by provisioning the necessary cloud infrastructure. It achieves this through fully automated installation, utilizing DNS to identify clusters, ensuring self-healing by running everything in Auto-Scaling Groups, and supporting multiple operating systems. Additionally, `kOps` can directly provision cloud infrastructure or generate Terraform manifests to do so.","Container environment variables are name=value pairs that provide useful information into containers running in a Kubernetes environment. They provide information that is required by the running containerized applications along with information about important related details. For example, file system details, information about the container itself, and other cluster resources such as service endpoints.","Container environment variables can include information required by running containerized applications, such as file system details, information about the container itself, and other cluster resources like service endpoints.","Container environment variables provide important information into containers, including details about the container itself and other cluster resources such as service endpoints. This allows containerized applications to access necessary context and configuration information without direct integration with Kubernetes' internal APIs.","In the **Kubernetes Community**, *downstream* refers to the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to improve their functionality.","In GitHub or git, the term 'downstream' refers to a forked repository, whereas in the Kubernetes Community, 'downstream' refers to the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase.","In the Kubernetes community, adopting a new feature 'downstream' means that applications, third-party tools, or the ecosystem that rely on the core Kubernetes codebase integrate or use this new feature to improve their functionality.","In GitHub or git, the convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","NetworkPolicies in Kubernetes are a specification of how groups of Pods are allowed to communicate with each other and with other network endpoints. They help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate, and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use selectors to choose Pods and define rules specifying what traffic is allowed to the selected Pods. They are implemented by a supported network plugin provided by a network provider. It's important to note that creating a NetworkPolicy object without a controller to implement it will have no effect.","NetworkPolicies define which Pods can communicate with each other by selecting Pods and defining rules that specify what traffic is allowed to the selected Pods. They help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate, and specifically which port numbers to enforce each policy on. NetworkPolicies are implemented by a supported network plugin provided by a network provider. Creating a NetworkPolicy object without a controller to implement it will have no effect.","To ensure a NetworkPolicy is effective in controlling network traffic, you need:

- A NetworkPolicy object to define the policy.
- A supported network plugin provided by a network provider to implement it.

Without a controller or network plugin to enforce it, a NetworkPolicy object alone will have no effect.",NetworkPolicies are implemented by a supported network plugin provided by a network provider. Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.,NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other and which namespaces are allowed to communicate.,"Pod Priority indicates the importance of a Pod relative to other Pods. It gives the ability to set the scheduling priority of a Pod to be higher or lower than other Pods, which is an important feature for workload management in production clusters.",Pod Priority indicates the importance of a Pod relative to other Pods. It gives the ability to set the scheduling priority of a Pod to be higher or lower than other Pods — an important feature for production clusters workload.,"Pod Priority is considered important for workloads in production clusters because it gives the ability to set the scheduling priority of a Pod to be higher or lower than other Pods. This is an important feature, as it ensures that critical workloads can be prioritized for scheduling, thereby enhancing the reliability and efficiency of production environment operations.",The purpose of allowing a kube-apiserver to proxy resource requests to a different peer API server is to enable requests to be served by the correct API server when a cluster has multiple API servers running different versions of Kubernetes. This feature is known as Mixed Version Proxy (MVP).,"The Mixed Version Proxy (MVP) feature benefits clusters with multiple API servers running different Kubernetes versions by enabling resource requests to be served by the correct API server. This ensures that each request is handled by an API server compatible with the requested resource's version, even when there are version discrepancies among the peer API servers. MVP is disabled by default but can be enabled by using the feature gate named `UnknownVersionInteroperabilityProxy`.","To activate the feature that allows the kube-apiserver to proxy requests, you need to enable the feature gate named `UnknownVersionInteroperabilityProxy` when the kube-apiserver is started.","To use the kube-apiserver proxy feature, the feature gate named `UnknownVersionInteroperabilityProxy` needs to be enabled.","No, the kube-apiserver proxy feature is disabled by default in Kubernetes. It can be activated by enabling the feature gate named `UnknownVersionInteroperabilityProxy` when the API server is started.","A stored instance of software contains a set of software needed to run an application. This includes all dependencies and metadata indicating what executable to run, who built it, and other relevant information.","Software is packaged for storage and execution in a container within Kubernetes using an image. An image is a stored instance that holds a set of software needed to run an application. It is a way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Metadata is included in the image that can indicate what executable to run, who built it, and other information.","A container image includes metadata that can indicate what executable to run, who built it, and other information.",Software packaged as a container can be stored in a container registry before it is pulled to a local system.,"The metadata in a container image can indicate what executable to run, who built it, and other information.","Disruptions are events that lead to one or more Pods going out of service. A disruption has consequences for workload management, such as applications that rely on the affected Pods.

If you, as cluster operator, destroy a Pod that belongs to an application, Kubernetes terms that a _voluntary disruption_. If a Pod goes offline because of a Node failure, or an outage affecting a wider failure zone, Kubernetes terms that an _involuntary disruption_.",A voluntary disruption in Kubernetes occurs when a cluster operator intentionally destroys a Pod that belongs to an application.,An involuntary disruption in Kubernetes occurs when a Pod goes offline due to reasons such as a Node failure or an outage affecting a wider failure zone. These disruptions are unintentional and can also be triggered by unavoidable issues like Nodes running out of resources or by accidental deletions.,"Involuntary disruptions in a Kubernetes cluster can be triggered by unavoidable issues like Nodes running out of resources, or by accidental deletions.",See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.,"Device plugins run on worker nodes and provide Pods with access to infrastructure, such as local hardware, that require vendor-specific initialization or setup steps. Device plugins advertise resources to the kubelet, so that workload Pods can access hardware features that relate to the Node where that Pod is running. You can deploy a device plugin as a DaemonSet, or install the device plugin software directly on each target Node.","Device plugins advertise resources to the Kubernetes system, so that workload Pods can access hardware features that relate to the Node where that Pod is running.",You can deploy a device plugin in Kubernetes as a DaemonSet or install the device plugin software directly on each target Node.,"Device plugins run on worker nodes and provide infrastructure access, such as local hardware, that require vendor-specific initialization or setup steps.",See [Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) for more information.,"The main problem with using a simple hash-based scheme for assigning requests to queues is that any high-intensity flow will crowd out all the low-intensity flows that hash to the same queue. Providing good insulation for a large number of flows requires a large number of queues, which is problematic.","Shuffle-sharding improves insulation between low-intensity and high-intensity request flows by using a more sophisticated method than simple hashing. Instead of placing requests into queues based directly on a hash value, shuffle-sharding involves hashing the flow-identifying characteristics of a request to produce a hash value with many bits. This hash value is then used as a source of entropy to shuffle and deal a subset of available queues (referred to as a ""hand"" of cards).

The request is examined across all the dealt queues, and it is placed into the queue with the shortest length. This approach allows low-intensity flows to have a higher chance of avoiding the queues that high-intensity flows are using, as not all requests are forced into the same queue based solely on a modulo operation. The hand size is an important factor: a modest hand size ensures low-intensity flows can avoid high-intensity flows without excessive overhead, whereas a large hand size might make it harder for low-intensity flows to dodge high-intensity flows due to increased examination cost.","The metaphor used to explain the shuffle-sharding technique is ""dealing a hand from a deck of cards,"" where each queue is a metaphorical card. This technique applies to queue selection by first hashing the flow-identifying characteristics of the request to produce a hash value. This hash value is then used as a source of entropy to shuffle the deck and deal a hand of cards (queues). All the dealt queues are examined, and the request is put into one of the examined queues with the shortest length.","The choice of hand size in the shuffle-sharding technique is important because it balances the trade-offs between insulation and resource cost. With a modest hand size, it is inexpensive to examine all the dealt queues, giving low-intensity flows a good chance to dodge the effects of high-intensity flows. However, with a large hand size, it becomes expensive to examine the dealt queues, and it is more difficult for low-intensity flows to avoid the collective effects of multiple high-intensity flows. Thus, hand size should be chosen judiciously to optimize insulation without incurring high resource costs.","The characteristics of a request typically used in a hash function to determine the queue index in a simple hash-based scheme are the 5-tuple of source and destination address, protocol, and source and destination port.","To define a core object in Kubernetes, the three required properties are key, value, and effect.",Tolerations enable the scheduling of pods on nodes or node groups that have matching configurations. They work together with taints to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a pod. A toleration indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching taints.,"A toleration indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching properties. Therefore, a pod with a specific toleration can be scheduled on a node without a matching property, but it is not guaranteed.",Tolerations enable the scheduling of pods on nodes or node groups that have matching taints. They work together with taints to ensure that pods are not scheduled onto inappropriate nodes. A toleration indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching taints.,"A toleration indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching properties. Therefore, it is not necessary for a pod to be scheduled on a node if its toleration matches the node's properties.","A UID in Kubernetes is a systems-generated string that uniquely identifies objects. Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID, and it is intended to distinguish between historical occurrences of similar entities.",Kubernetes ensures each object has a unique identity by assigning a system-generated string called a UID to every object. This UID is distinct for every object created over the entire lifetime of a Kubernetes cluster and is used to differentiate between historical occurrences of similar entities.,Every object in a Kubernetes cluster must have a distinct UID to uniquely identify objects and distinguish between historical occurrences of similar entities. This ensures that every object created over the whole lifetime of the cluster is uniquely identifiable.,A UID is a Kubernetes systems-generated string used to uniquely identify objects. Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.,A UID is a Kubernetes systems-generated string used to uniquely identify objects. Every object created over the entire lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.,"The two types of pod disruptions mentioned in Kubernetes are:

1. Voluntary disruptions
2. Involuntary disruptions",Application owners or cluster administrators typically initiate voluntary pod disruptions.,"Involuntary pod disruptions in Kubernetes are unintentional and can be triggered by unavoidable issues such as nodes running out of resources or by accidental deletions. Additionally, if a pod goes offline because of a node failure or an outage affecting a wider failure zone, these are considered involuntary disruptions.",The primary function of a Horizontal Pod Autoscaler (HPA) in Kubernetes is to automatically scale the number of replicas based on targeted utilization or custom metric targets.,"HorizontalPodAutoscaler (HPA) is typically used with Deployments, ReplicaSets, or ReplicationControllers. It cannot be applied to objects that cannot be scaled.","A HorizontalPodAutoscaler (HPA) might not be suitable for certain Kubernetes objects because it cannot be applied to objects that cannot be scaled. For example, it is typically used with Deployments, ReplicaSets, or StatefulSets, but cannot be applied to objects that lack scalable properties.","Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) differ mainly in the level of management provided by the cloud provider:

- **IaaS (Infrastructure as a Service):** With IaaS, the cloud provider is responsible for the underlying infrastructure, including servers, storage, and networking. You, as the user, manage everything built on top of that infrastructure, such as operating systems and applications. IaaS typically involves more control over the IT resources compared to PaaS.

- **PaaS (Platform as a Service):** In a PaaS offering, the cloud provider manages not only the infrastructure but also the Kubernetes control plane and the related services such as networking and storage. This reduces the level of control and management you need to perform, allowing you to focus more on application development rather than infrastructure maintenance. 

In summary, IaaS offers more control for managing the infrastructure components, while PaaS abstracts these details, providing a more managed experience.","When offering a managed infrastructure service, the cloud provider is responsible for servers, storage, and networking.","In a managed Kubernetes service, the cloud provider is responsible for overseeing the Kubernetes control plane and the infrastructure they rely on, which includes networking, storage, and possibly other elements such as load balancers.","When offering Kubernetes as a managed service, the cloud provider is responsible for the Kubernetes control plane as well as the infrastructure they rely on, including networking, storage, and possibly other elements such as load balancers. In contrast, with managed infrastructure services, the cloud provider is responsible only for the servers, storage, and networking, while you as the user manage the layers on top, such as running a Kubernetes cluster. This means the provider takes on more responsibility for managing the Kubernetes environment in a managed service compared to a managed infrastructure service.","In a Platform as a Service (PaaS) offering, a cloud provider might also manage load balancers in addition to networking and storage elements.",The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.,Kubernetes resources and records of intent are stored as API objects and can be modified via RESTful calls to the Kubernetes API. Users can interact with the API directly or through tools like `kubectl`.,"Configuring Kubernetes through the API in a declarative way allows users to manage configuration by storing Kubernetes resources and ""records of intent"" as API objects, and modifying them via RESTful calls to the API. This approach enables users to manage the configuration declaratively.",Users can use `kubectl` to interact with the Kubernetes API.,"To extend the core Kubernetes API, you can use CustomResourceDefinitions to define a new custom API, the Operator pattern to link a controller to one or more custom resources, or the aggregation layer to install additional Kubernetes-style APIs in your cluster. Extensions can also be employed to support new types of hardware.","A cluster operator is responsible for configuring, controlling, and monitoring clusters. Their primary responsibility is keeping a cluster up and running, which may involve periodic maintenance activities or upgrades. Additionally, cluster operations work includes tasks such as managing day-to-day operations, coordinating upgrades, deploying new nodes to scale the cluster, performing software upgrades, implementing security controls, adding or removing storage, configuring cluster networking, managing cluster-wide observability, and responding to events.","A cluster operator is a person who configures, controls, and monitors clusters, with their primary responsibility being to keep a cluster up and running, which may involve periodic maintenance activities or upgrades. The Operator pattern, on the other hand, is a system design that links to one or more custom resources. It involves extending Kubernetes by adding controllers to the cluster, which act as a controller and have API access to carry out tasks against a custom resource defined in the control plane.","To ensure a cluster remains operational, a cluster operator might perform the following tasks:

- Deploying new Nodes to scale the cluster
- Performing software upgrades
- Implementing security controls
- Adding or removing storage
- Configuring cluster networking
- Managing cluster-wide observability
- Responding to events","The container orchestration layer in Kubernetes is responsible for exposing the API and interfaces to define, deploy, and manage the lifecycle of containers.",The components of the orchestration layer can be run as traditional operating system services (daemons) or as containers.,"The hosts running these components were historically called ""Masters"".","kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes concept. It maintains network rules on nodes, allowing network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.","kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes networking concept. It maintains network rules on nodes which allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if available, otherwise it forwards the traffic itself.",kube-proxy forwards the traffic itself if the operating system packet filtering layer is unavailable.,kube-proxy maintains network rules on nodes that allow network communication to your Pods from network sessions both inside and outside of your cluster.,"To find more detailed information about kube-proxy's command-line tools, please refer to the Kubernetes documentation or resources related to that specific component.","A means of representing claims to be transferred between two parties.

JWTs can be digitally signed and encrypted. Kubernetes uses JWTs as authentication tokens to verify the identity of entities that want to perform actions in a cluster.",Kubernetes uses JWTs as authentication tokens to verify the identity of entities that want to perform actions in a cluster.,"Yes, JWTs can be both digitally signed and encrypted in Kubernetes.","You can use `kubectl` to create, inspect, update, and delete Kubernetes objects.","In English, `kubectl` is (officially) pronounced /kjuːb/ /kənˈtɹəʊl/ (like ""cube control"").",`kubectl` uses the Kubernetes API to interact with a Kubernetes cluster.,"The sequence of states through which a Pod passes during its lifetime. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the `PodStatus` `phase` field.","The current state of a Pod in Kubernetes is described in the `phase` field of the [PodStatus](https://kubernetes.io/docs/reference/generated/kubernetes-api//#podstatus-v1-core) object. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown.",The high-level description of a Pod's state is summarized in the `phase` field of [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core).,"A Pod's state might be described as 'Unknown' if the state of the Pod cannot be determined. This could occur, for example, if there is an error in communications between the control plane and the node the Pod is running on.","The significance of the PodStatus 'phase' field in Kubernetes is to provide a high-level description of the state of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. This field helps indicate the current lifecycle stage of the Pod during its lifetime.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","A CronJob determines when to execute a task by specifying a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format, similar to a line in a *crontab* file.",A CronJob specifies its schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.,A CronJob in Kubernetes is similar to a line in a crontab file as it specifies a schedule using the cron format.,"To find additional information about how a Job functions within Kubernetes, you can refer to the Kubernetes documentation on Jobs. This documentation provides a comprehensive overview of how Jobs are structured, configured, and managed within a Kubernetes cluster. Alternatively, using the `kubectl explain` command can help you understand the fields and structure of a Job resource:

```bash
kubectl explain jobs
```

For detailed exploration of specific fields within a Job:

```bash
kubectl explain jobs.spec
```

Consult the Kubernetes official documentation for a more in-depth explanation and examples.","Fractional numbers are represented using milli units in Kubernetes. For instance, the number `1.5` is represented as `1500m`.",The representation of the number 1000 using SI suffixes is `1k`.,"The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo, intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta), `E` (exa).",The equivalent of the binary number 2048 using binary-notation suffixes is `2Ki`.,"In Kubernetes, the unit `k` for kilo is intentionally lowercase, which differs from other SI units like `M` for mega and `G` for giga that use uppercase letters.",A person who writes an application that runs in a Kubernetes cluster. An application developer focuses on one part of an application. The scale of their focus may vary significantly in size.,The scale of an application developer's focus in a Kubernetes cluster may vary significantly in size. An application developer focuses on one part of an application.,A person who writes an application that runs in a Kubernetes cluster is an application developer.,"The dockershim is a component of Kubernetes version 1.23 and earlier. Starting with version 1.24, dockershim has been removed from Kubernetes.","Starting with version 1.24, dockershim has been removed from Kubernetes.","For more information about the removal of dockershim, see [Dockershim FAQ](/dockershim).","`sysctl` is a semi-standardized interface for reading or changing the attributes of the running Unix kernel. On Unix-like systems, `sysctl` is both the name of the tool that administrators use to view and modify these settings, and also the system call that the tool uses. Runtimes and network plugins may rely on `sysctl` values being set a certain way.","On Unix-like systems, `sysctl` exists in two forms: it is both the name of the tool that administrators use to view and modify settings, and also the system call that the tool uses.",Runtimes and network plugins may rely on `sysctl` values being set a certain way because `sysctl` is a semi-standardized interface for reading or changing the attributes of the running Unix kernel. The system configurations set by `sysctl` can affect the behavior and performance of these components.,"A Kubernetes worker node may be a VM or a physical machine, depending on the cluster.",The control plane manages the worker nodes and the Pods in the cluster.,"In early Kubernetes versions, nodes were called ""Minions"".","A verb that is used to track changes to an object in Kubernetes as a stream. It is used for the efficient detection of changes. Watches allow efficient detection of changes; for example, a component that needs to know whenever a ConfigMap has changed can use a watch rather than polling.

See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.","A verb that is used to track changes to an object in Kubernetes as a stream. It is used for the efficient detection of changes. Watches allow efficient detection of changes; for example, a system that needs to know whenever a ConfigMap has changed can use a watch rather than polling.","An example of a situation where a watch would be more efficient than polling in Kubernetes is when a component needs to know whenever a ConfigMap has changed. Using a watch allows for the efficient detection of these changes in real-time without the need for continuous polling, which can be resource-intensive.",See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.,"A workload is an application running on Kubernetes. Workload management in Kubernetes involves using various core objects that represent different types or parts of a workload, such as DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects. This management allows Kubernetes to ensure high availability, scalability, fault tolerance, and efficient load balancing for applications by maintaining multiple identical instances of a pod.

For example, a workload that has a web server and a database might run the database in one instance and the web server in another. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes automatically ensures that the specified number of instances are running and adjusts the count as needed. This enables workload scaling, distribution across multiple nodes, and provides self-healing capabilities within a cluster.","The control plane maintains the correct number of Pods running by using mechanisms like ReplicationController and ReplicaSet. These components ensure that a specific number of instances of an application are running at any given time, even if some Pods fail, are deleted manually, or if too many are started by mistake.","ReplicationController is deprecated. See other options such as ReplicaSet or Deployment, which are similar.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod in that class.

A PriorityClass is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from -2147483648 to 1000000000 inclusive. Higher values indicate higher priority.

PriorityClass is important for scheduling Pods because it allows the ability to set scheduling priority of a Pod to be higher and lower than other Pods, which is an important feature for production clusters workload.","A PriorityClass is a non-namespaced object that maps a name to an integer priority used for a Pod. The name is specified in the `metadata.name` field, and the priority value is specified in the `value` field. Priorities range from -2147483648 to 1000000000 inclusive, with higher values indicating higher priority.",The range of integer values for priority is from -2147483648 to 1000000000 inclusive. Higher values indicate higher priority.,"A PriorityClass is a non-namespaced object. This implies that it is not associated with any particular namespace, and applies globally across the entire cluster, affecting Pods irrespective of the namespace they belong to.",The name of a PriorityClass can be specified in the `metadata.name` field within its configuration.,"FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines, which requires root access. However, it is a deprecated interface, and the Storage SIG recommends implementing a newer driver interface that addresses the limitations with FlexVolumes.","FlexVolume driver binaries and dependencies must be installed on host machines, which requires root access.",The Storage SIG suggests implementing a CSI driver if possible since it addresses the limitations with FlexVolumes.,"FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The interface is a newer interface that addresses several problems with FlexVolume.

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a driver if possible since it addresses the limitations with FlexVolumes.","* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)",Ensuring a copy of something running across multiple nodes serves to deploy system daemons such as log collectors and monitoring agents that typically must run on every node.,System daemons like log collectors and monitoring agents are deployed using DaemonSets to ensure that a copy is running across a set of nodes. This deployment method is used because such daemons typically must run on every node in the cluster.,Used to deploy system daemons such as log collectors and monitoring agents that typically must run on every node.,This overloaded term may have different meanings depending on the context.,"The term is described as overloaded in the documentation because ""developer"" may refer to different meanings depending on the context.","To understand a term with multiple meanings in Kubernetes, it's helpful to look at its context and usage. For example, the term ""resource"" can refer to either infrastructure resources like CPU and memory available for workloads or it can describe a Kubernetes object like a pod, service, or config map. Paying attention to the context in which the term is used and reviewing related documentation or concepts can provide clarity on its specific meaning within that context.",Feature gates are a set of keys (opaque string values) that you can use to control which Kubernetes features are enabled in your cluster. You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component. Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component. The Kubernetes documentation lists all current [feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.,Feature gates can be enabled or disabled by using the `--feature-gates` command line flag on each Kubernetes component.,"Feature gates are specific to each Kubernetes component. You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component, and each component lets you enable or disable a set of feature gates that are relevant to that component.",The Kubernetes documentation lists all current [feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.,Feature gates are a set of keys (opaque string values) that you can use to control which Kubernetes features are enabled in your cluster. You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component. Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.,"The [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) is a system design that links a to one or more custom resources. You can extend Kubernetes by adding controllers to your cluster, beyond the built-in controllers that come as part of Kubernetes itself. If a running application acts as a controller and has API access to carry out tasks against a custom resource that's defined in the control plane, that's an example of the Operator pattern.","You can extend Kubernetes by adding controllers to your cluster, beyond the built-in controllers that come as part of Kubernetes itself. This is often done using the operator pattern, where a running application acts as a controller and has API access to carry out tasks against a custom resource that's defined in the control plane.","If a running application acts as a controller and has API access to carry out tasks against a custom resource that's defined in the control plane, that's an example of the Operator pattern.",An application is considered an example of the Operator pattern if it acts as a controller and has API access to carry out tasks against a custom resource that's defined in the control plane.,"In Kubernetes, a controller performs tasks against a custom resource by acting as a control loop that watches the state of a custom resource defined in the control plane and makes or requests changes to move the current cluster state closer to the desired state. This is an example of the Operator pattern, where a running application acts as a controller with API access to the custom resource.","The purpose of using the `kubectl drain` command in a Kubernetes cluster is to safely evict all pods from a node to prepare it for maintenance or removal. The command marks the node as going out of service, and if an eviction request is temporarily rejected, `kubectl drain` retries until all pods are terminated or a configurable timeout is reached.","The `kubectl drain` command is used to mark a node as going out of service. When executed, it evicts all pods from the node. If an eviction request is temporarily rejected, `kubectl drain` retries until all pods are terminated or a configurable timeout is reached.","The `kubectl drain` command is used to mark a node as going out of service. When executed, it evicts all pods from the node. If an eviction request is temporarily rejected, `kubectl drain` retries until all pods are terminated or a configurable timeout is reached.","To go out of service in Kubernetes usually refers to a disruption event. Disruptions are events that lead to one or more pods going out of service. If a node is marked as going out of service, it can affect workload management, with applications relying on the affected pods. 

If the disruption is caused by a voluntary action by the cluster operator, such as manually removing a pod, it is termed a voluntary disruption. If the node goes out of service due to a failure or outage, it is considered an involuntary disruption.",The `kubectl drain` command retries until all pods are terminated or a configurable timeout is reached.,"The context provided does not include information on the specific format of a URL that refers to a Kubernetes object. Therefore, I cannot answer this question based on the given context.","Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.","If you delete an object with a specific name, only one object of a given kind can have that name at a time. However, once you delete the object, you can create a new object with the same name.","Yes, you can create a new object with the same name as a deleted object. Only one object of a given kind can have a given name at a time, but after you delete the object, you can create a new object with the same name.","EndpointSlices track the IP addresses of Pods with matching Services, providing a scalable alternative to Endpoints. They can also be created manually to indicate the endpoints of Services that have no selector specified.",EndpointSlices can be configured manually for Services without selectors specified.,"EndpointSlices track the IP addresses of Pods with matching labels. For Services with selectors, the EndpointSlice controller will automatically create one or more EndpointSlices giving the IP addresses of the selected endpoint Pods. EndpointSlices can also be created manually to indicate the endpoints of Services that have no selector specified.","Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources (for example: Pods, Deployments, Services) and not for cluster-wide resources (for example: StorageClasses, Nodes, PersistentVolumes).","Namespaces are used to organize objects in a Kubernetes cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources (for example, Pods, Deployments, Services) and not for cluster-wide resources (for example, StorageClasses, Nodes, PersistentVolumes).","Namespace-based scoping in Kubernetes applies to namespaced resources such as Pods, Deployments, and Services.","No, namespaces cannot be used to organize cluster-wide resources in Kubernetes. Namespace-based scoping is applicable only for namespaced resources such as Pods, Deployments, and Services. Cluster-wide resources like StorageClasses, Nodes, and PersistentVolumes are not organized by namespaces.","Resource names need to be unique within a namespace because namespaces are used to organize objects in a cluster and enforce resource isolation. While names of resources need to be unique within a namespace, they do not need to be unique across different namespaces. This allows you to have separate sets of resources with the same name in different namespaces, facilitating the management and organization of resources within a Kubernetes cluster.","ResourceSlices represent one or more infrastructure resources that are attached to nodes. Drivers create and manage ResourceSlices in the cluster. They are used for dynamic resource allocation (DRA).

When a ResourceClaim is created, Kubernetes uses ResourceSlices to find nodes that have access to resources that can satisfy the claim. Kubernetes allocates resources to the ResourceClaim and schedules the Pod onto a node that can access the resources.","Kubernetes utilizes ResourceSlices to find nodes that have access to resources that can satisfy a ResourceClaim. When a ResourceClaim is created, Kubernetes allocates resources to the ResourceClaim and schedules the Pod onto a node that can access the resources.","When a ResourceClaim is created in a Kubernetes cluster, Kubernetes uses ResourceSlices to find nodes with resources that can satisfy the claim. It then allocates resources to the ResourceClaim and schedules the Pod onto a node that can access those resources.",Drivers are responsible for creating and managing ResourceSlices in a Kubernetes cluster.,"When a ResourceClaim is created, Kubernetes uses ResourceSlices to find nodes that have access to resources that can satisfy the claim. Kubernetes allocates resources to the ResourceClaim and schedules the Pod onto a node that can access the resources.","Helm charts provide a reproducible way of creating and sharing Kubernetes applications. They are packages of pre-configured Kubernetes configurations that can be managed with the Helm tool. A single chart can be used to deploy a variety of applications, ranging from something simple like a memcached Pod to more complex setups like a full web app stack with HTTP servers, databases, caches, and more.","A single Helm chart can be used to deploy applications of varying complexity. It ranges from deploying something simple, like a memcached Pod, to deploying something complex, like a full web app stack with HTTP servers, databases, caches, and more.","Charts provide a reproducible way of creating and sharing Kubernetes applications. A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","Charts provide a reproducible way of creating and sharing Kubernetes applications. A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","Charts provide a reproducible way of creating and sharing Kubernetes applications. A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","The QoS Class of a Pod is determined at creation time based on its requests and limits settings. Kubernetes can assign one of the following QoS classes to a Pod: `Guaranteed`, `Burstable`, or `BestEffort`. These classes are used to make decisions about Pods scheduling and eviction.",The QoS Class of a Pod is set at creation time based on its requests and limits settings.,"Kubernetes can assign one of the following QoS classes to a Pod: `Guaranteed`, `Burstable`, or `BestEffort`.","Kubernetes uses QoS (Quality of Service) classes to classify Pods within the cluster based on their resource requests and limits. This classification helps in making decisions about scheduling and eviction of Pods. The QoS class of a Pod is determined at creation time, and Kubernetes assigns one of the following classes to a Pod: `Guaranteed`, `Burstable`, or `BestEffort`. These classes help in prioritizing Pods, especially under resource contention within the cluster.",QoS Class (Quality of Service Class) in a Kubernetes cluster influences decisions about Pods scheduling and eviction.,"A PodTemplate in Kubernetes is an API object that defines a template for creating pods. It allows you to specify common metadata (such as labels or a template for the name of a new Pod) and the desired state of a pod. Pod templates are commonly used in workload management, where they are embedded into other objects such as Deployments or StatefulSets to define and manage one or more pods. Although you can create a PodTemplate object directly, it is rarely necessary to do so.","Workload management controllers use Pod templates (embedded into another object, such as a Deployment or StatefulSet) to define and manage one or more Pods.","When there can be multiple Pods based on the same template, these are called replicas. Replicas ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. They are commonly used in Kubernetes to achieve the desired application state and reliability, enabling workload scaling and distribution across multiple nodes in a cluster.","Although you can create a PodTemplate object directly, you rarely need to do so because Pod templates are typically embedded in API definitions for workload management, such as a Deployment or StatefulSet, which automatically handle the management of multiple Pods based on the same template.","In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.","In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. Controllers watch the shared state of your cluster through the Kubernetes API Server. Some controllers also run inside the control plane, providing control loops that are core to Kubernetes' operations.","Examples of controllers that run inside the control plane in Kubernetes include the deployment controller, the daemonset controller, the namespace controller, and the persistent volume controller.","In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.",Control loops are crucial to the operations of Kubernetes because they are responsible for continuously watching the state of the cluster and making or requesting changes as needed to move the current cluster state closer to the desired state. They are essential for maintaining the health and functionality of the cluster by ensuring that the actual state always aligns with the intended configuration.,"Computers provide fundamental hardware facilities as resources in Kubernetes, such as processing power, storage memory, and network. These resources have finite capacity, measured in a unit applicable to that resource (e.g., number of CPUs, bytes of memory, etc.). Kubernetes abstracts these common resources for allocation to workloads and utilizes operating system primitives to manage consumption.","Kubernetes abstracts common resources such as CPU, memory, and GPUs for allocation to workloads and utilizes operating system primitives to manage consumption. Computers provide fundamental hardware facilities like processing power, storage memory, and network, each with finite capacity measured in applicable units (e.g., number of CPUs, bytes of memory). Kubernetes uses these resources to schedule and run containers on nodes efficiently. You can also manage complex resource allocations automatically using dynamic resource allocation.",Kubernetes utilizes operating system primitives to manage resource consumption on nodes.,"Dynamic resource allocation in Kubernetes is a feature that lets you request and share resources among Pods. These resources are often attached like hardware accelerators. With dynamic resource allocation (DRA), device drivers and cluster admins define device classes that are available to claim in workloads. Kubernetes allocates matching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices.",A tool for quickly installing Kubernetes and setting up a secure cluster.,Kubeadm can install both the control plane and the components needed for a secure Kubernetes cluster.,"Kubeadm contributes to the security of a Kubernetes cluster by quickly installing Kubernetes and setting up a secure cluster. Additionally, the use of certificates and secrets enhances security within the cluster. Certificates are cryptographically secure files used to validate access to the Kubernetes cluster, ensuring that applications and clients access the Kubernetes API securely. Secrets store sensitive information such as passwords and tokens, giving more control over how this information is used and reducing the risk of accidental exposure. They can be configured to be encrypted at rest, adding an extra layer of security.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh.","Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh.","Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc. It acts as a layer of infrastructure between a service and the network, integrating microservices, managing traffic flow, enforcing policies, and aggregating telemetry data, typically forming a service mesh without requiring changes to application code.","Istio provides a uniform way to enforce policies and aggregate telemetry data. It acts as a layer of infrastructure between a service and the network, forming a service mesh when combined with service deployments.",containerd is a runtime that runs as a daemon on Linux or Windows.,"containerd is a runtime that runs as a daemon on Linux or Windows. It takes care of fetching and storing container images, executing containers, providing network access, and more.","containerd handles container images by fetching and storing them. It is responsible for the lifecycle of a container image, which includes pulling the image from a container registry, storing it locally, and managing the execution of containers based on that image.",An agent that runs on each node in the cluster is called the kubelet. It makes sure that containers described in the PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.,The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy.,The kubelet receives the PodSpecs through various mechanisms to ensure that the containers described in those PodSpecs are running and healthy.,"No, the kubelet does not manage containers that were not created by Kubernetes.",The kubelet doesn't manage containers which were not created by Kubernetes.,"Garbage collection in Kubernetes cleans up the following types of cluster resources:

- Unused containers and images
- Failed Pods
- Objects owned by the targeted resource
- Completed Jobs
- Resources that have expired or failed.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources.

Kubernetes uses garbage collection to clean up resources like [unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images), [failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection), [objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/), [completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources that have expired or failed.",Kubernetes uses garbage collection to clean up resources like failed Pods. This is part of the mechanisms Kubernetes employs to clean up cluster resources and ensure efficient resource utilization.,"Kubernetes uses garbage collection to clean up completed Jobs. This involves automatically removing Jobs that have finished executing to free up resources in the cluster. Jobs that are no longer needed after completion have their associated Pods and resources cleaned up, helping maintain cluster efficiency and organization.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources. Kubernetes uses garbage collection to clean up resources like objects owned by the targeted resource, among others.

When an object in Kubernetes is deleted, objects that are owned by that object are also subject to garbage collection. Kubernetes keeps track of these relationships using the ownerReferences field, which indicates what resource owns a particular object. If an object is deleted, the garbage collector inspects the ownerReferences to determine if the dependent objects should also be deleted. This ensures that resources do not leave behind unwanted dependencies and helps in cleaning up cluster resources efficiently.","Dynamic provisioning offers the advantage of eliminating the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage by user request.","Dynamic provisioning in Kubernetes allows users to request the automatic creation of storage. This eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage by user request. Dynamic volume provisioning is based on an API object, referring to a provisioner that provisions a volume and the set of parameters to pass to the Volume Plugin.","In dynamic volume provisioning, the API object refers to a StorageClass that provisions a PersistentVolume and the set of parameters to pass to the Volume Plugin.","cAdvisor (Container Advisor) provides container users an understanding of the usage and performance characteristics of their running containers. It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container, it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage, and network statistics. This data is exported by container and machine-wide.","cAdvisor (Container Advisor) is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container, it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage, and network statistics. This data is exported by container and machine-wide.","cAdvisor keeps track of the following specific types of information for containers:

- Resource isolation parameters
- Historical resource usage
- Histograms of complete historical resource usage
- Network statistics

This data is exported by container and machine-wide.",cAdvisor provides data at both the container level and the machine-wide level.,"cAdvisor maintains historical data such as resource isolation parameters, historical resource usage, histograms of complete historical resource usage, and network statistics for each container.","In the Kubernetes API system, objects serve as entities that represent the state of your cluster. They are essentially ""records of intent,"" which the Kubernetes system uses to ensure that the desired state of various parts of the cluster's workloads is maintained. By creating an object, you communicate to Kubernetes what you want your cluster's workload to look like.","An entity in the Kubernetes system, a Kubernetes object is typically a “record of intent.” Once you create the object, the Kubernetes system works constantly to ensure that the item it represents actually exists. By creating an object, you're effectively telling the Kubernetes system what you want that part of your cluster's workload to look like; this is your cluster's desired state.","Kubernetes works constantly to ensure that the item represented by an object actually exists, maintaining the desired state of your cluster.","By creating a Kubernetes object, you're effectively telling the Kubernetes system what you want that part of your cluster's workload to look like; this is your cluster's desired state.","A Kubernetes object is described as a ""record of intent"" because once you create the object, the Kubernetes system works constantly to ensure that the item it represents actually exists. By creating an object, you're effectively telling the Kubernetes system what you want that part of your cluster's workload to look like; this is your cluster's desired state.","If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics. Ephemeral containers have no resource or scheduling guarantees, and you should not use them to run any part of the workload itself.","Ephemeral containers have no resource or scheduling guarantees, and you should not use them to run any part of the workload itself.","Ephemeral containers have no resource or scheduling guarantees, and you should not use them to run any part of the workload itself.","To investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics. Ephemeral containers have no resource or scheduling guarantees, and you should not use them to run any part of the workload itself.","Ephemeral containers are not supported by certain environments or configurations within Kubernetes, but the specific environments are not detailed in the provided context.",A Volume Plugin enables integration of storage within a Kubernetes cluster. It lets you attach and mount storage volumes for use by a Pod. Volume plugins can be _in tree_ or _out of tree_. _In tree_ plugins are part of the Kubernetes code repository and follow its release cycle. _Out of tree_ plugins are developed independently.,In-tree and out-of-tree.,_In tree_ volume plugins are part of the Kubernetes code repository and follow its release cycle. _Out of tree_ plugins are developed independently of the Kubernetes codebase.,"A ResourceClaimTemplate defines a template that Kubernetes uses to create ResourceClaim objects. ResourceClaimTemplates are used in dynamic resource allocation (DRA) to provide per-Pod access to separate, similar resources. When a ResourceClaimTemplate is referenced in a workload specification, Kubernetes automatically creates ResourceClaim objects based on the template. Each ResourceClaim is bound to a specific Pod, and when the Pod terminates, Kubernetes deletes the corresponding ResourceClaim.","In dynamic resource allocation, when a ResourceClaimTemplate is referenced in a workload specification, Kubernetes automatically creates ResourceClaim objects based on the template. Each ResourceClaim is bound to a specific Pod. When the Pod terminates, Kubernetes deletes the corresponding ResourceClaim. This process provides per-Pod access to separate, similar resources.","When the associated Pod terminates in Kubernetes, Kubernetes deletes the corresponding ResourceClaim object.","When a ResourceClaimTemplate is referenced in a workload specification, Kubernetes automatically creates ResourceClaim objects based on the template. Each ResourceClaim is bound to a specific Pod. This ensures that each Pod accesses a separate, similar resource. When the Pod terminates, Kubernetes deletes the corresponding ResourceClaim.","When a ResourceClaimTemplate is referenced in a workload specification, Kubernetes automatically creates ResourceClaim objects based on the template.",A manifest specifies the desired state of an object that Kubernetes will maintain when you apply the manifest.,A Kubernetes API object can be specified in JSON or YAML format.,A single YAML file in Kubernetes can contain multiple manifests.,"Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes objects that are marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.

When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp` and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the control plane, or other components, take the actions defined by the finalizers. After these actions are complete, the controller removes the relevant finalizers from the target object. When the `metadata.finalizers` field is empty, Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control the lifecycle of resources. For example, you can define a finalizer to clean up related resources or infrastructure before the controller deletes the object being finalized.","When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`, and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the control plane, or other components, take the actions defined by the finalizers.","After the finalizers' conditions are met, the controller removes the relevant finalizers from the target object. When the `metadata.finalizers` field is empty, Kubernetes considers the deletion complete and deletes the object.","Finalizers in Kubernetes can be used to manage resource deletion by specifying actions or conditions that must be completed before the actual deletion of an object. When you want to delete an object that has a finalizer specified, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`, and the object enters a terminating state. The deletion is not completed until the actions defined by the finalizers are executed, such as cleaning up related resources or infrastructure. Once these actions are completed, the relevant finalizers are removed from the object's metadata, allowing Kubernetes to finish the deletion process. This ensures that necessary cleanup operations occur before the object is fully removed from the cluster.","When a delete request is accepted for an object with finalizers, Kubernetes returns a `202` status code (HTTP ""Accepted"").","A group of Linux processes with optional isolation, accounting and limits.

cgroup is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network) for a collection of processes.","A cgroup can manage the resource usage of CPU, memory, disk I/O, and network for a collection of processes.","A cgroup differs from individual process management in Linux as it allows for the grouping of processes with optional isolation, accounting, and limits on resource usage (CPU, memory, disk I/O, network). This enables more comprehensive management across a collection of processes, rather than dealing with each process separately. This feature helps in efficiently organizing resources and enforcing policies at a group level.","An API object in Kubernetes manages a replicated application, typically by running Pods with no local state.","Each replica in a Kubernetes cluster is represented by a Pod, and the Pods are distributed among the nodes of a cluster.","For workloads that require local state, consider using a StatefulSet.",Certificates enable applications within a Kubernetes cluster to access the Kubernetes API securely. Certificates validate that clients are allowed to access the API.,"Certificates ensure secure access to the Kubernetes API by cryptographically validating access. They are used to confirm that clients accessing the API are authorized to do so, providing a secure method of authentication and ensuring that sensitive operations can only be performed by authenticated users or applications.",Certificates enable applications within a Kubernetes cluster to access the Kubernetes API securely. Certificates validate that clients are allowed to access the API.,"The main purpose of Kubernetes in the context of containerized applications is to decouple applications from the underlying host infrastructure, making deployment easier in different cloud or OS environments and enabling easier scaling. Kubernetes manages the allocation of resources to workloads and utilizes operating system primitives to manage consumption, ensuring that containerized applications run efficiently and effectively.","To ensure that containerized applications are deployed and managed consistently across different environments, Kubernetes uses several key components and concepts:

1. **Replicas**: Kubernetes uses replicas to ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed. This contributes to efficient load balancing, rolling updates, and self-healing capabilities.

2. **Kubelet**: The kubelet is an agent that runs on each node in the cluster. It takes a set of PodSpecs, provided through various mechanisms, and ensures that the containers described in those PodSpecs are running and healthy. The kubelet's role is to ensure the correct pods are running on the node and does not manage containers not created by Kubernetes, thus maintaining consistency.

3. **StatefulSet**: For applications that require persistent identity and storage, Kubernetes uses StatefulSets. StatefulSets manage the deployment and scaling of a set of pods, providing guarantees about ordering and uniqueness. This is especially useful for stateful applications where pods aren't interchangeable.

These mechanisms allow Kubernetes to abstract the underlying infrastructure and provide a consistent environment for deploying and managing containerized applications across different environments.","A Kubernetes object is an entity in the Kubernetes system, used to represent the state of your cluster. It is typically a “record of intent”—once you create the object, the Kubernetes system works constantly to ensure that the item it represents actually exists. By creating an object, you effectively tell the Kubernetes system what you want that part of your cluster's workload to look like; this is your cluster's desired state.","Kubernetes handles scaling of applications by using replicas. Replicas ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed. This replica management allows for efficient load balancing, rolling updates, and self-healing capabilities in a Kubernetes cluster.","Kubernetes offers several benefits for managing microservices architectures:

- **Scalability**: Kubernetes allows you to easily scale microservices horizontally by adjusting the number of replicas for a given service, ensuring the system can handle varying loads efficiently.

- **High Availability**: Through replication and self-healing, Kubernetes ensures that microservices are highly available, automatically restarting failed instances and distributing them across available nodes to minimize downtime.

- **Fault Tolerance**: By using multiple replicas and distributing them across nodes, Kubernetes can provide fault tolerance, maintaining service availability even when some instances fail.

- **Automated Deployments and Rollbacks**: Kubernetes supports automated deployments with zero downtime and can roll back changes swiftly if issues are detected, making it easier to manage microservices.

- **Resource Optimization**: Kubernetes provides resource management capabilities to allocate CPU, memory, and other resources effectively among microservices, ensuring optimal performance.

- **Service Discovery and Load Balancing**: Kubernetes simplifies service discovery and includes built-in load balancing, enabling microservices to discover each other and distribute traffic efficiently.

- **Consistency and Declarative Configuration**: With Kubernetes, you can define the desired state of your microservices using configuration files, ensuring consistency across environments and simplifying management.

- **Isolation and Security**: Kubernetes provides mechanisms to isolate microservices, control access, and secure communication between them, enhancing the overall security of the architecture.","Working groups are a way of organizing people to accomplish a discrete task. They facilitate the discussion and/or implementation of a short-lived, narrow, or decoupled project for a committee, or cross-SIG effort.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and working groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Working groups in Kubernetes facilitate the discussion and/or implementation of a short-lived, narrow, or decoupled project for a committee or cross-SIG effort. They are a way of organizing people to accomplish a discrete task. In contrast, long-term organizational structures like SIGs are groups who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project, focusing on advancing specific areas and following specific governance guidelines.","For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Logs are the list of events that are logged by applications or the system. 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Logs are crucial in a Kubernetes environment as they help you understand what is happening inside your cluster. They are particularly useful for debugging problems and monitoring cluster activity. Logs provide a record of events that have occurred, which you can analyze to identify and resolve issues efficiently.","Logs are important for monitoring activities in a cluster because they help you understand what is happening inside your cluster. They are particularly useful for debugging problems and monitoring cluster activity. Logs provide a list of events that are logged by the system or application, offering insights into the operational state and behavior of the cluster.","Logs in a Kubernetes cluster include application and systems logs. These logs help you understand what is happening inside your cluster and are particularly useful for debugging problems and monitoring cluster activity. Additionally, Kubernetes generates events that describe state changes or notable occurrences in the cluster, which can also be considered a form of logging.",Logs are the list of events that are logged by an application or system. They can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.,CLAs help resolve legal disputes involving contributed material and intellectual property (IP).,CLAs help resolve legal disputes involving contributed material and intellectual property (IP).,CLAs help resolve legal disputes involving contributed material and intellectual property (IP) in open source projects.,"The infrastructure layer in Kubernetes is responsible for providing and maintaining VMs, networking, security groups, and others.",The infrastructure layer provides and maintains networking among other components such as VMs and security groups. This foundational networking setup is crucial for Kubernetes environments as it allows for the communication paths and routing necessary to connect various Kubernetes resources and services.,The infrastructure layer enhances security within Kubernetes by providing and maintaining networking and security groups.,"A core object related to scheduling in Kubernetes consists of three essential properties: key, value, and effect.","Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node, and a node should only schedule a pod that has the matching tolerations for the configured taints. Taints prevent the scheduling of pods on nodes, whereas tolerations allow pods to be scheduled on nodes that have matching taints, indicating that the pod is allowed (but not required) to be scheduled on those nodes.","Taints are applied to nodes in Kubernetes to prevent the scheduling of pods onto inappropriate nodes. They work in conjunction with tolerations to ensure that a node only schedules pods with matching tolerations for the configured taints. This way, taints help manage node workloads and ensure that only suitable pods are scheduled on nodes based on specific criteria.","A managed service is a software offering maintained by a third-party provider. Some examples of managed services are AWS EC2, Azure SQL Database, and GCP Pub/Sub, but they can be any software offering that can be used by an application.","Some examples of Managed Services provided by third-party providers are AWS EC2, Azure SQL Database, and GCP Pub/Sub. These are software offerings maintained by third-party providers and can be used by applications.",Managed Services can be any software offering that can be used by an application.,"A static pod in Kubernetes is a pod managed directly by the kubelet daemon on a specific node without the API server observing it. Static Pods do not support replication controllers, replica sets, or any other controllers that manage pod lifecycles. They are typically used for cluster components that need to run on every node, such as kube-proxy or kube-controller-manager, and are defined in files located on the node’s file system.","The API server does not directly interact with static pods. Static pods are managed directly by the kubelet daemon on a specific node without the API server observing them. However, when the kubelet finds a static pod in its configuration, it automatically tries to create a Pod object on the Kubernetes API server for it, making the pod visible on the API server, but not controllable from there. For example, removing a mirror pod from the API server will not stop the kubelet daemon from running it.","Static Pods do not support being managed by the API server, as they are managed directly by the kubelet daemon on a specific node without the API server observing them.","A StatefulSet maintains a unique identity for each of its Pods through persistent identifiers. Each Pod retains this identifier even if it is rescheduled, ensuring that the Pods are not interchangeable and have a consistent identity across rescheduling events. This approach also facilitates the matching of existing storage volumes to the new Pods that replace any failed ones.","To use storage volumes to provide persistence for your workload, a StatefulSet can be part of the solution because it maintains a sticky identity for each of its Pods, making it easier to match existing volumes to the new Pods that replace any that have failed.","A StatefulSet manages Pods based on an identical container spec but provides guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. The Pods in a StatefulSet have persistent identifiers, which allows them to maintain their identity across rescheduling. In contrast, Deployments manage replicated applications by running Pods with no local state and the Pods are interchangeable, without maintaining persistent identifiers.","StatefulSet is preferred when a sticky identity for Pods is required because it maintains a persistent identifier for each Pod across any rescheduling. This ensures that each Pod is not interchangeable and retains its identity, which is crucial for workloads that need to persist data across Pod restarts and replacements. This feature makes StatefulSets ideal for applications that require stable network identifiers or persistent storage volumes to maintain consistency in the face of failures.","The primary function of a ResourceQuota in a Kubernetes namespace is to constrain aggregate resource consumption within that namespace. It can limit the quantity of specific types of resources that can be created in the namespace, or it can set a limit on the total amount of resources that may be consumed on behalf of the namespace and the objects within it.",A ResourceQuota can limit the quantity of resources that can be created in a namespace by type. It constrains aggregate resource consumption on behalf of the namespace and the objects within it.,"A ResourceQuota can manage overall resource consumption in a namespace by setting a limit on the total amount of resources that may be consumed within that namespace. It does this either by limiting the quantity of resources that can be created in a namespace by type, or by setting a cap on the total amount of resources that may be consumed on behalf of the namespace and the objects within it.","User namespaces are a Linux kernel feature that allows a non-root user to emulate superuser (""root"") privileges, for example in order to run containers without being a superuser outside the container. User namespace is effective for mitigating damage of potential container break-out attacks. In the context of user namespaces, the namespace is a Linux kernel feature, and not in the Kubernetes sense of the term.","User namespaces help enhance container security by allowing a non-root user to emulate superuser (""root"") privileges within the container, while not having those superuser privileges outside the container. This feature mitigates the damage from potential container break-out attacks by lowering the chances that a compromised container can affect the host system or other containers.","User namespaces in Linux are a kernel feature that allows a non-root user to emulate superuser (""root"") privileges, enabling the running of containers without needing superuser access outside the container. This feature is primarily used for security, mitigating potential container break-out attacks. In contrast, namespaces in Kubernetes are an abstraction used to organize objects in a cluster, providing isolation for groups of resources. They allow for the division of cluster resources, with unique resource names required only within a namespace. These namespaces are specific to Kubernetes and are unrelated to the Linux user namespaces.","User namespaces are a Linux kernel feature that allows a non-root user to emulate superuser (""root"") privileges, for example, in order to run containers without being a superuser outside the container. This is effective for mitigating damage from potential container break-out attacks.","User namespaces mitigate the risks of potential container break-out attacks by allowing a non-root user to emulate superuser (""root"") privileges. This means that even if a container break-out occurs, the attacker does not gain root access on the host system, thus reducing the potential damage.","Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins.

Unlike app containers, init containers do not remain running after Pod startup.","Init containers differ in execution order compared to regular app containers because they must run to completion before any app containers can start. They run in series, meaning each init container must complete before the next one begins. In contrast, app containers are started after any init containers have completed, and they do not require completion of one before starting another.",Init containers do not remain running after Pod startup.,"For more information, read [init containers](/docs/concepts/workloads/pods/init-containers/).","Gateway API provides a family of extensible, role-oriented, protocol-aware API kinds for modeling service networking in Kubernetes.","Gateway API provides a family of extensible, role-oriented, protocol-aware API kinds specifically for modeling service networking in Kubernetes, distinguishing it from other API kinds that may not focus on these particular aspects of networking.","Gateway API provides a family of extensible, role-oriented, protocol-aware API kinds for modeling service networking in Kubernetes.","Gateway API provides a family of extensible, role-oriented, protocol-aware API kinds for modeling service networking in Kubernetes.","Gateway API is described as extensible in that it provides a family of extensible, role-oriented, protocol-aware API kinds for modeling service networking in Kubernetes.","Events in a Kubernetes cluster describe state changes or notable occurrences. They have a limited retention time and provide informative, best-effort, supplemental data. However, triggers and messages may evolve over time, so event consumers should not rely on the timing of an event with a given reason to reflect a consistent underlying trigger, nor should they rely on the continued existence of events with that reason. Events are separate from audit records, which are generated by the [auditing process](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/).","Event consumers should not depend on the timing or consistency of events because events have a limited retention time, and the triggers and messages associated with them may evolve over time. Events should be treated as informative, best-effort, supplemental data, and not as a consistent or reliable source for action.","Kubernetes auditing generates a different kind of Event record in the API group `audit.k8s.io`, compared to regular events. Regular events describe state changes or notable occurrences in the cluster and should be treated as informative, best-effort, supplemental data. They have a limited retention time and their triggers and messages may evolve over time.","Events should be treated as informative, best-effort, supplemental data. They describe state changes or notable occurrences in the cluster and have a limited retention time. Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger or the continued existence of events with that reason.","Events in Kubernetes have a limited retention time. Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger or the continued existence of events with that reason. Events should be treated as informative, best-effort, supplemental data.","Annotations in Kubernetes use key-value pairs to attach arbitrary non-identifying metadata to objects. This metadata can be small or large, structured or unstructured, and can include characters that are not permitted in labels. Clients, such as tools and libraries, can retrieve this metadata.","Annotations in Kubernetes can include any arbitrary non-identifying metadata. This metadata can be small or large, structured or unstructured, and can include characters not permitted by labels.",Clients such as tools and libraries can retrieve the metadata stored in Kubernetes annotations.,"The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.","Yes, characters that are usually not permitted can be included in the metadata of Kubernetes annotations. An annotation is a key-value pair used to attach arbitrary non-identifying metadata to objects, and this metadata can be small or large, structured or unstructured, including characters not normally permitted.","API-initiated eviction is the process by which you use the [Eviction API](/docs/reference/generated/kubernetes-api//#create-eviction-pod-v1-core) to create an `Eviction` object that triggers graceful pod termination.

You can request eviction either by directly calling the Eviction API using a client of the kube-apiserver, like the `kubectl drain` command. When an `Eviction` object is created, the API server terminates the Pod.

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/) and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).","To request an eviction using the kube-apiserver, you can directly call the Eviction API. This can be done using a client of the kube-apiserver, such as the `kubectl drain` command.",API-initiated evictions respect your configured [`PodDisruptionBudgets`](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [`terminationGracePeriodSeconds`](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).,"API-initiated eviction is the process by which you use the Eviction API to create an `Eviction` object that triggers graceful pod termination. This type of eviction respects your configured `PodDisruptionBudgets` and `terminationGracePeriodSeconds`. On the other hand, node-pressure eviction is the process where the kubelet proactively terminates pods to reclaim resources on nodes when resources like CPU, memory, disk space, and filesystem inodes reach specific consumption levels. Node-pressure eviction ensures resource availability and prevents starvation. Importantly, API-initiated eviction and node-pressure eviction are not the same.","A method for exposing a network application that is running as one or more Pods in your cluster.

The set of Pods targeted by a Service is (usually) determined by a selector. If more Pods are added or removed, the set of Pods matching the selector will change. The Service makes sure that network traffic can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","A Service determines which Pods to target using a selector. If more Pods are added or removed, the set of Pods matching the selector will change, ensuring that network traffic can be directed to the current set of Pods for the workload.","The set of Pods targeted by a Service is (usually) determined by a selector. If more Pods are added or removed, the set of Pods matching the selector will change. The Service makes sure that network traffic can be directed to the current set of Pods for the workload.","Kubernetes Services can use IP networking (IPv4, IPv6, or both) or reference an external name in the Domain Name System (DNS).","The Service abstraction enables other mechanisms, such as Ingress and Gateway.","The `securityContext` field defines privilege and access control settings for a Pod or container.

In a `securityContext`, you can define: the user that processes run as, the group that processes run as, and privilege settings. You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","In a `securityContext`, you can define: the user that processes run as, the group that processes run as, and privilege settings. You can also configure security policies (for example: SELinux, AppArmor or seccomp).",The `PodSpec.securityContext` setting applies to all containers in a Pod by defining privilege and access control settings for those containers.,"In a `securityContext`, you can configure security policies such as SELinux, AppArmor, or seccomp.","Yes, in a `securityContext`, you can define the user that processes run as and the group that processes run as.",Eviction is the process of terminating one or more Pods on Nodes.,"There are two kinds of eviction in Kubernetes:

* Node-pressure eviction
* API-initiated eviction","There are two kinds of eviction mentioned in the Kubernetes documentation:

- [Node-pressure eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/)
- [API-initiated eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/)",Minikube is a tool for running Kubernetes locally. It runs an all-in-one or a multi-node local Kubernetes cluster inside a VM on your computer. You can use Minikube to try Kubernetes in a learning environment.,Minikube runs an all-in-one or a multi-node local Kubernetes cluster inside a VM on your computer. You can use Minikube to try Kubernetes in a learning environment.,"To try Kubernetes in a learning environment, you can use Minikube.",The two nested object fields that govern the configuration of a Kubernetes object are the object spec and the object status.,"When configuring the 'spec' field of a Kubernetes object, you must provide a description of the characteristics you want the object to have, which defines its desired state. This includes settings such as containers, volumes, replicas, ports, and other specifications unique to each object type. The 'spec' field encapsulates what state Kubernetes should maintain for the defined object.","The 'spec' field in Kubernetes differs among various objects like Pods, StatefulSets, and Services as it details their desired state and configuration specifics:

- **Pods**: The spec for a Pod includes settings like containers, volumes, ports, and other configurations needed to define what state Kubernetes should maintain for the Pod.

- **StatefulSets**: For StatefulSets, the spec also manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It maintains a sticky identity for each Pod with persistent identifiers, making it easier to manage their state and persistence.

- **Services**: The spec for a Service would define details such as the ports and selectors needed to configure the load balancing and networking for a set of Pods. The spec dictates how the Service routes traffic to the Pods and how it should expose them.","The `spec` field of a Kubernetes object includes settings such as containers, volumes, replicas, ports, and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined object.","Kubernetes uses the 'spec' field to define the desired state of an object. When you create an object, you set the spec to describe the characteristics you want that object to have, such as containers, volumes, replicas, or ports. The Kubernetes system continuously works to ensure that the actual state of the object matches the desired state defined in the spec. This involves maintaining, managing, and reconciling the object's state to align with what is specified.","A cluster architect designs infrastructure that involves one or more Kubernetes clusters. They are concerned with best practices for distributed systems, such as high availability and security.","Cluster architects are concerned with best practices for distributed systems, such as high availability and security.","Cluster architects are concerned with best practices for distributed systems, including high availability. This involves designing infrastructure that ensures continuous service without interruptions by preventing single points of failure and ensuring redundancy in the system.","Cluster architects ensure security within Kubernetes clusters by following best practices for distributed systems, which include implementing high availability and robust security measures.","Multiple Kubernetes clusters might be deployed in a single infrastructure design to achieve high availability and security, which are best practices for distributed systems. Cluster architects design such infrastructures to ensure reliability, scalability, and fault tolerance across different environments or geographical regions.","A core function of Kubernetes in relation to container management is the execution and lifecycle management of containers within the Kubernetes environment, facilitated by the container runtime.","Kubernetes supports container runtimes that are implementations of the Kubernetes CRI (Container Runtime Interface), such as CRI-O, which allows the use of OCI (Open Container Initiative) compliant runtimes.",Kubernetes relies on the Container Runtime Interface (CRI) for container runtime implementations.,"A Role in Kubernetes defines permission rules in a specific namespace, while a ClusterRole defines permission rules cluster-wide.","A RoleBinding is scoped to a specific namespace and grants the permissions defined in a Role to a set of users within that namespace. In contrast, a ClusterRoleBinding is scoped cluster-wide and grants the permissions defined in a ClusterRole to a set of users throughout the entire cluster.","RBAC utilizes four kinds of Kubernetes objects:

- **Role**: Defines permission rules in a specific namespace.
- **ClusterRole**: Defines permission rules cluster-wide.
- **RoleBinding**: Grants the permissions defined in a role to a set of users in a specific namespace.
- **ClusterRoleBinding**: Grants the permissions defined in a role to a set of users cluster-wide.","No, a Role cannot be used to define permissions that apply across an entire Kubernetes cluster. A Role defines permission rules in a specific namespace. To define permissions cluster-wide, you need to use a ClusterRole.","Administrators can dynamically configure access policies in Kubernetes using RBAC (Role-Based Access Control). RBAC allows configuring authorization decisions through roles and bindings as follows:

- **Role**: Defines permission rules in a specific namespace.
- **ClusterRole**: Defines permission rules cluster-wide.
- **RoleBinding**: Grants the permissions defined in a role to a set of users in a specific namespace.
- **ClusterRoleBinding**: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods. There are two kinds of affinity:

* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes scheduler, and they can be specified as either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, there are two kinds of affinity:
* Node affinity
* Pod-to-pod affinity","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods. There are two kinds of affinity:

- **Node affinity**: It refers to the rules that impact which nodes are eligible to have the pod scheduled based on the labels of nodes.

- **Pod-to-pod affinity**: It refers to the rules that require certain pods to be scheduled to the same or different nodes based on other pods' labels.

The rules are defined using Kubernetes constructs and can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","The rules are defined using the Kubernetes and specified in, and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, affinity rules are a set of rules that give hints to the scheduler about where to place pods. There are two kinds of affinity:

- Node affinity
- Pod-to-pod affinity

The rules are defined using the Kubernetes API and specified in the pod specification. They can be either required or preferred, depending on how strictly you want the scheduler to enforce them. The scheduler takes these affinity and anti-affinity specifications into account when making scheduling decisions, alongside factors like individual and collective resource requirements, hardware/software/policy constraints, data locality, inter-workload interference, and deadlines.","A former Kubernetes API that enforced security restrictions during creation and updates.

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25. As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.",PodSecurityPolicy was removed in Kubernetes v1.25.,"As an alternative to PodSecurityPolicy, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a third-party admission plugin.",PodSecurityPolicy was deprecated as of Kubernetes v1.21.,"Pod Security Admission is an alternative to the deprecated PodSecurityPolicy, used in Kubernetes to enforce security constraints on pods during their creation and updates. It ensures that pods comply with defined security standards, thus enhancing the cluster's security posture. It is recommended for use in place of PodSecurityPolicy, which was removed in Kubernetes v1.25.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an application owner to create an object for a replicated application, ensuring that a certain number or percentage of Pods with an assigned label will not be voluntarily evicted at any point in time.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an application owner to create an object for a replicated application, that ensures a certain number or percentage of Pods with an assigned label will not be voluntarily evicted at any point in time.","No, Pod Disruption Budgets (PDBs) cannot prevent involuntary disruptions to pods. However, involuntary disruptions do count against the budget.","Involuntary disruptions cannot be prevented by Pod Disruption Budgets (PDBs); however, they do count against the budget.","A ReplicaSet (aims to) maintain a set of replica Pods running at any given time. Workload objects such as deployments make use of ReplicaSets to ensure that the configured number of replicas are running in your cluster, based on the spec of that ReplicaSet.","Workload objects make use of ReplicaSets to ensure that the configured number of replicas are running in your cluster, based on the spec of that ReplicaSet.",A ReplicaSet aims to maintain a set of replica Pods running at any given time.,"The smallest and simplest Kubernetes object. A Pod represents a set of running on your cluster.

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a .",A Pod is typically set up to run a single primary container.,"Sidecar containers can add supplementary features to a Pod, such as logging. They provide a Pod-local service to the main app container and continue running after Pod startup.",Pods are commonly managed by a Kubernetes controller.,"To ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. Replicas are commonly used in Kubernetes to achieve the desired application state and reliability, enabling workload scaling and distribution across multiple nodes in a cluster. Replica management allows for efficient load balancing, rolling updates, and self-healing capabilities in a Kubernetes cluster.","Defining the number of replicas in a Deployment or ReplicaSet affects a Kubernetes cluster by ensuring that the specified number of instances are running. This is crucial for achieving high availability, scalability, and fault tolerance. Kubernetes automatically adjusts the count of replicas as needed, which helps in workload scaling and distribution across multiple nodes in the cluster. Replica management facilitates efficient load balancing, rolling updates, and self-healing capabilities within the cluster.","Replicas ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. They enable workload scaling and distribution across multiple nodes in a cluster. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed. Replica management allows for efficient load balancing, rolling updates, and self-healing capabilities in a Kubernetes cluster.","Replicas contribute to load balancing in Kubernetes by ensuring that multiple identical instances of a pod are distributed across different nodes in the cluster. This distribution allows for efficient load balancing, as incoming network traffic can be spread evenly among the replica pods, preventing any single pod from becoming a bottleneck.

Regarding the update process, replicas enable rolling updates. Kubernetes can update the application gradually by replacing instances one at a time, maintaining the desired number of replicas running while deploying new versions. This approach minimizes downtime and ensures high availability during updates.

Overall, replicas enhance fault tolerance, scalability, and seamless update processes within a Kubernetes cluster.","Kubernetes handles a situation where the number of running instances differs from the specified number of replicas by automatically adjusting the count to match the desired state. This is managed by Kubernetes through Deployments or ReplicaSets, which ensure that the specified number of instances are running. The control plane monitors the state and will create new instances or remove excess instances as needed to maintain the defined number of replicas, providing self-healing capabilities and efficient load balancing.","An API resource in the Kubernetes type system represents an entity corresponding to an endpoint on the API server. It typically defines the schema for the objects or operations on that resource. Some resources represent an operation on other objects, such as a permission check. Each resource is essentially an HTTP endpoint (URI) on the Kubernetes API server.","An operation on objects is represented by a Kubernetes API resource. Some resources themselves represent operations on other objects, such as a permission check. Each resource defines the schema for the objects or operations via an HTTP endpoint (URI) on the Kubernetes API server.","Each resource on the Kubernetes API server signifies an entity or operation in the Kubernetes system. Resources typically represent objects you can manage, such as Pods, Services, or ConfigMaps. Some resources might represent operations on other objects, like a permission check. Each resource is represented as an HTTP endpoint on the API server, defining the schema for the objects or operations associated with that resource.",Each resource in the Kubernetes API server defines its schema for the objects or operations on that resource. This schema represents an HTTP endpoint (URI) on the Kubernetes API server.,Members can participate in issues and PRs assignment and have pre-submit tests automatically run for their PRs through GitHub teams in the Kubernetes community.,Pre-submit tests are automatically run for members' PRs.,A member is expected to remain an active contributor to the community. Members can have issues and PRs assigned to them and participate in discussions through GitHub teams. Pre-submit tests are automatically run for members' PRs.,"The primary function of the Kubernetes cloud controller manager is to embed cloud-specific control logic. It lets you link your cluster into your cloud provider's API and separates the components that interact with that cloud platform from components that only interact with your cluster. By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.",The cloud controller manager benefits the development pace by decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure. This enables cloud providers to release features at a different pace compared to the main Kubernetes project.,"The cloud controller manager is a Kubernetes component that embeds cloud-specific control logic. It allows you to link your cluster into your cloud provider's API and separates out the components that interact with that cloud platform from components that only interact with your cluster. By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.","Docker Engine uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance.",Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces to allow independent containers to run within a single Linux instance.,"Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).","A workload is an application running on Kubernetes.

Various core objects that represent different types or parts of a workload include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects.","Various core objects that represent different types or parts of a workload include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects.","A workload in Kubernetes might have different components distributed as separate objects. For example, a web application could be set up with the web server running as a Deployment, while the database runs as a StatefulSet. Each part manages its own replicas and state according to the specific needs of the application, ensuring high availability, scalability, and fault tolerance.","In Kubernetes, CEL (Common Expression Language) is used to run queries and perform fine-grained filtering. For example, you can use CEL expressions with dynamic admission control to filter for specific fields in requests and with dynamic resource allocation (DRA) to select resources based on specific attributes.","In Kubernetes, CEL expressions can be utilized with dynamic admission control to perform fine-grained filtering by allowing you to filter for specific fields in requests. This means you can use CEL to define conditions on incoming API requests, ensuring that only requests meeting specific criteria are admitted into the cluster.","In Kubernetes, CEL can be used with dynamic resource allocation (DRA) to select resources based on specific attributes. This fine-grained filtering allows you to define rules and queries using CEL expressions to ensure that the allocated resources meet certain criteria or conditions established by the user or cluster administrator.","The essential resources for running containers within Kubernetes include processing power (CPU), storage memory, and network capabilities. These resources provide the fundamental hardware facilities necessary for container operation and have finite capacities measured in units applicable to each resource, such as the number of CPUs or bytes of memory. Kubernetes abstracts these common resources for allocation to workloads and utilizes operating system primitives to manage their consumption.","Containers connect to a network in Kubernetes using Container Network Interface (CNI) plugins, which are a type of Network plugin adhering to the appc/CNI specification. For more information, see [Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).","The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network is called the Data Plane.","When processes inside Pods access the cluster, they are authenticated by the API server as a particular service account, for example, `default`. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.","When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.","When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.","A Job in Kubernetes is used to create one or more pods that run a specified task to completion. Once the task is successfully completed, the Job ensures that the task does not execute again. It is typically used for batch processing or tasks that require guaranteed execution to completion rather than running continuously.","A Job creates one or more objects and ensures that a specified number of them successfully terminate. As Pods successfully complete, the Job tracks the successful completions.",The Job tracks the successful completions as Pods successfully complete.,"Group Version Resources (GVRs) specify the API group, API version, and resource (name for the object kind as it appears in the URI) associated with accessing a particular id of objects in Kubernetes. GVRs let you define and distinguish different Kubernetes objects, and specify a way of accessing objects that is stable even as APIs change.","Group Version Resources (GVRs) specify the API group, API version, and resource associated with accessing a particular object in Kubernetes. GVRs allow you to define and distinguish different Kubernetes objects and provide a stable way of accessing these objects even as APIs change.","In the context of Group Version Resources (GVRs), the term 'resource' refers to an HTTP resource. It is the name for the object kind as it appears in the URI, associated with accessing a particular id of an object in Kubernetes.","A GVR may not refer to a specific namespace because some APIs are namespaced, and a GVR represents a stable means of accessing Kubernetes objects, which includes both namespaced and cluster-wide resources. Therefore, a GVR inherently does not mandate a namespace association.","Group Version Resources (GVRs) contribute to distinguishing different Kubernetes objects by specifying the API group, API version, and resource (name for the object kind as it appears in the URI) associated with accessing a particular object in Kubernetes. GVRs allow you to define and differentiate various Kubernetes objects and provide a stable method of accessing these objects even as APIs evolve.","In computing, a proxy is a server that acts as an intermediary for a remote service.

A client interacts with the proxy; the proxy copies the client's data to the actual server; the actual server replies to the proxy; the proxy sends the actual server's reply to the client.","kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes networking concept. It maintains network rules on nodes which allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if available, otherwise, it forwards the traffic itself.","kube-proxy can operate in two modes: 

1. Plain userland proxy service mode.
2. Hybrid mode.

In terms of resource usage, if your operating system supports it, running kube-proxy in hybrid mode achieves the same overall effect as the userland proxy service mode but uses less system resources.","Running kube-proxy in hybrid mode, if the operating system supports it, can achieve the same overall effect as running it as a plain userland proxy service but with less system resources.","In computing, a proxy is a server that acts as an intermediary for a remote service.

A client interacts with the proxy; the proxy copies the client's data to the actual server; the actual server replies to the proxy; the proxy sends the actual server's reply to the client.","To manage the scheduling of newly created pods in Kubernetes, the `kube-scheduler` is the control plane component responsible for this task. It watches for newly created pods with no assigned nodes and selects a node for them to run on.

Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.","Factors taken into account for scheduling decisions in Kubernetes include:

- Individual and collective requirements
- Hardware/software/policy constraints
- Affinity and anti-affinity specifications
- Data locality
- Inter-workload interference
- Deadlines","In Kubernetes, affinity and anti-affinity specifications influence the scheduling of pods by providing the scheduler with a set of rules that indicate how pods should be placed in relation to each other or to specific nodes. These rules can be classified into two types: node affinity and pod-to-pod affinity.

Node affinity involves rules related to the scheduling of pods on nodes based on node labels, allowing you to specify that a pod should be placed on nodes with specific labels. Pod-to-pod affinity and anti-affinity, on the other hand, allow you to define rules concerning the co-location of pods. These rules determine whether certain pods should be scheduled together (affinity) or apart (anti-affinity).

The rules can be either required or preferred, depending on how strictly you want the scheduler to enforce them. By using these specifications, you can optimize your workload placement for performance, resource usage, and availability.",Data locality is one of the factors considered by the Kubernetes scheduler when making scheduling decisions. It impacts scheduling decisions by influencing where to place pods based on the location of the data they need to access. This can help reduce latency and improve the performance of workloads by ensuring that pods are scheduled on nodes that are close to the data they require.,"Deadlines are an important factor in the scheduling process for Kubernetes workloads because they help in ensuring that time-sensitive workloads are completed within a specified timeframe. This is critical for applications where meeting time constraints is essential for functionality, performance, or compliance reasons. By considering deadlines, the Kubernetes scheduler can prioritize resources and scheduling decisions to meet these time-sensitive requirements.","An API object that manages external access to the services in a cluster, typically HTTP.","An Ingress in a Kubernetes cluster can manage external access to the services, typically through HTTP. It may provide functionalities such as load balancing, SSL termination, and name-based virtual hosting.",An Ingress may provide SSL termination in a Kubernetes cluster.,"Yes, an Ingress object can provide name-based virtual hosting features for a cluster.",An Ingress in Kubernetes may provide load balancing.,"Approvers consider the following aspects for the holistic acceptance of a Kubernetes code contribution:

- Backwards/forwards compatibility
- Adhering to API and flag conventions
- Subtle performance and correctness issues
- Interactions with other parts of the system

Approver status is scoped to a part of the codebase and involves ensuring these factors are adequately addressed.","An approver in Kubernetes is focused on the holistic acceptance of a contribution, which includes aspects such as backwards/forwards compatibility, adherence to API and flag conventions, performance and correctness issues, and interactions with other parts of the system. Their status is scoped to a part of the codebase.

In contrast, a code reviewer is focused on code quality and correctness on some part of the project. Reviewers are knowledgeable about the codebase and software engineering principles, and their status is also scoped to a part of the codebase.","The previous title used for someone with approver status in Kubernetes was ""maintainer"".",Approver status is scoped to a part of the codebase. Approvers were previously referred to as maintainers.,"Resources that extend the functionality of Kubernetes.

[Installing addons](/docs/concepts/cluster-administration/addons/) explains more about using add-ons with your cluster, and lists some popular add-ons.","To find more information about installing add-ons in Kubernetes, you can visit the following link:

[Installing addons](/docs/concepts/cluster-administration/addons/)","To learn about add-ons in Kubernetes, you would find resources that extend the functionality of Kubernetes. The documentation includes explanations about using add-ons with your cluster and lists some popular add-ons.",One or more devices that are directly or indirectly attached to your Kubernetes cluster might include commercial products like GPUs or custom hardware such as ASIC boards. Attached devices usually require device drivers that let Kubernetes access the devices.,Attached devices usually require device drivers in Kubernetes because these drivers allow Kubernetes to access the devices.,One example of custom hardware that could be attached to a Kubernetes environment is ASIC boards.,"The lifecycle hooks expose events in the management lifecycle and let the user run code when the events occur.

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.",The PostStart hook executes immediately after a container is created.,"The PreStop hook is a lifecycle hook in Kubernetes that is triggered immediately before a container is terminated. It is blocking, meaning that the termination of the container is delayed until the hook completes its execution.","The lifecycle hooks expose events in the management lifecycle and let the user run code when the events occur.

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","Two hooks are exposed to Containers: PostStart, which executes immediately after a container is created, and PreStop, which is blocking and is called immediately before a container is terminated.","When the kubelet finds a static pod in its configuration, it automatically tries to create a Pod object on the Kubernetes API server for it. This means that the pod will be visible on the API server, but cannot be controlled from there.","When the kubelet finds a static pod in its configuration, it automatically tries to create a Pod object on the Kubernetes API server for it. This means that the pod will be visible on the API server, but cannot be controlled from there. (For example, removing a mirror pod will not stop the kubelet daemon from running it).","When you remove a mirror pod from the API server, it does not stop the kubelet daemon from running it.","Application containers are the containers in a pod that are started after any init containers have completed. They run the actual application and its services once the necessary initialization is complete. If a pod doesn't have any init containers configured, then all the containers in that pod function as application containers.",One might use an init container in a pod configuration to perform initialization tasks that are important for the overall system setup and must complete before any application containers (app containers) can run. Init containers run to completion in series and do not need to remain running once the application containers have started. This allows the separation of initialization processes from the main application logic.,"If a pod lacks init containers, all the containers in that pod are app containers.",The format of a duration in Kubernetes represents an amount of time and is based on the `time.Duration` type from the Go programming language.,"In Kubernetes APIs, durations are expressed as a series of non-negative integers combined with time unit suffixes. The format is based on the `time.Duration` type from the Go programming language. You can have more than one time quantity, and the duration is the sum of those time quantities. Valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"". For example, `5s` represents a duration of five seconds, and `1m30s` represents a duration of one minute and thirty seconds.","The valid time units you can use when specifying a duration in Kubernetes are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".","Yes, a Kubernetes duration can consist of multiple time quantities. The quantities are expressed as a series of non-negative integers combined with time unit suffixes, and the duration is interpreted as the sum of those time quantities. For example, `1m30s` represents a duration of one minute and thirty seconds.","A Kubernetes duration can be formatted by combining non-negative integers with time unit suffixes. For example: 

- `5s` represents a duration of five seconds.
- `1m30s` represents a duration of one minute and thirty seconds.

Valid time unit suffixes include ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".",You can enable or disable each API group by changing the configuration of your API server.,An API group makes it easier to extend the Kubernetes API.,Read [API Group](/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) for more information.,"Selectors are used in Kubernetes to filter a list of resources based on labels. They allow users to query lists of resources and filter them by their assigned labels, enabling more targeted operations on specific subsets of resources.",Selectors are applied when querying lists of resources to filter them by labels.,"Labels are key/value pairs that are attached to objects such as pods, services, etc. They are used to organize and select subsets of objects. Selectors use these labels to filter a list of resources based on the specified criteria. When querying lists of resources, selectors can be applied to filter them by their labels.","In the context of Kubernetes, CIDR (Classless Inter-Domain Routing) is used to assign a range of IP addresses through the start address and a subnet mask. This allows Nodes to assign each pod a unique IP address. CIDR enables efficient IP address allocation and management in the cluster, supporting both IPv4 and IPv6.","In the context of Kubernetes, CIDR (Classless Inter-Domain Routing) facilitates IP address allocation by assigning each Node a range of IP addresses through a start address and a subnet mask using CIDR notation. This allows Nodes to assign each Pod a unique IP address. Although originally a concept for IPv4, CIDR has also been expanded to include IPv6.","Yes, CIDR is applicable to both IPv4 and IPv6 in Kubernetes.","In the context of Kubernetes, a subnet mask in CIDR notation is used to assign a range of IP addresses to each Node. This allows each Node to assign unique IP addresses to pods. CIDR (Classless Inter-Domain Routing) notation describes these blocks of IP addresses through the start address and the subnet mask, facilitating the management of IP addresses for networking configurations in Kubernetes.",The main protocol for the communication between the Kubernetes components and the Container Runtime is the gRPC protocol defined by the Kubernetes Container Runtime Interface (CRI).,The Kubernetes Container Runtime Interface (CRI) defines the gRPC protocol used for communication between node components and the Container Runtime.,"To find more details about node components in the Kubernetes architecture, you can visit the documentation page dedicated to the topic. However, the specific link or section guide isn't available in the provided context. You might want to check the official Kubernetes documentation for more comprehensive information.","The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers. CSI allows vendors to create custom storage plugins for Kubernetes without adding them to the Kubernetes repository (out-of-tree plugins). To use a CSI driver from a storage provider, you must first [deploy it to your cluster](https://kubernetes-csi.github.io/docs/deploying.html). You will then be able to create a that uses that CSI driver.","Custom storage plugins in Kubernetes, provided through the Container Storage Interface (CSI), differ from traditional ""in-tree"" plugins in that they do not require inclusion in the Kubernetes code repository. CSI allows vendors to develop storage plugins independently as ""out-of-tree"" plugins. These custom plugins can be created and used without impacting the Kubernetes release cycle. By deploying a CSI driver from a storage provider to your cluster, you can integrate these custom storage solutions.","To use a CSI driver from a storage provider, you must first deploy it to your cluster.","For more detailed information on specific CSI drivers available for use with Kubernetes, you can refer to the [list of available CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html).","The layer provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network.",The layer where various containerized applications run is the Applications layer.,"The applications are prepared for operation in the ""Applications"" layer by running as containerized applications.","A StorageClass includes the following main components to help manage storage types:

- `provisioner`: Defines the type of provisioner used for the storage.
- `parameters`: Specifies the parameters that the provisioner uses when creating volumes.
- `reclaimPolicy`: Determines what happens to persistent volumes when they are released from their claims.","To request a specific storage type in Kubernetes, users can specify the name of a StorageClass object in their PersistentVolumeClaim.","In a StorageClass, the fields `provisioner`, `parameters`, and `reclaimPolicy` play the following roles:

- `provisioner`: This field identifies the plugin that is used to provision the storage. It determines which storage plugin to use and tells Kubernetes how to interface with it.

- `parameters`: This field defines specific parameters that are passed to the `provisioner` to determine how the storage is provisioned. These parameters can be used to customize the configuration and behavior of the storage being provisioned.

- `reclaimPolicy`: This field specifies the policy for what happens to the storage once it is no longer needed by a PersistentVolumeClaim. Common policies include retaining the storage, recycling it for new usage, or deleting it.","In Kubernetes, the main purpose of a SIG (special interest group) is to collectively manage an ongoing piece or aspect of the larger Kubernetes open source project. Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.","SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.","For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.","Yes, SIGs can have their own contribution policy and channels of communication, apart from the governance guidelines they must follow.","The primary purpose of a PersistentVolume in a Kubernetes cluster is to represent a piece of storage in the cluster. It provides an API that abstracts details about how storage is provided from how it is consumed, allowing storage to persist beyond the lifecycle of any individual pod or container.","PersistentVolumes (PVs) provide an API that abstracts details of how storage is provided from how it is consumed. They are used directly in scenarios where storage can be created ahead of time (static provisioning). For scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead.","In scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead of directly using PersistentVolumes (PVs), which are used in static provisioning scenarios where storage can be created ahead of time.","Static provisioning in Kubernetes storage involves creating PersistentVolumes (PVs) ahead of time. Cluster administrators pre-configure these PVs, and they remain available for use by Pods. This approach requires the storage to be set up in advance, and it is generally used when the specific requirements are known beforehand.

Dynamic provisioning, on the other hand, eliminates the need for pre-provisioning. It automatically provisions storage upon user request. With dynamic provisioning, users submit PersistentVolumeClaims (PVCs) which trigger the automatic creation of PVs based on predefined StorageClass configurations. This method is more flexible and efficient for scenarios where on-demand storage is needed, as it allows storage to be created dynamically without manual intervention by administrators.","To list all images running in a Kubernetes cluster across all namespaces, use the following command:

```bash
kubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'
```","```bash
kubectl get pods --namespace default --output=custom-columns=""NAME:.metadata.name,IMAGE:.spec.containers[*].image""
```","To list all images in a cluster, excluding the specific image named 'registry.k8s.io/coredns:1.6.2', use the following command:

```bash
kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=""registry.k8s.io/coredns:1.6.2"")].image'
```","To display all metadata fields for pods in every namespace, regardless of their specific names, use the following command:

```bash
kubectl get pods -A -o=custom-columns='DATA:metadata.*'
```","The purpose of using the `-o=jsonpath` option in `kubectl get pods` is to extract specific fields from the output using a JSONPath expression. This allows you to format and filter the information displayed, tailoring it to your specific needs. JSONPath provides a way to navigate through the JSON data returned by Kubernetes and to retrieve particular data points or values, such as names, statuses, or labels of resources.","To retrieve only the name of the first pod using the jsonpath option, use the following command:

```bash
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
```","To get the metadata name and status capacity of all pods, use the following jsonpath query:

```shell
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
```","On Windows, you must double quote any JSONPath template that contains spaces. This means you should use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```","When using the `-o json` flag, the output is a full JSON representation of the resource, providing complete details about the specified resources including all their metadata, status, spec, and any other fields. This format is useful for detailed examination or processing by tools that can handle JSON input.

When using the `-o=jsonpath` flag, the output is customized based on the JSONPath template provided. JSONPath allows you to query and extract specific fields from the resource according to the path specified. This is useful for retrieving only the information you need rather than the complete JSON output. JSONPath outputs selected parts of the resource's data, allowing for concise and tailored information retrieval.","A Kubernetes cluster consists of several main components:

1. **Control Plane Components**: These components manage the state of the cluster. They include:
   - **kube-apiserver**: Serves as the front end of the Kubernetes control plane, providing the API that users and applications interact with.
   - **etcd**: Stores the configuration data of the cluster, representing the state of the cluster at any given point.
   - **kube-scheduler**: Assigns workloads to nodes based on resource availability and policy constraints.
   - **kube-controller-manager**: Runs controllers that perform routine tasks, such as responding to health events and managing replication.

2. **Node Components**: These run on every node and ensure that the containers are running in a Pod. They include:
   - **kubelet**: Ensures that containers are running in a Pod.
   - **kube-proxy**: Manages network rules and enables communication to your Pods from network sessions inside or outside of your cluster.

3. **Cloud Controller Manager**: Integrates with the cloud provider’s API to manage cloud resources and separates cloud-specific control logic from the main Kubernetes components.

These components work together to maintain the desired state of your applications and environments in a Kubernetes cluster.","To ensure that applications are running as desired, Kubernetes allows you to configure application resources using commands like:

```bash
kubectl set SUBCOMMAND
```","A node is a worker machine in Kubernetes. A worker node may be a VM or physical machine, depending on the cluster. It has local daemons or services necessary to run and is managed by the control plane. The daemons on a node include, and a container runtime implementing the. In early Kubernetes versions, Nodes were called ""Minions"".","The smallest and simplest Kubernetes object. A Pod represents a set of running processes on your cluster.

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a controller.","Kubernetes handles the scaling of applications through the use of replicas and the `kubectl scale` command. A replica is a copy or duplicate of a pod, and having multiple replicas ensures high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. You define the number of replicas in a Deployment or ReplicaSet, and Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed.

To change the number of replicas and scale an application, you can use the `kubectl scale` command. For example:

```bash
# Scale a replica set named 'foo' to 3 replicas
kubectl scale --replicas=3 rs/foo

# Scale a deployment named 'mysql' to 3 replicas
kubectl scale --replicas=3 deployment/mysql

# Scale a stateful set named 'web' to 3 replicas
kubectl scale --replicas=3 statefulset/web
```

Kubernetes automatically handles distributing these replicas across the nodes in a cluster, ensuring efficient load balancing and self-healing capabilities.","`kubectl` controls the Kubernetes cluster manager. It is a command line tool for communicating with a Kubernetes cluster using the Kubernetes API. You can use `kubectl` to create, inspect, update, and delete Kubernetes objects.",Find more information at: https://kubernetes.io/docs/reference/kubectl/,"The basic syntax for using kubectl with flags is:

```shell
kubectl [flags]
```","The context you provided does not explicitly list the main components of a Kubernetes cluster. However, based on common knowledge, the main components usually include the Control Plane (with elements like the API server, etcd, scheduler, and controller managers), Nodes (each running a kubelet and kube-proxy), and optionally, the Cloud Controller Manager for cloud-specific interactions. If you would like detailed information about these components, please refer to Kubernetes documentation or other resources.","A Pod is the smallest and simplest Kubernetes object that represents a set of running processes on your cluster. It is typically set up to run a single primary container and can also run optional sidecar containers for supplementary features like logging. Pods are commonly managed by higher-level controllers.

A Node, on the other hand, is a worker machine in Kubernetes. It can be a VM or a physical machine and has local daemons or services necessary to run Pods. It is managed by the control plane and includes daemons such as kubelet, container runtime, and kube-proxy.

In summary, a Pod is an application-centric unit for running containers, while a Node is the infrastructure component that provides the environment for Pods to run.",The API server is a component of the Kubernetes that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.,"The primary purpose of Kubernetes in managing containerized applications is to provide a platform for automating deployment, scaling, and operations of application containers across clusters of hosts. Kubernetes aims to efficiently manage the life cycle of containerized applications by handling the scheduling of containers, scaling them according to resource usage and demand, managing changes to existing containerized applications, and helping optimize the use of underlying hardware infrastructure.","To ensure high availability for applications, Kubernetes employs replicas. Replicas are copies or duplicates of a set of pods that ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed. This replica management allows for efficient load balancing, rolling updates, and self-healing capabilities in a Kubernetes cluster.","The smallest and simplest Kubernetes object. A Pod represents a set of running containers on your cluster.

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a controller.","To scale applications automatically, Kubernetes uses replicas to ensure high availability, scalability, and fault tolerance by maintaining multiple identical instances of a pod. By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that the specified number of instances are running, automatically adjusting the count as needed. This replica management allows for efficient load balancing, rolling updates, and self-healing capabilities in a Kubernetes cluster.","Kubernetes handles resource allocation for different applications by abstracting common resources and allocating them to workloads. These resources include CPU, memory, and GPUs, and are managed at the node level. Kubernetes utilizes operating system primitives to manage resource consumption by these workloads. Additionally, Kubernetes offers dynamic resource allocation (DRA), which allows device drivers and cluster admins to define device classes for workloads to claim. This enables automatic management of complex resource allocations. Kubernetes also employs affinity rules to provide hints to the scheduler about pod placement, ensuring resources are allocated efficiently based on specified preferences or requirements.","A tool for quickly installing Kubernetes and setting up a secure cluster. 

You can use kubeadm to install both the control plane and the components.","In Kubernetes, containerized applications are managed across a cluster using controllers. Controllers are control loops that observe the state of the cluster and make or request changes to align with the desired state. Each controller is responsible for ensuring that the current state of the cluster moves closer to the desired state.

The controllers continuously watch the shared state of the cluster and make necessary adjustments. They operate either in the control plane or within the components that manage the cluster. Examples of such controllers include the deployment controller, daemonset controller, namespace controller, and persistent volume controller. Each plays a crucial role in the operations of Kubernetes and helps maintain the smooth running of containerized applications.","The API server is a component of the Kubernetes cluster that exposes the Kubernetes API. It acts as the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is called `kube-apiserver`. It is designed to scale horizontally by deploying multiple instances, allowing you to run several instances of `kube-apiserver` and balance traffic between those instances.","In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state. Controllers watch the shared state of your cluster through the control plane's API server.

The scheduler in Kubernetes works with affinity rules. Affinity is a set of rules that give hints to the scheduler about where to place pods. There are two kinds of affinity: node affinity and pod-to-pod affinity. The rules are defined using the Kubernetes API, specified in Pod specifications, and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","The smallest and simplest Kubernetes object. A Pod represents a set of running containers on your cluster. 

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a controller.","To update the service account in a cluster role binding, use the following command:

```bash
kubectl set subject clusterrolebinding NAME --serviceaccount=namespace:serviceaccountname
```","To update a role binding to include multiple users and a group, use the following command:

```bash
kubectl set subject rolebinding ROLEBINDING_NAME --user=user1 --user=user2 --group=groupname
```","The `--dry-run=client` option simulates the execution of a command locally without applying or sending any change to the server. It allows you to see the output of a command as if it were executed, showing what would happen, but without making any actual updates to the resources.",Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.,"A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.",Service objects currently support setting selectors with the method described.,"Specifying the --resource-version affects the update process for selectors by ensuring that updates will use the specified resource version. If --resource-version is not specified, the existing resource-version will be used.",The `--dry-run=client` option is used to print the corresponding API objects without actually creating them. This allows you to preview the outcome of the `kubectl` command without making any changes to the cluster.,"Possible resources where environment variables can be managed in Kubernetes include:

- pod (po)
- replicationcontroller (rc)
- deployment (deploy)
- daemonset (ds)
- statefulset (sts)
- cronjob (cj)
- replicaset (rs)","To update the environment variables of a specific deployment to include a new key-value pair, use the following command:

```bash
kubectl set env deployment/DEPLOYMENT_NAME KEY=VALUE
```

Replace `DEPLOYMENT_NAME` with the name of your deployment and `KEY` and `VALUE` with the desired environment variable key and value.","To list all environment variables for a specific deployment, use the following command:

```bash
kubectl set env deployment/DEPLOYMENT_NAME --list
```","To import environment variables from a config map with a specified prefix using `kubectl`, use the following command:

```bash
kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
```","To remove the environment variable from all deployment configurations while targeting a specific container, use the following command:

```bash
kubectl set env deployments --all --containers=""c1"" ENV-
```","To update the service account of a deployment, use the following command:

```bash
kubectl set serviceaccount deployment DEPLOYMENT_NAME SERVICE_ACCOUNT
```","To update the service account of pod template resources, you can use the following resources:

- replicationcontroller (rc)
- deployment (deploy)
- daemonset (ds)
- job
- replicaset (rs)
- statefulset","To simulate updating a deployment's service account without sending the command to the API server, use the following command:

```bash
kubectl set serviceaccount deployment YOUR_DEPLOYMENT serviceaccount1 --local --dry-run=client -o yaml
```

Replace `YOUR_DEPLOYMENT` with the name of your deployment.","If a request is not specified for a compute resource like CPU or memory, the request will default to the limit if a limit is specified.","To set both limits and requests for CPU and memory on a deployment's container using `kubectl`, use the following command:

```bash
kubectl set resources deployment DEPLOYMENT_NAME --limits=cpu=LIMIT,memory=LIMIT --requests=cpu=REQUEST,memory=REQUEST
```

Replace `DEPLOYMENT_NAME`, `LIMIT`, and `REQUEST` with your specific deployment name and desired CPU and memory values. For example:

```bash
kubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
```","To remove the resource requests for a deployment's containers, use the following command:

```bash
kubectl set resources deployment DEPLOYMENT_NAME --limits=cpu=0,memory=0 --requests=cpu=0,memory=0
```","To update resource limits for a container in a YAML file locally without applying changes to the server, use the following command:

```bash
kubectl set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml
```","To update the image of a specific container in a deployment to a new version, use the following command:

```bash
kubectl set image deployment/DEPLOYMENT_NAME CONTAINER_NAME=NEW_CONTAINER_IMAGE
```","To update the image of all containers in a specific DaemonSet to a specified image, use the following command:

```bash
kubectl set image daemonset DAEMONSET_NAME *=SPECIFIED_IMAGE
```","To update the container image from a file without connecting to the server, use the `--local` flag.

```bash
kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","To update multiple resource types like deployments and replication controllers at the same time using a single command, you can use the `kubectl rollout` command with appropriate subcommands for each resource type you intend to manage. You will have to specify each resource type and name, usually separating commands for different types unless you use a label selector that applies to multiple resources.

For example, if you want to restart both a deployment and a replication controller, you might need separate commands, as `kubectl rollout` typically manages one resource type at a time. However, if both types share a common label, you can target them using:

```bash
kubectl rollout restart deployment,rc --selector=app=myapp
```

Make sure the resources are labeled appropriately for such an operation. Otherwise, you will need to explicitly list and execute commands for each resource type.","Yes, it is possible to specify multiple containers and their respective images in one command when updating a resource using `kubectl set image`. Here is a sample command:

```bash
kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
```","To configure application resources in Kubernetes, you can use the following command:

```
kubectl set SUBCOMMAND
```","To configure application resources, use the following syntax:

```
kubectl set SUBCOMMAND
```

These commands help you make changes to existing application resources.","To configure application resources using `kubectl set`, you can make changes to existing application resources.","To start a pod with a specific name and image, use the following command:

```bash
kubectl run NAME --image=image
```","To expose a specific port on a pod using the `kubectl run` command, you can use the `--port` flag as follows:

```bash
kubectl run POD_NAME --image=IMAGE_NAME --port=PORT_NUMBER
```

For example, to start a pod with the name `hazelcast` and expose port `5701`, you can run:

```bash
kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
```","The purpose of using the `--dry-run` option when running a pod is to print the corresponding API objects without creating them. This allows you to preview the results of a command before actually executing it, ensuring that the configuration is correct. 

```bash
kubectl run nginx --image=nginx --dry-run=client
```","To set multiple environment variables when starting a pod with the `kubectl run` command, you can use the `--env` flag multiple times. Here's an example:

```bash
kubectl run mypod --image=myimage --env=""KEY1=VALUE1"" --env=""KEY2=VALUE2""
```","When starting a pod, the `--overrides` option allows you to overload the pod's specification with a partial set of values parsed from JSON. This can be used to customize the pod's configuration beyond the default settings provided by the `kubectl run` command. For example:

```bash
kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
```","To print all supported API resources on a Kubernetes server, use the following command:

```bash
kubectl api-resources
```","To print the supported API resources along with additional details, use the following command:

```bash
kubectl api-resources -o wide
```","To sort the list of supported API resources by name, use the following command:

```bash
kubectl api-resources --sort-by=name
```","To display only the API resources that are namespaced, use the following command:

```bash
kubectl api-resources --namespaced=true
```","To list API resources from a specific API group, such as 'rbac.authorization.k8s.io', use the following command:

```bash
kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Annotations in Kubernetes are key-value pairs used to attach arbitrary non-identifying metadata to objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. Unlike labels, which are used to identify and organize objects, annotations are generally used by tools and system extensions for storing additional data with objects, such as structured JSON.

Labels, in contrast, are key-value pairs used to identify and organize objects. They serve as an effective way to specify the selection criteria for queries and are instrumental in defining service selectors, replication controller queries, and other operations that depend on grouping resources logically. Labels have specific naming requirements and constraints, such as the ability to begin with a DNS subdomain prefix.","Attempting to set an annotation that already exists will fail unless the `--overwrite` flag is set. To resolve this, use the `--overwrite` option with the `kubectl annotate` command:

```bash
kubectl annotate --overwrite TYPE NAME KEY_1=VAL_1 ...
``` 

This allows the existing annotation to be updated with the new value you provide.","To update an annotation on multiple Kubernetes resources at once, you can use the following command:

```bash
kubectl annotate pods --all KEY=VALUE
```

This command will apply the annotation to all pods in the namespace. If you need to overwrite an existing annotation, add the `--overwrite` flag:

```bash
kubectl annotate --overwrite pods --all KEY=VALUE
```","The purpose of the --resource-version flag when modifying annotations is to ensure that the update is applied only if the resource is unchanged from the specified resource version. If the specified --resource-version does not match the current resource version on the server, the command will fail.","To remove an annotation from a Kubernetes resource, use the following command:

```bash
kubectl annotate RESOURCE_NAME ANNOTATION_KEY-
```

For example, to remove an annotation named 'description' from a pod named 'foo', you would use:

```bash
kubectl annotate pods foo description-
```","By default, when running the `kubectl cluster-info dump` command without specifying an output directory, the output is dumped to stdout.","To include log data for pods in the cluster information dump and organize them, use the following command:

```bash
kubectl cluster-info dump
```

The logs of all of the pods in the cluster will be dumped into different directories based on namespace and pod name.","To include information from all namespaces in the cluster dump, use the following flag:

```bash
--all-namespaces
```","To specify a particular directory location to save the cluster dump files, use the following command:

```bash
kubectl cluster-info dump --output-directory=/path/to/cluster-state
```","To dump information from specific namespaces only, use the `--namespaces` flag.","To display the addresses of the control plane and services in Kubernetes, use the following command:

```bash
kubectl cluster-info
```",The label associated with the services whose addresses are displayed by the 'kubectl cluster-info' command is `kubernetes.io/cluster-service=true`.,"To debug and diagnose cluster issues in Kubernetes, you can use the following command:

```bash
kubectl cluster-info dump
```","The purpose of using flags with `kubectl cluster-info` is to modify the command's behavior and specify options for displaying addresses of the control plane and services or obtaining additional debugging and diagnostic information about the cluster. For example, you can use the `kubectl cluster-info dump` command with flags to customize the output, such as specifying an output directory or selecting specific namespaces to include in the dump.","To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.","The purpose of the `kubectl explain` command in Kubernetes is to describe the fields and structure of various resources. This command provides information about each field associated with supported API resources in OpenAPI format, helping users understand the structure and fields of resources available in the cluster.

Usage examples:

```bash
# Get the documentation of the resource and its fields
kubectl explain pods

# Get all the fields in the resource
kubectl explain pods --recursive

# Get the explanation for deployment in supported api versions
kubectl explain deployments --api-version=apps/v1

# Get the documentation of a specific field of a resource
kubectl explain pods.spec.containers

# Get the documentation of resources in different format
kubectl explain deployment --output=plaintext-openapiv2
```","To retrieve information about all fields of a resource using 'kubectl explain', use the following command:

```bash
kubectl explain TYPE --recursive
```

For example, to get all the fields in the pods resource:

```bash
kubectl explain pods --recursive
```","The API version parameter in the `kubectl explain` command specifies the API version of the resource you want documentation for. It allows you to get the explanation for a resource in the specified API version. For example:

```bash
kubectl explain deployments --api-version=apps/v1
```",Information about each field is retrieved from the server in OpenAPI format.,"To get the explanation of a specific field within a Kubernetes resource, use the following command:

```bash
kubectl explain RESOURCE.FIELD
```

For example, to get the documentation of a specific field of a pod, you could use:

```bash
kubectl explain pods.spec.containers
```","To describe resources in Kubernetes using their name prefix or label selector, you can use the following command format:

```bash
kubectl describe TYPE [NAME_PREFIX | -l label]
```

Examples include:

```bash
# Describe a specific type of resource with a name prefix
kubectl describe pods frontend

# Describe resources of a particular type using a label selector
kubectl describe pods -l name=myLabel
```","To get a detailed description of all resources of a particular type using kubectl, use the following command:

```bash
kubectl describe TYPE
```","To describe all pods created by a specific replication controller, use the following command:

```bash
kubectl describe pods replication-controller-name
```","If there is no exact match for the TYPE and NAME_PREFIX provided in a `kubectl describe` command, it will output details for every resource that has a name prefixed with NAME_PREFIX.","To describe a resource using a configuration file in Kubernetes, use the following command:

```bash
kubectl describe -f FILENAME
```",The default editor used for editing resources if the KUBE_EDITOR or EDITOR environment variables are not set is 'vi' for Linux or 'notepad' for Windows.,"To edit a resource in JSON format instead of the default YAML, use the `-o json` option with the `kubectl edit` command. For example:

```bash
kubectl edit RESOURCE/NAME -o json
```","In the event another editor changes the resource on the server while you are editing it, you will have to apply your changes to the newer version of the resource or update your temporary saved copy to include the latest resource version.","Yes, you can edit multiple resources at once with the `kubectl edit` command. However, changes are applied one at a time.","To specify the use of Windows line endings when editing a resource, you can use the `--windows-line-endings` flag with either the `kubectl edit` or `kubectl apply edit-last-applied` commands.","To retrieve logs from a pod that has multiple containers, you can use the `kubectl logs` command with the `--all-containers=true` option:

```bash
kubectl logs nginx --all-containers=true
```

Additionally, if you want to access logs from a specific container within the pod, you can specify the container name using the `-c` option:

```bash
kubectl logs -c CONTAINER_NAME POD_NAME
```","To stream logs from a pod and continue logging even if errors occur, use the following command:

```bash
kubectl logs nginx -f --ignore-errors=true
```","To obtain logs from a specific pod and limit the log file size to a certain number of bytes, use the following command:

```bash
kubectl logs POD_NAME --limit-bytes=NUMBER_OF_BYTES
```","To get logs from all containers in pods that have a specific label, use the following command:

```bash
kubectl logs -l LABEL_NAME=LABEL_VALUE --all-containers=true
```","To fetch logs from a job named 'hello', you can use the following command:

```bash
kubectl logs job/hello
```","To create a namespace with the specified name in Kubernetes, use the command:

```bash
kubectl create namespace NAME [--dry-run=server|client|none]
```

Example:

```bash
# Create a new namespace named my-namespace
kubectl create namespace my-namespace
```","To create a namespace called 'my-namespace' using kubectl, use the following command:

```bash
kubectl create namespace my-namespace
```","The available options for the `--dry-run` flag when creating a namespace are `server`, `client`, and `none`.","To request a service account token for a specific namespace using kubectl, use the following command:

```bash
kubectl create token SERVICE_ACCOUNT_NAME --namespace NAMESPACE_NAME
```","To get a token with a custom expiration time for a service account, use the following command:

```bash
kubectl create token SERVICE_ACCOUNT_NAME --duration 10m
```","To request a token with a specific audience using the kubectl command, use the following command:

```bash
kubectl create token SERVICE_ACCOUNT_NAME --audience https://example.com
```","Binding a token to a Secret object is used to ensure that the token is specifically associated with the Secret, providing additional security and context for token usage. It is specified in the command using the `--bound-object-kind`, `--bound-object-name`, and optionally `--bound-object-uid` flags.

To request a token bound to an instance of a Secret object, use the following command:

```bash
kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
```

If you need to specify the Secret object with a specific UID, use this command:

```bash
kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","To request a token bound to a Secret object with a specific UID, use the following command:

```bash
kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","To create an ingress that directs all requests to '/path' to a specific service and specifies an Ingress Class, use the following command:

```bash
kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
```","To create an ingress that uses TLS with a specified secret and a path type of Prefix, use the following command:

```bash
kubectl create ingress ingsecret --class=default \
--rule=""foo.com/*=svc:8080,tls=secret1""
```","To create an ingress with the same host and multiple paths, use the following command:

```bash
kubectl create ingress multipath --class=default \
--rule=""foo.com/=svc:port"" \
--rule=""foo.com/admin/=svcadmin:portadmin""
```","To create an ingress with two specific annotations and a default Ingress Class, use the following command:

```bash
kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
--annotation ingress.annotation1=foo \
--annotation ingress.annotation2=bla
```","To create an ingress with a default backend, use the following command:

```bash
kubectl create ingress ingdefault --class=default \
--default-backend=defaultsvc:http \
--rule=""foo.com/*=svc:8080,tls=secret1""
```","To create a deployment named 'my-dep' that uses the busybox image and specifies a command to run, use the following command:

```bash
kubectl create deployment my-dep --image=busybox -- date
```","To create a deployment with a specific number of replicas using the nginx image, you can use the following command:

```bash
kubectl create deployment my-dep --image=nginx --replicas=3
```

This example creates a deployment named `my-dep` that runs the nginx image with 3 replicas. Adjust the number of replicas as needed for your use case.","To create a deployment that uses multiple container images, use the following command:

```bash
kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","To specify a port to be exposed when creating a deployment, use the following command:

```bash
kubectl create deployment my-dep --image=busybox --port=5701
```","To create a deployment with `kubectl create deployment`, you need to specify the name of the deployment and the image to use. Here is the basic command:

```bash
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

Here are some examples:

- Create a deployment named `my-dep` that runs the `busybox` image:

  ```bash
  kubectl create deployment my-dep --image=busybox
  ```

- Create a deployment with a command:

  ```bash
  kubectl create deployment my-dep --image=busybox -- date
  ```

- Create a deployment named `my-dep` that runs the `nginx` image with 3 replicas:

  ```bash
  kubectl create deployment my-dep --image=nginx --replicas=3
  ```

- Create a deployment named `my-dep` that runs the `busybox` image and expose port 5701:

  ```bash
  kubectl create deployment my-dep --image=busybox --port=5701
  ```

- Create a deployment named `my-dep` that runs multiple containers:

  ```bash
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
  ```",Create a LoadBalancer service with the specified name.,"To specify the ports when creating a LoadBalancer service using kubectl, you can use the `--tcp` flag followed by the port and targetPort. Here is the command:

```bash
kubectl create service loadbalancer NAME --tcp=port:targetPort
```

For example, to create a new LoadBalancer service named `my-lbs` with port 5678 mapped to targetPort 8080, use:

```bash
kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","The `--dry-run` flag, when used with the `kubectl create service loadbalancer` command, simulates the creation of the LoadBalancer service without actually performing the operation. It is useful for testing and validating what the command would do.

You can specify `--dry-run=server`, `--dry-run=client`, or `--dry-run=none`:

- `--dry-run=server`: The request is sent to the server, and the server simulates the creation.
- `--dry-run=client`: The client (kubectl) performs the simulation without sending the request to the server. 
- `--dry-run=none`: The default behavior, where the service is actually created.","To create a LoadBalancer service with a specific name and port forwarding, use the following command:

```bash
kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","The parameter `--tcp=port:targetPort` in the command to create a LoadBalancer service represents the mapping of a port on the LoadBalancer to a target port on the backend Pods. The `port` is the port exposed by the service, and `targetPort` is the port on the Pods that the traffic is directed to.","Create an ExternalName service with the specified name.

ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.","An ExternalName service in Kubernetes references an external DNS address instead of pointing only to pods, allowing application authors to reference services existing off-platform, on other clusters, or locally. In contrast, a regular service typically exposes a network application running as one or more pods within the cluster, directing network traffic to the current set of pods matching a selector.","To create an ExternalName service named 'my-ns' that points to 'bar.com', use the following command:

```bash
kubectl create service externalname my-ns --external-name bar.com
```","The options `--dry-run=server|client|none` specify the strategy to use when executing the command without actually creating the resource.

- `server`: Validate the command against the server without applying changes.
- `client`: Only validate the command locally without contacting the server.
- `none`: Execute the command normally without dry-run behavior.","An example of when you might use an ExternalName service in Kubernetes is when you want to reference services that exist off-platform, on other clusters, or locally. This service type allows application authors to point to an external DNS address instead of only targeting pods within the cluster. For instance, if you have a service running on a separate system outside of your Kubernetes cluster, you can create an ExternalName service to provide a consistent way for your internal workloads to access that external service by name.","Create a new secret for use with Docker registries.

Dockercfg secrets are used to authenticate against Docker registries.

When using the Docker command line to push images, you can authenticate to a given registry by running:
`$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL`.

That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

When creating applications, you may have a Docker registry that requires authentication. In order for the nodes to pull images on your behalf, they must have the credentials. You can provide this information by creating a dockercfg secret and attaching it to your service account.","A generic type secret in Kubernetes is referred to as an ""Opaque"" secret type.",A TLS type secret in Kubernetes holds a TLS certificate and its associated key. The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.,"The `--value` option when creating a priority class is used to assign an integer priority to the PriorityClass. This priority determines the importance of Pods using this PriorityClass, with higher values indicating higher priority.","To set a priority class as the global default priority, use the following command:

```bash
kubectl create priorityclass NAME --value=VALUE --global-default=true --description=""DESCRIPTION""
```

Example:

```bash
kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
```","The `--preemption-policy` option, when creating a priority class, specifies whether the priority class can preempt pods with lower priority. For example, using `--preemption-policy=""Never""` will create a priority class that cannot preempt pods with lower priority.","To specify a dry run when creating a priority class, use the following command:

```bash
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL --dry-run=server|client|none
```","Yes, two PriorityClasses can have the same value. When two PriorityClasses have the same value, pods are scheduled based on their priority values, and if the priority values are the same, the scheduler does not distinguish between them based on priority alone. Instead, other scheduling criteria and policies will be used to decide the order of pod scheduling.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an application owner to create an object for a replicated application, that ensures a certain number or percentage of Pods with an assigned label will not be voluntarily evicted at any point in time. Involuntary disruptions cannot be prevented by PDBs; however, they do count against the budget.","To specify which pods a disruption budget applies to, you use a selector in the `kubectl create poddisruptionbudget` command. For example:

```bash
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N
```

This selector determines which pods are included in the disruption budget. You specify it using label selectors, such as `--selector=app=rails`, to target pods with specific labels.","The `--min-available` option defines the minimum number or percentage of pods that must be available at any point in time for the selected pods. This ensures that despite voluntary disruptions, a certain number of pods will remain running.","If the specified number or percentage of available pods is not met according to the pod disruption budget, voluntary disruptions will not occur. This means that actions such as rolling updates, draining nodes, or other voluntary evictions that would decrease the available pods below the desired minimum as defined in a pod disruption budget will be blocked. However, involuntary disruptions such as node failures cannot be prevented by pod disruption budgets, although they do count against the budget.","To create a pod disruption budget for an application with a specific label, use the following command:

```bash
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N
```

For example, to create a pod disruption budget named `my-pdb` that will select all pods with the `app=rails` label and require at least one of them to be available at any point in time, you can use:

```bash
kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
```","When creating a config map from a file, the key will default to the basename of the file, and the value will default to the file content.","When creating a config map from a directory in Kubernetes, any directory entries that are not regular files (e.g. subdirectories, symlinks, devices, pipes, etc.) are ignored.","Yes, you can specify custom keys instead of using the file basenames when creating a config map from files. Use the following command:

```bash
kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
```","To create a config map using literal key-value pairs, use the following command:

```bash
kubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2
```","To create a ConfigMap from environment variable files, use the following command:

```bash
kubectl create configmap NAME --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","To create a service using a specified subcommand in Kubernetes, use the following command:

```
kubectl create service [flags]
```","To ensure the output of a service creation is in JSON format, you can use the `-o json` flag with the `kubectl create service` command. Here is an example:

```bash
kubectl create service CLUSTER-IP my-service --tcp=80:80 -o json
```",Flags in the `kubectl create service` command are used to specify additional options and configurations for the service being created. They provide a way to customize the behavior of the command according to your needs.,Create a cluster role binding for a particular cluster role.,"To create a cluster role binding for multiple users and a group using the cluster-admin role, use the following command:

```bash
kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1
```","The `--dry-run` option in the `kubectl create clusterrolebinding` command allows you to simulate the command without actually applying any changes. It can take the values `server`, `client`, or `none`. When set to `server`, it will perform a dry run against the server, validating the command without persisting the result. When set to `client`, it validates the command locally without contacting the API server. The default value `none` will execute the command and persist the changes.","When creating a secret based on a file, the key will default to the basename of the file.","When creating a secret from a directory, Kubernetes ignores any directory entries that are not regular files, such as subdirectories, symlinks, devices, pipes, etc. Only regular files with valid basenames are packaged into the secret.","Yes, you can specify custom keys when creating a secret from files. Use the `--from-file=key=source` option where `key` is the custom key you want to specify, and `source` is the path to the file.

For example:

```bash
kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
```","The purpose of the `--dry-run` option in the `kubectl create secret` command is to simulate the command without actually persisting the resource to the cluster. It can be used to test and validate your command. The option can be specified with `server`, `client`, or `none` which dictates how the dry run validation is performed.","To create a Kubernetes secret using a combination of files and literal values, use the following command:

```bash
kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
```","To create a new resource quota named 'my-quota' with hard limits, use the following command:

```bash
kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
```","To specify scopes when creating a resource quota in Kubernetes, use the `--scopes` option in the `kubectl create quota` command. Here is an example:

```bash
kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","The purpose of the `--dry-run` option in the `kubectl create quota` command is to simulate the creation of the resource quota without actually creating it. This can be used to verify that the command is correct and would succeed if run for real. The `--dry-run` option can have the values `server`, `client`, or `none`, which determine how the dry run is executed.","To create a resource quota with a specific scope of 'BestEffort' for a 'best-effort' quota, use the following command:

```bash
kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","To set limits when creating a resource quota in Kubernetes, you can define hard limits on resources such as:

- CPU
- Memory
- Pods
- Services
- ReplicationControllers
- ResourceQuotas
- Secrets
- PersistentVolumeClaims",The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.,The public key certificate must be .PEM encoded.,"To create a TLS secret named 'tls-secret' using kubectl, use the following command:

```bash
kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","The `--dry-run` option in the `kubectl create secret` command is used to simulate the creation of the secret without actually performing the action. It helps you verify the command before executing it by previewing the result. The option can take values of `server`, `client`, or `none`:

- `server`: The request is sent to the API server and processed there.
- `client`: The command is processed locally without contacting the API server.
- `none`: Default behavior; the command is executed normally without a dry run.","To create a TLS secret using the `kubectl` command, you should specify the paths to the certificate file and the key file. Use the following command:

```bash
kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","To specify when the cron job should run, using the [cron](https://en.wikipedia.org/wiki/Cron) format, which determines the periodic schedule for executing the task.","To create a cron job with a specific command to execute, use the following command:

```bash
kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```",The `image` parameter when creating a cron job specifies the image containing the software needed to run the application. It determines what software environment the job will run in.,A cron job scheduled with '*/1 * * * *' will execute every minute.,"The `--` in the cron job creation command signifies the end of command options. After `--`, any following arguments are considered as command and arguments that you want the container to run.","To create a job in Kubernetes, use the following command structure:

```bash
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```","To specify an image for the job you are creating, use the following command:

```bash
kubectl create job NAME --image=image
```","To create a job based on an existing cron job, use the following command:

```bash
kubectl create job test-job --from=cronjob/a-cronjob
```","To specify a particular command for the job to execute, you can use the following syntax:

```bash
kubectl create job my-job --image=busybox -- date
```","The `--from` flag is used to create a job from an existing cron job. For example, to create a job from a cron job named ""a-cronjob"", you would use the following command:

```bash
kubectl create job test-job --from=cronjob/a-cronjob
```","The `--tcp` flag when creating a ClusterIP service is used to specify the port mappings for the service. You define the external port and the target port for traffic. For example, using `--tcp=5678:8080` maps port 5678 on the service to port 8080 on the pods.","To specify a ClusterIP service to run in headless mode, use the following command:

```bash
kubectl create service clusterip my-cs --clusterip=""None""
```","The `--dry-run` option, when specified, allows you to simulate the creation of a ClusterIP service without actually making any changes. It can be set to `server`, `client`, or `none` to indicate where the dry run should be executed.","To create a role that allows getting, watching, and listing pods in Kubernetes, use the following command:

```bash
kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
```","To specify multiple resource names for a role in Kubernetes, use the following command:

```bash
kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
```","To create a role in Kubernetes with actions specified for a subresource, use the following command:

```bash
kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
```",The primary purpose of using a role binding in Kubernetes is to grant the permissions defined in a role to a set of users in a specific namespace.,"To specify multiple users and groups when creating a role binding, use the following command:

```bash
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME --user=user1 --user=user2 --group=group1 --group=group2
```

For example, to create a role binding for `user1`, `user2`, and `group1`, use:

```bash
kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
```","The difference between using `--role=NAME` and `--clusterrole=NAME` when creating a role binding lies in the scope of the permissions being granted:

- `--role=NAME`: This is used to bind a role that defines permission rules within a specific namespace. It grants the permissions defined in a role to a set of users within that particular namespace.

- `--clusterrole=NAME`: This is used to bind a cluster role that defines permission rules cluster-wide. It grants the permissions defined in a role to a set of users across the entire cluster, regardless of namespace boundaries.","To create a role binding specifically for a service account in Kubernetes, use the following command:

```bash
kubectl create rolebinding ROLEBINDING_NAME --role=ROLE_NAME --serviceaccount=NAMESPACE:SERVICEACCOUNT_NAME
```

Example:

```bash
# Create a role binding for service account monitoring:sa-dev using the admin role
kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","The `--dry-run` option in the role binding command allows you to preview the actions that would be taken by the command without actually executing them. You can specify `server`, `client`, or `none`:

- `server`: The request is sent to the server with the dry run flag to get a preview of the operation.
- `client`: The resources are processed on the client without contacting the server.
- `none`: Executes the command fully without dry run (default behavior).","To create a Dockercfg secret in Kubernetes is to provide nodes with the credentials needed to authenticate against Docker registries. This is necessary because when creating applications, you may have a Docker registry that requires authentication for pulling images. By creating a Dockercfg secret and attaching it to your service account, you ensure that the nodes can pull images on your behalf using the provided credentials, which are produced by using the Docker command line to login and generate a `.dockercfg` file.","To authenticate to a Docker registry using the Docker command line, run the following command:

```bash
docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL
```","To create a docker-registry secret using kubectl, you need the following information:

- Docker Registry Server
- Docker Username
- Docker Password
- (Optional) Docker Email

You can create the secret with this command:

```bash
kubectl create secret docker-registry NAME --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL [--docker-server=DOCKER_REGISTRY_SERVER]
```

Here's an example:

```bash
# If you do not already have a .dockercfg file, create a dockercfg secret directly
kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
```

Additionally, you can create the secret from an existing `.docker/config.json` file:

```bash
# Create a new secret named my-secret from ~/.docker/config.json
kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","No, the email address is optional when authenticating to a Docker registry or creating a Docker secret.","To create a Kubernetes secret from an existing `.docker/config.json` file, use the following command:

```bash
kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```",Create a NodePort service with the specified name.,"In the given example command, `--tcp=5678:8080` specifies a TCP port mapping, where `5678` is the port exposed by the service and `8080` is the target port on the container. This means that traffic sent to port `5678` on the service will be forwarded to port `8080` on the containers.","The '--dry-run' option when creating a NodePort service is used to simulate the command without actually making any changes. It can have the values 'server', 'client', or 'none', indicating where the dry run is executed.",my-ns,"To specify a target port when creating a NodePort service using kubectl, use the following command:

```bash
kubectl create service nodeport NAME --tcp=port:targetPort
```

For example:

```bash
kubectl create service nodeport my-ns --tcp=5678:8080
```",Create a service account with the specified name.,"To create a service account with a specific name in Kubernetes, use the following command:

```bash
kubectl create serviceaccount NAME
```

Example:

```bash
kubectl create serviceaccount my-service-account
```","The `--dry-run` option in Kubernetes is used to simulate the creation of a service account without actually making any changes to the cluster. It helps in verifying what the resource creation would look like. There are different modes for the `--dry-run` option:

- `server`: The server will validate the request without persisting it.
- `client`: The request is only processed by the client's configuration without sending it to the server.
- `none`: This option won't perform a dry run and will create the resource directly.

Here is the command to create a service account with the `--dry-run` option:

```bash
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```","To create a service account named 'my-service-account', use the following command:

```bash
kubectl create serviceaccount my-service-account
```","The possible values for the `--dry-run` option when creating a service account are `server`, `client`, and `none`.","Specifying multiple verbs in the cluster role creation command allows you to define a set of actions (such as ""get"", ""list"", ""watch"") that the cluster role is permitted to perform on the specified resources. This enables fine-grained control over what operations users are authorized to execute on particular Kubernetes resources when using the created cluster role.","To create a cluster role that allows accessing specific pod resources by name, use the following command:

```bash
kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
```","Specifying a NonResourceURL in a cluster role allows you to create permissions for accessing non-resource endpoints, such as custom URLs like `/logs/`. This means you can grant users the ability to perform actions on URLs that are not associated with Kubernetes resources. 

Example command to create a cluster role with NonResourceURL specified:

```bash
kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
```",Specifying an AggregationRule when creating a cluster role allows you to automatically aggregate multiple roles into the cluster role based on certain criteria. This means that the cluster role can dynamically include permissions from other roles that are marked with specific labels that match the aggregation rule. This helps in managing permissions more efficiently by allowing a cluster role to automatically incorporate updates from other roles without manually changing the cluster role itself.,Specifying an API Group when creating a cluster role allows you to define which sets of related paths in the Kubernetes API the role has permissions to access. This makes it easier to extend the Kubernetes API and manage access control for various resources grouped under the same API group.,JSON and YAML formats are accepted for creating a resource with kubectl.,"To create a pod using information from a file, use the following command:

```bash
kubectl create -f FILENAME
```","To create a pod using data passed through stdin, use the following command:

```bash
cat pod.json | kubectl create -f -
```","To edit a YAML file in JSON format before creating a resource, use the following command:

```bash
kubectl create -f FILENAME.yaml --edit -o json
```","To display the supported API versions in Kubernetes, use the following command:

```bash
kubectl api-versions
```","To print the supported API versions on the server, use the following command:

```bash
kubectl api-versions
```

The supported API versions are displayed in the format of ""group/version"".","No, there isn't a JSON output format available for listing supported API versions using `kubectl api-versions`. The command simply outputs the supported API versions in the form of ""group/version"".","To execute a command in a specific container within a pod, use the following `kubectl exec` command:

```bash
kubectl exec POD_NAME -c CONTAINER_NAME -- COMMAND [args...]
```

Replace `POD_NAME` with your pod's name, `CONTAINER_NAME` with your container's name, and `COMMAND [args...]` with the command and arguments you wish to execute.","Context:
title: kubectl exec
answer: 
`kubectl exec` allows you to execute commands in a container in a pod. You can use it to run commands inside a pod's container for debugging purposes or to check the state of an application running in the pod.

The `-c` option is used to specify the name of the container to execute the command in if the pod has multiple containers. This is necessary when you have multiple containers running in a single pod, and you want to target a specific one for the command execution.

In contrast, if the pod has only one container, you don't need to specify the container name.

kubectl exec example Usage:
```bash
kubectl exec <pod-name> -c <container-name> -- <command>
```
The above command will execute a specified command inside the specified container of a running pod. If you omit the `-c <container-name>`, the command is executed in the one and only container if the pod has a single container.","To open an interactive shell in a container using `kubectl exec`, you can use the following command:

```bash
kubectl exec (POD | TYPE/NAME) -c CONTAINER -i -t -- bash -il
```

For example, to switch to a raw terminal mode and open a bash shell in the `ruby-container` from pod `mypod`, use:

```bash
kubectl exec mypod -c ruby-container -i -t -- bash -il
```","To apply a YAML file in Kubernetes, use the following command:

```bash
kubectl apply -f FILENAME.yaml
```","To execute a command in a pod from a deployment or service using `kubectl exec`, use the following commands:

```bash
# Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
kubectl exec deploy/mydeployment -- date

# Get output from running 'date' command from the first pod of the service myservice, using the first container by default
kubectl exec svc/myservice -- date
```","A label key and value must begin with a letter or number and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each. Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.","The `--overwrite` flag allows existing labels on a resource to be overwritten. Without specifying this flag, attempting to overwrite a label will result in an error.","If you use the --resource-version flag while updating labels on a resource, the updates will use this specified resource version. If the version on the server does not match the resource version you provide, the update will fail.","To update all pods in a namespace with a specific label, use the following command:

```bash
kubectl label pods --all KEY=VALUE
```","To remove a specific label from a pod without using the --overwrite flag, use the following command:

```bash
kubectl label pods POD_NAME LABEL_NAME-
```","To list all pods with detailed information, such as the node name, in Kubernetes, use the following command:

```bash
kubectl describe pods
```","To specify which attributes of a fetched resource should be displayed using a template, you can use the following command:

```bash
kubectl get TYPE NAME -o go-template='{{.spec.field}}'
```

Replace `TYPE` with the type of resource, `NAME` with the name of the resource, and `.spec.field` with the specific field you want to display.","To list all deployments in the 'v1' version of the 'apps' API group in JSON format, use the following command:

```bash
kubectl get deployments.v1.apps -o json
```","To list resources in a specific namespace, include the namespace in the command as follows:

```bash
kubectl get resources -n NAMESPACE_NAME
```","To display the phase value of a specific pod using the `kubectl get` command, use the following command:

```bash
kubectl get -o template pod/POD_NAME --template={{.status.phase}}
```","When deleting resources in Kubernetes, you can specify the following different arguments:

- File names
- Standard input (stdin)
- Resource types and names
- Resources and label selectors

Here are some examples of deletion commands:

```bash
# Delete a resource using a configuration file
kubectl delete -f FILENAME.yaml

# Delete resources from a directory with kustomization.yaml
kubectl delete -k DIRECTORY

# Delete all resources of a specific type
kubectl delete TYPE --all 

# Delete specific resources by type and name
kubectl delete TYPE NAME

# Delete resources by label selector
kubectl delete TYPE -l label=VALUE

# Use stdin to delete a resource
cat FILENAME.yaml | kubectl delete -f -
```

Remember, only one type of argument can be specified for each delete command.","The `--grace-period` flag, when deleting resources, overrides the default grace period before resources are forcibly terminated. However, it may be ignored for resources that do not support graceful deletion. In such cases, specifying a `--grace-period` has no effect.","If you force delete a pod, it does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or communicate with a remote API and depend on the pod's name for identification, force deleting those pods may result in multiple processes running on different machines using the same identification. This can lead to data corruption or inconsistency. Therefore, you should only force delete pods when you are certain that the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once.","To delete resources from a directory containing a kustomization.yaml file, use the following command:

```bash
kubectl delete -k DIRECTORY
```","If someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.","To view the resource usage of pods in Kubernetes, you can use the following command:

```bash
kubectl top pod
```","To view the resource consumption of a specific pod by its name, use the following syntax:

```bash
kubectl top pod POD_NAME
```","To display metrics for pods that are selected using a specific label, use the following command:

```bash
kubectl top pod -l name=myLabel
```","Due to the metrics pipeline delay, resource usage metrics for newly created pods may be unavailable for a few minutes.","To view the resource usage metrics for all pods within a specific namespace in Kubernetes, use the following command:

```bash
kubectl top pod --namespace=NAMESPACE
```","Display resource (CPU/memory) usage of nodes.

The top-node command allows you to see the resource consumption of nodes.

```bash
kubectl top node [NAME | -l label]
```

```bash
# Show metrics for all nodes
kubectl top node

# Show metrics for a given node
kubectl top node NODE_NAME
```","To display the resource usage for all nodes using kubectl, use the following command:

```bash
kubectl top node
```","To use the `kubectl top node NODE_NAME` command, you can view the resource (CPU/memory) usage of a specific node.","To apply a YAML file in Kubernetes, use the following command:

```bash
kubectl apply -f FILENAME.yaml
```

Context:
title: kubectl top
answer: Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```

title: kubectl top node
answer: Display resource (CPU/memory) usage of nodes.

 The top-node command allows you to see the resource consumption of nodes.

```
kubectl top node [NAME | -l label]
```
``` 

```bash
# Show metrics for all nodes
kubectl top node

# Show metrics for a given node
kubectl top node NODE_NAME
```
  

title: kubectl top pod
answer: Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```bash
# Show metrics for all pods in the default namespace
kubectl top pod

# Show metrics for all pods in the given namespace
kubectl top pod --namespace=NAMESPACE

# Show metrics for a given pod and its containers
kubectl top pod POD_NAME --containers

# Show metrics for the pods defined by label name=myLabel
kubectl top pod -l name=myLabel
```

title: kubectl Commands
answer: 

title: kubectl
answer: kubectl controls the Kubernetes cluster manager.

Find more information in [Command line tool](/docs/reference/kubectl/) (`kubectl`).

```shell
kubectl [flags]
```
User's Question:
What is the primary function of the 'kubectl top' command in Kubernetes?

Answer: 

The primary function of the 'kubectl top' command in Kubernetes is to display resource (CPU/memory) usage.","The resource metrics provided by Kubernetes may differ from those offered by OS tools like 'top' because the metrics in Kubernetes are specifically optimized to provide a stable signal for autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). The design goal of these metrics is to support Kubernetes autoscalers rather than for pinpoint accuracy, whereas OS tools like 'top' are designed for detailed monitoring of system resource usage.","For the `kubectl top` command to operate correctly, the Metrics Server must be installed and running in the Kubernetes cluster.","To use the 'kubectl top' command in Kubernetes, a user might consider the following situations:

- For on-the-fly spot-checks of resource usage, such as identifying which pods are consuming the most resources at a glance or getting a quick sense of the load on your nodes.
- To understand current resource consumption patterns.
- To validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.","The 'kubectl top' command is not suitable for high-accuracy reporting or alerting in Kubernetes environments because its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. It is intended for on-the-fly spot-checks of resource usage and to understand current resource consumption patterns. For high-accuracy reporting, historical analysis, dashboarding, or alerting, a dedicated monitoring solution should be used.","Specifying preconditions with the scale command in Kubernetes helps ensure that certain conditions are met before the scaling action is attempted. For example, using `--current-replicas` or `--resource-version` as a precondition ensures that it is validated before the scale is attempted, guaranteeing that the specified condition holds true when the scale request is sent to the server.","To scale a replica set named 'foo' to 3 replicas using kubectl, use the following command:

```bash
kubectl scale --replicas=3 rs/foo
```","To scale a Kubernetes resource specified in a file named 'foo.yaml', use the following command:

```bash
kubectl scale --replicas=3 -f foo.yaml
```","To ensure that a deployment is only scaled if its current replica count is a specific number, you can use the `--current-replicas` option with the `kubectl scale` command. For example, if you want to scale a deployment named `mysql` to 3 replicas only if its current size is 2, you can run the following command:

```bash
kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
```","To scale multiple replication controllers at once, you can use the following `kubectl scale` command:

```bash
kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
```","A taint in Kubernetes consists of three components: key, value, and effect. It is expressed as key=value:effect. The key starts with a letter or number and can include letters, numbers, hyphens, dots, and underscores, up to 253 characters. Optionally, the key can start with a DNS subdomain prefix and a single '/', like example.com/my-app. The value is optional, starts with a letter or number, and can have letters, numbers, hyphens, dots, and underscores, up to 63 characters. The effect must be one of NoSchedule, PreferNoSchedule, or NoExecute.","A taint consists of a key, value, and effect. The rules for formatting the key and value are as follows:

- The key must begin with a letter or number and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
- Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
- The value is optional. If given, it must begin with a letter or number and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
- The effect must be NoSchedule, PreferNoSchedule, or NoExecute.","In Kubernetes, the allowed taint effects are:

- **NoSchedule**: Pods that do not tolerate this taint are not scheduled onto the node.
- **PreferNoSchedule**: The scheduler tries to avoid placing Pods that do not tolerate this taint on the node, but it is not a strict requirement.
- **NoExecute**: Pods that do not tolerate this taint are evicted from the node if they are already running and not scheduled onto the node if they are not running.","To remove a taint from a node if it matches a specific key and effect, use the following command:

```bash
kubectl taint nodes NODE_NAME KEY:TAINT_EFFECT-
```

For example, to remove a taint with key 'dedicated' and effect 'NoSchedule' from node 'foo', use:

```bash
kubectl taint nodes foo dedicated:NoSchedule-
```",Taints can currently only be applied to nodes in Kubernetes.,"To roll back to the previous deployment in Kubernetes, use the following command:

```bash
kubectl rollout undo deployment/abc
```","To roll back a daemonset to a specific revision, such as revision 3, use the following command:

```bash
kubectl rollout undo daemonset/abc --to-revision=3
```","The `--dry-run=server` flag simulates the rollback of a deployment without making any actual changes. It performs all validation checks necessary for the rollout to occur as though it is processed on the server, but does not actually execute the rollback.","Resuming a paused resource in Kubernetes allows it to be reconciled again by a controller. Paused resources are not reconciled until they are resumed. Currently, only deployments support being resumed.","To apply a YAML file in Kubernetes, use the following command:

```bash
kubectl apply -f FILENAME.yaml
```

Only deployments support being resumed after being paused.","To resume a paused deployment named 'nginx', use the following command:

```bash
kubectl rollout resume deployment/nginx
```","To apply a YAML file in Kubernetes, use the following command:

```bash
kubectl rollout restart RESOURCE
```

Restart a resource. Resource rollout will be restarted.","To restart all deployments within a specific namespace, use the following command:

```bash
kubectl rollout restart deployment -n NAMESPACE_NAME
```","To restart a specific deployment named 'nginx', use the following command:

```bash
kubectl rollout restart deployment/nginx
```","To restart a daemon set using kubectl, use the following command:

```bash
kubectl rollout restart daemonset/abc
```","To restart deployments that have a specific label, such as 'app=nginx', use the following command:

```bash
kubectl rollout restart deployment --selector=app=nginx
```","Show the status of the rollout.

By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```bash
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```bash
# Watch the rollout status of a deployment
kubectl rollout status deployment/nginx
```","To avoid waiting for the rollout to complete using the 'rollout status' command, use the `--watch=false` flag:

```bash
kubectl rollout status (TYPE NAME | TYPE/NAME) --watch=false
```","If a new rollout starts in-between while 'rollout status' is monitoring the previous one, it will continue watching the latest revision.","To watch a specific revision of a rollout and terminate if it gets replaced by another, use the `kubectl rollout status` command with the `--revision` flag:

```bash
kubectl rollout status TYPE/NAME --revision=N
```

Replace `TYPE/NAME` with the appropriate resource type and name, and `N` with the revision number you want to watch.","To check the rollout status of a deployment named 'nginx', use the following command:

```bash
kubectl rollout status deployment/nginx
```","Marking a Kubernetes resource as paused means that the resource will not be reconciled by a controller. This is achieved using the `kubectl rollout pause` command. Currently, only deployments support being paused. When a resource is paused, any current state of the deployment will continue its function, but new updates to the deployment will not have an effect as long as it remains paused. To resume the reconciliation, the `kubectl rollout resume` command is used.","Currently, only deployments support being paused in Kubernetes.","To resume a paused resource in Kubernetes, use the following command:

```bash
kubectl rollout resume RESOURCE
```

For example, to resume an already paused deployment named ""nginx"":

```bash
kubectl rollout resume deployment/nginx
```","Yes, a paused deployment will continue its current operations. Any current state of the deployment will continue its function; however, new updates to the deployment will not have an effect as long as the deployment is paused. To mark the deployment as paused, you can use the following command:

```bash
kubectl rollout pause deployment/nginx
```","Updating a paused deployment will have no effect as long as the deployment remains paused. Any current state of the deployment will continue to function, but new updates to the deployment will not be applied until the deployment is resumed.","Valid resource types that can be managed using the `kubectl rollout` command in Kubernetes include:

- deployments
- daemonsets
- statefulsets","To roll back to a previous deployment using the rollout command, use the following command:

```bash
kubectl rollout undo deployment/abc
```","To check the rollout status of a daemonset called 'foo', use the following command:

```bash
kubectl rollout status daemonset/foo
```","To restart a specific deployment called 'abc', use the following command:

```bash
kubectl rollout restart deployment/abc
```","To restart all deployments labeled 'app=nginx', use the following command:

```bash
kubectl rollout restart deployment --selector=app=nginx
```","To view the rollout history of a specific deployment, use the following command:

```bash
kubectl rollout history deployment/abc
```","To view the details of a specific revision of a daemonset, use the following command:

```bash
kubectl rollout history daemonset/abc --revision=3
```","To check the rollout history of Kubernetes resources, use the following command:

```bash
kubectl rollout history (TYPE NAME | TYPE/NAME) [flags]
```","Set the latest last-applied-configuration annotations by setting it to match the contents of a file. This results in the last-applied-configuration being updated as though 'kubectl apply -f FILENAME' was run, without updating any other parts of the object.

```bash
kubectl apply set-last-applied -f FILENAME
```","To update the last-applied-configuration for multiple files within a directory, use the following command:

```bash
kubectl apply set-last-applied -f path/
```","If the last-applied-configuration annotation does not exist when using `kubectl apply set-last-applied`, the annotation will be created to match the contents of the file. You can ensure this by using the `--create-annotation=true` flag.","In the event an error occurs while updating using the edit-last-applied command, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.","To specify that edits should be made in JSON format rather than the default YAML, use the `-o json` option with the command.

For example, when using `kubectl apply edit-last-applied`, you can edit in JSON with:

```bash
kubectl apply edit-last-applied -f FILENAME -o json
```

Similarly, for `kubectl edit`, you can do:

```bash
kubectl edit RESOURCE_NAME -o json
```","In the event another editor changes a resource on the server while you are editing it, a temporary file will be created on disk that contains your unapplied changes. You will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.","The `--windows-line-endings` flag can be used to force Windows line endings when using the `edit-last-applied` command. If this flag is not used, the default line endings for your operating system will be applied.","To choose which editor to use when running the `edit-last-applied` command, the user can set the `KUBE_EDITOR` or `EDITOR` environment variables. If these variables are not set, the command will default to using 'vi' for Linux or 'notepad' for Windows.",The default output format for viewing last-applied-configuration annotations is YAML.,"To change the output format when viewing last-applied-configuration annotations, use the `-o` option with the desired format. For example:

```bash
kubectl apply view-last-applied -f deploy.yaml -o json
```","To view the last-applied-configuration annotations for a specific deployment by name, use the following command:

```bash
kubectl apply view-last-applied deployment/nginx
```","To view the last-applied-configuration annotations from a file in JSON format, use the following command:

```bash
kubectl apply view-last-applied -f deploy.yaml -o json
```","To specify which annotations to view when using the `kubectl apply view-last-applied` command, you can use the following options:

- By type/name:
  
  ```bash
  kubectl apply view-last-applied TYPE/NAME
  ```

- By file:

  ```bash
  kubectl apply view-last-applied -f FILENAME
  ```

You can also specify the output format, with the default being YAML. To change the output to JSON, use the `-o json` option.",JSON and YAML formats are accepted for applying configurations to a Kubernetes resource.,"To ensure the resource is properly configured before using the `apply` command, always create the resource initially with either `apply` or `create --save-config`.","To apply resources from a directory containing `kustomization.yaml`, use the following command:

```bash
kubectl apply -k DIRECTORY/
```","The `--prune` flag is used to apply configurations such that it deletes resources that are not defined in the configuration files. It is particularly used to manage and clean up resources by ensuring that only the resources defined in the files are present in the cluster.

Caution advised: The `--prune` functionality is still in Alpha and not yet complete. It should be used only if you are aware of the current limitations and behaviors.","To apply a configuration in Kubernetes that matches specific labels and delete others that do not match, use the following command with the `--prune` option:

```bash
kubectl apply --prune -f manifest.yaml -l app=nginx
```"
"Tags objects with identifying attributes that are meaningful and relevant to users.

<!--more--> 

Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.","Tags objects with identifying attributes that are meaningful and relevant to users.

<!--more--> 

Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.","Tags objects with identifying attributes that are meaningful and relevant to users.

<!--more--> 

Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.","Tags objects with identifying attributes that are meaningful and relevant to users.

<!--more--> 

Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.","Tags objects with identifying attributes that are meaningful and relevant to users.

<!--more--> 

Labels are key/value pairs that are attached to objects such as . They are used to organize and to select subsets of objects.","A check that the  periodically performs against a container that is 
running in a pod, that will define container's state and health and informing container's lifecycle.

<!--more-->
 
To learn more, read [container probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).","A check that the  periodically performs against a container that is 
running in a pod, that will define container's state and health and informing container's lifecycle.

<!--more-->
 
To learn more, read [container probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).","A check that the  periodically performs against a container that is 
running in a pod, that will define container's state and health and informing container's lifecycle.

<!--more-->
 
To learn more, read [container probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).","Constraints resource consumption per  or ,
specified for a particular .

<!--more--> 

A [LimitRange](/docs/concepts/policy/limit-range/) either limits the quantity of 
that can be created (for a particular resource type),
or the amount of 
that may be requested/consumed by individual containers or Pods within a namespace.","Constraints resource consumption per  or ,
specified for a particular .

<!--more--> 

A [LimitRange](/docs/concepts/policy/limit-range/) either limits the quantity of 
that can be created (for a particular resource type),
or the amount of 
that may be requested/consumed by individual containers or Pods within a namespace.","Constraints resource consumption per  or ,
specified for a particular .

<!--more--> 

A [LimitRange](/docs/concepts/policy/limit-range/) either limits the quantity of 
that can be created (for a particular resource type),
or the amount of 
that may be requested/consumed by individual containers or Pods within a namespace.","Constraints resource consumption per  or ,
specified for a particular .

<!--more--> 

A [LimitRange](/docs/concepts/policy/limit-range/) either limits the quantity of 
that can be created (for a particular resource type),
or the amount of 
that may be requested/consumed by individual containers or Pods within a namespace.","A Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
 like hardware
accelerators.

<!--more-->

With DRA, device drivers and cluster admins define device _classes_ that are
available to _claim_ in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.","A Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
 like hardware
accelerators.

<!--more-->

With DRA, device drivers and cluster admins define device _classes_ that are
available to _claim_ in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.","A Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
 like hardware
accelerators.

<!--more-->

With DRA, device drivers and cluster admins define device _classes_ that are
available to _claim_ in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.","A person who customizes the Kubernetes platform to fit the needs of their project.

<!--more--> 

A platform developer may, for example, use [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/) or
[Extend the Kubernetes API with the aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)
to add functionality to their instance of Kubernetes, specifically for their application.
Some Platform Developers are also  and
develop extensions which are contributed to the Kubernetes community.
Others develop closed-source commercial or site-specific extensions.","A person who customizes the Kubernetes platform to fit the needs of their project.

<!--more--> 

A platform developer may, for example, use [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/) or
[Extend the Kubernetes API with the aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)
to add functionality to their instance of Kubernetes, specifically for their application.
Some Platform Developers are also  and
develop extensions which are contributed to the Kubernetes community.
Others develop closed-source commercial or site-specific extensions.","A person who customizes the Kubernetes platform to fit the needs of their project.

<!--more--> 

A platform developer may, for example, use [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/) or
[Extend the Kubernetes API with the aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)
to add functionality to their instance of Kubernetes, specifically for their application.
Some Platform Developers are also  and
develop extensions which are contributed to the Kubernetes community.
Others develop closed-source commercial or site-specific extensions.","A person who reviews code for quality and correctness on some part of the project.

<!--more--> 

Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.","A person who reviews code for quality and correctness on some part of the project.

<!--more--> 

Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.","A person who reviews code for quality and correctness on some part of the project.

<!--more--> 

Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.","A person who reviews code for quality and correctness on some part of the project.

<!--more--> 

Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.","A person who reviews code for quality and correctness on some part of the project.

<!--more--> 

Reviewers are knowledgeable about both the codebase and software engineering principles. Reviewer status is scoped to a part of the codebase.","A kind of  that defines a new custom API to add
to your Kubernetes API server, without building a complete custom server.

<!--more-->

CustomResourceDefinitions let you extend the Kubernetes API for your environment if the built-in
 can't meet your needs.","A kind of  that defines a new custom API to add
to your Kubernetes API server, without building a complete custom server.

<!--more-->

CustomResourceDefinitions let you extend the Kubernetes API for your environment if the built-in
 can't meet your needs.","A kind of  that defines a new custom API to add
to your Kubernetes API server, without building a complete custom server.

<!--more-->

CustomResourceDefinitions let you extend the Kubernetes API for your environment if the built-in
 can't meet your needs.","May refer to: core Kubernetes or the source repo from which a repo was forked.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *upstream* to mean the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, [community members](#term-member) may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.
* In **GitHub** or **git**: The convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","May refer to: core Kubernetes or the source repo from which a repo was forked.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *upstream* to mean the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, [community members](#term-member) may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.
* In **GitHub** or **git**: The convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","May refer to: core Kubernetes or the source repo from which a repo was forked.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *upstream* to mean the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, [community members](#term-member) may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.
* In **GitHub** or **git**: The convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","May refer to: core Kubernetes or the source repo from which a repo was forked.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *upstream* to mean the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, [community members](#term-member) may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.
* In **GitHub** or **git**: The convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","May refer to: core Kubernetes or the source repo from which a repo was forked.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *upstream* to mean the core Kubernetes codebase, which the general ecosystem, other code, or third-party tools rely upon. For example, [community members](#term-member) may suggest that a feature is moved upstream so that it is in the core codebase instead of in a plugin or third-party tool.
* In **GitHub** or **git**: The convention is to refer to a source repo as *upstream*, whereas the forked repo is considered *downstream*.","A former extension API that enabled applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

<!--more--> 

It provided a way to list, provision, and bind with external  without needing detailed knowledge about how those services would be created or managed.","A former extension API that enabled applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

<!--more--> 

It provided a way to list, provision, and bind with external  without needing detailed knowledge about how those services would be created or managed.","A former extension API that enabled applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

<!--more--> 

It provided a way to list, provision, and bind with external  without needing detailed knowledge about how those services would be created or managed.","A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.

<!--more-->

Admission controllers are configurable for the Kubernetes API server and may be ""validating"", ""mutating"", or
both. Any admission controller may reject the request. Mutating controllers may modify the objects they admit;
validating controllers may not.

* [Admission controllers in the Kubernetes documentation](/docs/reference/access-authn-authz/admission-controllers/)","A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.

<!--more-->

Admission controllers are configurable for the Kubernetes API server and may be ""validating"", ""mutating"", or
both. Any admission controller may reject the request. Mutating controllers may modify the objects they admit;
validating controllers may not.

* [Admission controllers in the Kubernetes documentation](/docs/reference/access-authn-authz/admission-controllers/)","A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.

<!--more-->

Admission controllers are configurable for the Kubernetes API server and may be ""validating"", ""mutating"", or
both. Any admission controller may reject the request. Mutating controllers may modify the objects they admit;
validating controllers may not.

* [Admission controllers in the Kubernetes documentation](/docs/reference/access-authn-authz/admission-controllers/)","A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.

<!--more-->

Admission controllers are configurable for the Kubernetes API server and may be ""validating"", ""mutating"", or
both. Any admission controller may reject the request. Mutating controllers may modify the objects they admit;
validating controllers may not.

* [Admission controllers in the Kubernetes documentation](/docs/reference/access-authn-authz/admission-controllers/)","A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.

<!--more-->

Admission controllers are configurable for the Kubernetes API server and may be ""validating"", ""mutating"", or
both. Any admission controller may reject the request. Mutating controllers may modify the objects they admit;
validating controllers may not.

* [Admission controllers in the Kubernetes documentation](/docs/reference/access-authn-authz/admission-controllers/)","The work involved in managing a Kubernetes cluster: managing
day-to-day operations, and co-ordinating upgrades.

<!--more-->

 Examples of cluster operations work include: deploying new Nodes to
scale the cluster; performing software upgrades; implementing security
controls; adding or removing storage; configuring cluster networking;
managing cluster-wide observability; and responding to events.","The work involved in managing a Kubernetes cluster: managing
day-to-day operations, and co-ordinating upgrades.

<!--more-->

 Examples of cluster operations work include: deploying new Nodes to
scale the cluster; performing software upgrades; implementing security
controls; adding or removing storage; configuring cluster networking;
managing cluster-wide observability; and responding to events.","The work involved in managing a Kubernetes cluster: managing
day-to-day operations, and co-ordinating upgrades.

<!--more-->

 Examples of cluster operations work include: deploying new Nodes to
scale the cluster; performing software upgrades; implementing security
controls; adding or removing storage; configuring cluster networking;
managing cluster-wide observability; and responding to events.","The work involved in managing a Kubernetes cluster: managing
day-to-day operations, and co-ordinating upgrades.

<!--more-->

 Examples of cluster operations work include: deploying new Nodes to
scale the cluster; performing software upgrades; implementing security
controls; adding or removing storage; configuring cluster networking;
managing cluster-wide observability; and responding to events.","The work involved in managing a Kubernetes cluster: managing
day-to-day operations, and co-ordinating upgrades.

<!--more-->

 Examples of cluster operations work include: deploying new Nodes to
scale the cluster; performing software upgrades; implementing security
controls; adding or removing storage; configuring cluster networking;
managing cluster-wide observability; and responding to events.","Claims storage  defined in a
, so that the storage can be mounted as
a volume in a .

<!--more--> 

Specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive) and how it is reclaimed (retained, recycled or deleted). Details of the storage itself are described in the PersistentVolume object.","Claims storage  defined in a
, so that the storage can be mounted as
a volume in a .

<!--more--> 

Specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive) and how it is reclaimed (retained, recycled or deleted). Details of the storage itself are described in the PersistentVolume object.","Claims storage  defined in a
, so that the storage can be mounted as
a volume in a .

<!--more--> 

Specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive) and how it is reclaimed (retained, recycled or deleted). Details of the storage itself are described in the PersistentVolume object.","Claims storage  defined in a
, so that the storage can be mounted as
a volume in a .

<!--more--> 

Specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive) and how it is reclaimed (retained, recycled or deleted). Details of the storage itself are described in the PersistentVolume object.","Claims storage  defined in a
, so that the storage can be mounted as
a volume in a .

<!--more--> 

Specifies the amount of storage, how the storage will be accessed (read-only, read-write and/or exclusive) and how it is reclaimed (retained, recycled or deleted). Details of the storage itself are described in the PersistentVolume object.","Someone who donates code, documentation, or their time to help the Kubernetes project or community.

<!--more--> 

Contributions include pull requests (PRs), issues, feedback,  participation, or organizing community events.","Someone who donates code, documentation, or their time to help the Kubernetes project or community.

<!--more--> 

Contributions include pull requests (PRs), issues, feedback,  participation, or organizing community events.","Someone who donates code, documentation, or their time to help the Kubernetes project or community.

<!--more--> 

Contributions include pull requests (PRs), issues, feedback,  participation, or organizing community events.","Someone who donates code, documentation, or their time to help the Kubernetes project or community.

<!--more--> 

Contributions include pull requests (PRs), issues, feedback,  participation, or organizing community events.","Someone who donates code, documentation, or their time to help the Kubernetes project or community.

<!--more--> 

Contributions include pull requests (PRs), issues, feedback,  participation, or organizing community events.","An endpoint of a  is one of the  (or external servers) that implements the Service.

<!--more-->
For Services with ,
the EndpointSlice controller will automatically create one or more {{<
glossary_tooltip text=""EndpointSlices"" term_id=""endpoint-slice"" >}} giving the
IP addresses of the selected endpoint Pods.

EndpointSlices can also be created manually to indicate the endpoints of
Services that have no selector specified.","An endpoint of a  is one of the  (or external servers) that implements the Service.

<!--more-->
For Services with ,
the EndpointSlice controller will automatically create one or more {{<
glossary_tooltip text=""EndpointSlices"" term_id=""endpoint-slice"" >}} giving the
IP addresses of the selected endpoint Pods.

EndpointSlices can also be created manually to indicate the endpoints of
Services that have no selector specified.","An endpoint of a  is one of the  (or external servers) that implements the Service.

<!--more-->
For Services with ,
the EndpointSlice controller will automatically create one or more {{<
glossary_tooltip text=""EndpointSlices"" term_id=""endpoint-slice"" >}} giving the
IP addresses of the selected endpoint Pods.

EndpointSlices can also be created manually to indicate the endpoints of
Services that have no selector specified.","Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

<!--more-->

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.
 are a good example of immutable infrastructure because persistent changes to containers
can only be made by creating a new version of the container or recreating the existing container from its image.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. 
Operating such a system becomes a lot more straightforward because administrators can make assumptions about it.
After all, they know no one made mistakes or changes they forgot to communicate.
Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed
to create infrastructure is stored in version control (such as Git).
This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

<!--more-->

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.
 are a good example of immutable infrastructure because persistent changes to containers
can only be made by creating a new version of the container or recreating the existing container from its image.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. 
Operating such a system becomes a lot more straightforward because administrators can make assumptions about it.
After all, they know no one made mistakes or changes they forgot to communicate.
Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed
to create infrastructure is stored in version control (such as Git).
This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

<!--more-->

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.
 are a good example of immutable infrastructure because persistent changes to containers
can only be made by creating a new version of the container or recreating the existing container from its image.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. 
Operating such a system becomes a lot more straightforward because administrators can make assumptions about it.
After all, they know no one made mistakes or changes they forgot to communicate.
Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed
to create infrastructure is stored in version control (such as Git).
This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

<!--more-->

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.
 are a good example of immutable infrastructure because persistent changes to containers
can only be made by creating a new version of the container or recreating the existing container from its image.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. 
Operating such a system becomes a lot more straightforward because administrators can make assumptions about it.
After all, they know no one made mistakes or changes they forgot to communicate.
Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed
to create infrastructure is stored in version control (such as Git).
This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","Immutable Infrastructure refers to computer infrastructure (virtual machines, containers, network appliances) that cannot be changed once deployed.

<!--more-->

Immutability can be enforced by an automated process that overwrites unauthorized changes or through a system that won’t allow changes in the first place.
 are a good example of immutable infrastructure because persistent changes to containers
can only be made by creating a new version of the container or recreating the existing container from its image.

By preventing or identifying unauthorized changes, immutable infrastructures make it easier to identify and mitigate security risks. 
Operating such a system becomes a lot more straightforward because administrators can make assumptions about it.
After all, they know no one made mistakes or changes they forgot to communicate.
Immutable infrastructure goes hand-in-hand with infrastructure as code where all automation needed
to create infrastructure is stored in version control (such as Git).
This combination of immutability and version control means that there is a durable audit log of every authorized change to a system.","An API object used to store non-confidential data in key-value pairs.
 can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
.

<!--more--> 

A ConfigMap allows you to decouple environment-specific configuration from your , so that your applications are easily portable.","An API object used to store non-confidential data in key-value pairs.
 can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
.

<!--more--> 

A ConfigMap allows you to decouple environment-specific configuration from your , so that your applications are easily portable.","An API object used to store non-confidential data in key-value pairs.
 can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
.

<!--more--> 

A ConfigMap allows you to decouple environment-specific configuration from your , so that your applications are easily portable.","Extensions are software components that extend and deeply integrate with Kubernetes to support new types of hardware.

<!--more-->

Many cluster administrators use a hosted or distribution instance of Kubernetes. These clusters
come with extensions pre-installed. As a result, most Kubernetes users will not need to install
[extensions](/docs/concepts/extend-kubernetes/) and even fewer users will need to author new ones.","Extensions are software components that extend and deeply integrate with Kubernetes to support new types of hardware.

<!--more-->

Many cluster administrators use a hosted or distribution instance of Kubernetes. These clusters
come with extensions pre-installed. As a result, most Kubernetes users will not need to install
[extensions](/docs/concepts/extend-kubernetes/) and even fewer users will need to author new ones.","Extensions are software components that extend and deeply integrate with Kubernetes to support new types of hardware.

<!--more-->

Many cluster administrators use a hosted or distribution instance of Kubernetes. These clusters
come with extensions pre-installed. As a result, most Kubernetes users will not need to install
[extensions](/docs/concepts/extend-kubernetes/) and even fewer users will need to author new ones.","Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

<!--more-->

If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for the data.

You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).","Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

<!--more-->

If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for the data.

You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).","Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

<!--more-->

If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for the data.

You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).","A HostAliases is a mapping between the IP address and hostname to be injected into a 's hosts file.

<!--more-->

[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.","A HostAliases is a mapping between the IP address and hostname to be injected into a 's hosts file.

<!--more-->

[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.","A HostAliases is a mapping between the IP address and hostname to be injected into a 's hosts file.

<!--more-->

[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.","A HostAliases is a mapping between the IP address and hostname to be injected into a 's hosts file.

<!--more-->

[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.","A HostAliases is a mapping between the IP address and hostname to be injected into a 's hosts file.

<!--more-->

[HostAliases](/docs/reference/generated/kubernetes-api//#hostalias-v1-core) is an optional list of hostnames and IP addresses that will be injected into the Pod's hosts file if specified. This is only valid for non-hostNetwork Pods.","Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.

<!--more-->

* For information on Kubernetes and CNI, see [**Network Plugins**](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).","Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.

<!--more-->

* For information on Kubernetes and CNI, see [**Network Plugins**](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).","Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.

<!--more-->

* For information on Kubernetes and CNI, see [**Network Plugins**](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).","One or more  that are typically started before any app containers run.

<!--more--> 

Sidecar containers are like regular app containers, but with a different purpose: the sidecar provides a Pod-local service to the main app container.
Unlike , sidecar containers
continue running after Pod startup.

Read [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) for more information.","One or more  that are typically started before any app containers run.

<!--more--> 

Sidecar containers are like regular app containers, but with a different purpose: the sidecar provides a Pod-local service to the main app container.
Unlike , sidecar containers
continue running after Pod startup.

Read [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) for more information.","One or more  that are typically started before any app containers run.

<!--more--> 

Sidecar containers are like regular app containers, but with a different purpose: the sidecar provides a Pod-local service to the main app container.
Unlike , sidecar containers
continue running after Pod startup.

Read [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) for more information.","A category of  in the
 cluster that can be used with dynamic resource allocation (DRA).

<!--more-->

Administrators or device owners use DeviceClasses to define a set of devices
that can be claimed and used in workloads. Devices are claimed by creating

that filter for specific device parameters in a DeviceClass.

For more information, see
[Dynamic Resource Allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass)","A category of  in the
 cluster that can be used with dynamic resource allocation (DRA).

<!--more-->

Administrators or device owners use DeviceClasses to define a set of devices
that can be claimed and used in workloads. Devices are claimed by creating

that filter for specific device parameters in a DeviceClass.

For more information, see
[Dynamic Resource Allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass)","A category of  in the
 cluster that can be used with dynamic resource allocation (DRA).

<!--more-->

Administrators or device owners use DeviceClasses to define a set of devices
that can be claimed and used in workloads. Devices are claimed by creating

that filter for specific device parameters in a DeviceClass.

For more information, see
[Dynamic Resource Allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass)","A category of  in the
 cluster that can be used with dynamic resource allocation (DRA).

<!--more-->

Administrators or device owners use DeviceClasses to define a set of devices
that can be claimed and used in workloads. Devices are claimed by creating

that filter for specific device parameters in a DeviceClass.

For more information, see
[Dynamic Resource Allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#deviceclass)","The API server is a component of the Kubernetes
 that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.

<!--more-->

The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.","The API server is a component of the Kubernetes
 that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.

<!--more-->

The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.","The API server is a component of the Kubernetes
 that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.

<!--more-->

The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.","The API server is a component of the Kubernetes
 that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.

<!--more-->

The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.","The API server is a component of the Kubernetes
 that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.

<!--more-->

The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.","A directory containing data, accessible to the  in a .

<!--more-->

A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.

See [storage](/docs/concepts/storage/) for more information.","A directory containing data, accessible to the  in a .

<!--more-->

A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.

See [storage](/docs/concepts/storage/) for more information.","A directory containing data, accessible to the  in a .

<!--more-->

A Kubernetes volume lives as long as the Pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data in the volume is preserved across container restarts.

See [storage](/docs/concepts/storage/) for more information.","Describes the resources that a workload needs, such as
. ResourceClaims are
used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide Pods with access to a specific resource.

<!--more-->

ResourceClaims can be created by workload operators or generated by Kubernetes
based on a
.","Describes the resources that a workload needs, such as
. ResourceClaims are
used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide Pods with access to a specific resource.

<!--more-->

ResourceClaims can be created by workload operators or generated by Kubernetes
based on a
.","Describes the resources that a workload needs, such as
. ResourceClaims are
used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide Pods with access to a specific resource.

<!--more-->

ResourceClaims can be created by workload operators or generated by Kubernetes
based on a
.","Node-pressure eviction is the process by which the  proactively terminates
pods to reclaim 
on nodes.

<!--more-->

The kubelet monitors resources like CPU, memory, disk space, and filesystem 
inodes on your cluster's nodes. When one or more of these resources reach
specific consumption levels, the kubelet can proactively fail one or more pods
on the node to reclaim resources and prevent starvation. 

Node-pressure eviction is not the same as [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).","Node-pressure eviction is the process by which the  proactively terminates
pods to reclaim 
on nodes.

<!--more-->

The kubelet monitors resources like CPU, memory, disk space, and filesystem 
inodes on your cluster's nodes. When one or more of these resources reach
specific consumption levels, the kubelet can proactively fail one or more pods
on the node to reclaim resources and prevent starvation. 

Node-pressure eviction is not the same as [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).","Node-pressure eviction is the process by which the  proactively terminates
pods to reclaim 
on nodes.

<!--more-->

The kubelet monitors resources like CPU, memory, disk space, and filesystem 
inodes on your cluster's nodes. When one or more of these resources reach
specific consumption levels, the kubelet can proactively fail one or more pods
on the node to reclaim resources and prevent starvation. 

Node-pressure eviction is not the same as [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).","Node-pressure eviction is the process by which the  proactively terminates
pods to reclaim 
on nodes.

<!--more-->

The kubelet monitors resources like CPU, memory, disk space, and filesystem 
inodes on your cluster's nodes. When one or more of these resources reach
specific consumption levels, the kubelet can proactively fail one or more pods
on the node to reclaim resources and prevent starvation. 

Node-pressure eviction is not the same as [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).","Node-pressure eviction is the process by which the  proactively terminates
pods to reclaim 
on nodes.

<!--more-->

The kubelet monitors resources like CPU, memory, disk space, and filesystem 
inodes on your cluster's nodes. When one or more of these resources reach
specific consumption levels, the kubelet can proactively fail one or more pods
on the node to reclaim resources and prevent starvation. 

Node-pressure eviction is not the same as [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).","Preemption logic in Kubernetes helps a pending  to find a suitable  by evicting low priority Pods existing on that Node.

<!--more-->

If a Pod cannot be scheduled, the scheduler tries to [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption) lower priority Pods to make scheduling of the pending Pod possible.","Preemption logic in Kubernetes helps a pending  to find a suitable  by evicting low priority Pods existing on that Node.

<!--more-->

If a Pod cannot be scheduled, the scheduler tries to [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption) lower priority Pods to make scheduling of the pending Pod possible.","Preemption logic in Kubernetes helps a pending  to find a suitable  by evicting low priority Pods existing on that Node.

<!--more-->

If a Pod cannot be scheduled, the scheduler tries to [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption) lower priority Pods to make scheduling of the pending Pod possible.","A lightweight and portable executable image that contains software and all of its dependencies.

<!--more--> 

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling.
The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","A lightweight and portable executable image that contains software and all of its dependencies.

<!--more--> 

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling.
The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","A lightweight and portable executable image that contains software and all of its dependencies.

<!--more--> 

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling.
The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","A lightweight and portable executable image that contains software and all of its dependencies.

<!--more--> 

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling.
The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","A lightweight and portable executable image that contains software and all of its dependencies.

<!--more--> 

Containers decouple applications from underlying host infrastructure to make deployment easier in different cloud or OS environments, and for easier scaling.
The applications that run inside containers are called containerized applications. The process of bundling these applications and their dependencies into a container image is called containerization.","A set of worker machines, called ,
that run containerized applications. Every cluster has at least one worker node.

<!--more-->
The worker node(s) host the  that are
the components of the application workload. The
 manages the worker
nodes and the Pods in the cluster. In production environments, the control plane usually
runs across multiple computers and a cluster usually runs multiple nodes, providing
fault-tolerance and high availability.","A set of worker machines, called ,
that run containerized applications. Every cluster has at least one worker node.

<!--more-->
The worker node(s) host the  that are
the components of the application workload. The
 manages the worker
nodes and the Pods in the cluster. In production environments, the control plane usually
runs across multiple computers and a cluster usually runs multiple nodes, providing
fault-tolerance and high availability.","A set of worker machines, called ,
that run containerized applications. Every cluster has at least one worker node.

<!--more-->
The worker node(s) host the  that are
the components of the application workload. The
 manages the worker
nodes and the Pods in the cluster. In production environments, the control plane usually
runs across multiple computers and a cluster usually runs multiple nodes, providing
fault-tolerance and high availability.","The aggregation layer lets you install additional Kubernetes-style APIs in your cluster.

<!--more-->

When you've configured the  to [support additional APIs](/docs/tasks/extend-kubernetes/configure-aggregation-layer/), you can add `APIService` objects to ""claim"" a URL path in the Kubernetes API.","The aggregation layer lets you install additional Kubernetes-style APIs in your cluster.

<!--more-->

When you've configured the  to [support additional APIs](/docs/tasks/extend-kubernetes/configure-aggregation-layer/), you can add `APIService` objects to ""claim"" a URL path in the Kubernetes API.","The aggregation layer lets you install additional Kubernetes-style APIs in your cluster.

<!--more-->

When you've configured the  to [support additional APIs](/docs/tasks/extend-kubernetes/configure-aggregation-layer/), you can add `APIService` objects to ""claim"" a URL path in the Kubernetes API.","Legacy term, used as synonym for  hosting the .

<!--more-->
The term is still being used by some provisioning tools, such as , and managed services, to   with `kubernetes.io/role` and control placement of  .","Legacy term, used as synonym for  hosting the .

<!--more-->
The term is still being used by some provisioning tools, such as , and managed services, to   with `kubernetes.io/role` and control placement of  .","Legacy term, used as synonym for  hosting the .

<!--more-->
The term is still being used by some provisioning tools, such as , and managed services, to   with `kubernetes.io/role` and control placement of  .","A person who develops and contributes code to the Kubernetes open source codebase.

<!--more--> 

They are also an active  who participates in one or more .","A person who develops and contributes code to the Kubernetes open source codebase.

<!--more--> 

They are also an active  who participates in one or more .","A person who develops and contributes code to the Kubernetes open source codebase.

<!--more--> 

They are also an active  who participates in one or more .","Control plane component that runs  processes.

<!--more-->

Logically, each  is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.","Control plane component that runs  processes.

<!--more-->

Logically, each  is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.","Control plane component that runs  processes.

<!--more-->

Logically, each  is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.","A tool that lets you use OCI container runtimes with Kubernetes CRI.

<!--more-->

CRI-O is an implementation of the 
to enable using 
runtimes that are compatible with the Open Container Initiative (OCI)
[runtime spec](https://www.github.com/opencontainers/runtime-spec).

Deploying CRI-O allows Kubernetes to use any OCI-compliant runtime as the container
runtime for running , and to fetch
OCI container images from remote registries.","A tool that lets you use OCI container runtimes with Kubernetes CRI.

<!--more-->

CRI-O is an implementation of the 
to enable using 
runtimes that are compatible with the Open Container Initiative (OCI)
[runtime spec](https://www.github.com/opencontainers/runtime-spec).

Deploying CRI-O allows Kubernetes to use any OCI-compliant runtime as the container
runtime for running , and to fetch
OCI container images from remote registries.","A tool that lets you use OCI container runtimes with Kubernetes CRI.

<!--more-->

CRI-O is an implementation of the 
to enable using 
runtimes that are compatible with the Open Container Initiative (OCI)
[runtime spec](https://www.github.com/opencontainers/runtime-spec).

Deploying CRI-O allows Kubernetes to use any OCI-compliant runtime as the container
runtime for running , and to fetch
OCI container images from remote registries.","A tool that lets you use OCI container runtimes with Kubernetes CRI.

<!--more-->

CRI-O is an implementation of the 
to enable using 
runtimes that are compatible with the Open Container Initiative (OCI)
[runtime spec](https://www.github.com/opencontainers/runtime-spec).

Deploying CRI-O allows Kubernetes to use any OCI-compliant runtime as the container
runtime for running , and to fetch
OCI container images from remote registries.","A tool that lets you use OCI container runtimes with Kubernetes CRI.

<!--more-->

CRI-O is an implementation of the 
to enable using 
runtimes that are compatible with the Open Container Initiative (OCI)
[runtime spec](https://www.github.com/opencontainers/runtime-spec).

Deploying CRI-O allows Kubernetes to use any OCI-compliant runtime as the container
runtime for running , and to fetch
OCI container images from remote registries.","Stores sensitive information, such as passwords, OAuth tokens, and SSH keys.

<!--more-->

Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
[encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted).

A  can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/) are
designed for non-confidential data.","Stores sensitive information, such as passwords, OAuth tokens, and SSH keys.

<!--more-->

Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
[encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted).

A  can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/) are
designed for non-confidential data.","Stores sensitive information, such as passwords, OAuth tokens, and SSH keys.

<!--more-->

Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
[encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted).

A  can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/) are
designed for non-confidential data.","Stores sensitive information, such as passwords, OAuth tokens, and SSH keys.

<!--more-->

Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
[encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted).

A  can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/) are
designed for non-confidential data.","Stores sensitive information, such as passwords, OAuth tokens, and SSH keys.

<!--more-->

Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
[encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted).

A  can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/) are
designed for non-confidential data.","Kubernetes' mechanism to expose Pod and container field values to code running in a container.
<!--more-->
It is sometimes useful for a container to have information about itself, without
needing to make changes to the container code that directly couple it to Kubernetes.

The Kubernetes downward API allows containers to consume information about themselves
or their context in a Kubernetes cluster. Applications in containers can have
access to that information, without the application needing to act as a client of
the Kubernetes API.

There are two ways to expose Pod and container fields to a running container:

- using [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
- using [a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)

Together, these two ways of exposing Pod and container fields are called the _downward API_.","Kubernetes' mechanism to expose Pod and container field values to code running in a container.
<!--more-->
It is sometimes useful for a container to have information about itself, without
needing to make changes to the container code that directly couple it to Kubernetes.

The Kubernetes downward API allows containers to consume information about themselves
or their context in a Kubernetes cluster. Applications in containers can have
access to that information, without the application needing to act as a client of
the Kubernetes API.

There are two ways to expose Pod and container fields to a running container:

- using [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
- using [a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)

Together, these two ways of exposing Pod and container fields are called the _downward API_.","Kubernetes' mechanism to expose Pod and container field values to code running in a container.
<!--more-->
It is sometimes useful for a container to have information about itself, without
needing to make changes to the container code that directly couple it to Kubernetes.

The Kubernetes downward API allows containers to consume information about themselves
or their context in a Kubernetes cluster. Applications in containers can have
access to that information, without the application needing to act as a client of
the Kubernetes API.

There are two ways to expose Pod and container fields to a running container:

- using [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
- using [a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)

Together, these two ways of exposing Pod and container fields are called the _downward API_.","Kubernetes' mechanism to expose Pod and container field values to code running in a container.
<!--more-->
It is sometimes useful for a container to have information about itself, without
needing to make changes to the container code that directly couple it to Kubernetes.

The Kubernetes downward API allows containers to consume information about themselves
or their context in a Kubernetes cluster. Applications in containers can have
access to that information, without the application needing to act as a client of
the Kubernetes API.

There are two ways to expose Pod and container fields to a running container:

- using [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
- using [a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)

Together, these two ways of exposing Pod and container fields are called the _downward API_.","Kubernetes' mechanism to expose Pod and container field values to code running in a container.
<!--more-->
It is sometimes useful for a container to have information about itself, without
needing to make changes to the container code that directly couple it to Kubernetes.

The Kubernetes downward API allows containers to consume information about themselves
or their context in a Kubernetes cluster. Applications in containers can have
access to that information, without the application needing to act as a client of
the Kubernetes API.

There are two ways to expose Pod and container fields to a running container:

- using [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)
- using [a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)

Together, these two ways of exposing Pod and container fields are called the _downward API_.","The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and
 fosters a community around [projects](https://www.cncf.io/projects/) that
 orchestrate containers as part of a microservices architecture.

Kubernetes is a CNCF project.

<!--more-->

The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).
Its mission is to make cloud native computing ubiquitous.","The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and
 fosters a community around [projects](https://www.cncf.io/projects/) that
 orchestrate containers as part of a microservices architecture.

Kubernetes is a CNCF project.

<!--more-->

The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).
Its mission is to make cloud native computing ubiquitous.","The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and
 fosters a community around [projects](https://www.cncf.io/projects/) that
 orchestrate containers as part of a microservices architecture.

Kubernetes is a CNCF project.

<!--more-->

The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).
Its mission is to make cloud native computing ubiquitous.","The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and
 fosters a community around [projects](https://www.cncf.io/projects/) that
 orchestrate containers as part of a microservices architecture.

Kubernetes is a CNCF project.

<!--more-->

The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).
Its mission is to make cloud native computing ubiquitous.","The Cloud Native Computing Foundation (CNCF) builds sustainable ecosystems and
 fosters a community around [projects](https://www.cncf.io/projects/) that
 orchestrate containers as part of a microservices architecture.

Kubernetes is a CNCF project.

<!--more-->

The CNCF is a sub-foundation of [the Linux Foundation](https://www.linuxfoundation.org/).
Its mission is to make cloud native computing ubiquitous.","A person responsible for the high-level design of an application.

<!--more--> 

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","A person responsible for the high-level design of an application.

<!--more--> 

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","A person responsible for the high-level design of an application.

<!--more--> 

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","A person responsible for the high-level design of an application.

<!--more--> 

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","A person responsible for the high-level design of an application.

<!--more--> 

An architect ensures that an app's implementation allows it to interact with its surrounding components in a scalable, maintainable way. Surrounding components include databases, logging infrastructure, and other microservices.","`kOps` will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

<!--more--> 


AWS (Amazon Web Services) is currently officially supported, with DigitalOcean, GCE and OpenStack in beta support, and Azure in alpha.


`kOps` is an automated provisioning system:
  * Fully automated installation
  * Uses DNS to identify clusters
  * Self-healing: everything runs in Auto-Scaling Groups
  * Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu)
  * High-Availability support
  * Can directly provision, or generate terraform manifests","`kOps` will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

<!--more--> 


AWS (Amazon Web Services) is currently officially supported, with DigitalOcean, GCE and OpenStack in beta support, and Azure in alpha.


`kOps` is an automated provisioning system:
  * Fully automated installation
  * Uses DNS to identify clusters
  * Self-healing: everything runs in Auto-Scaling Groups
  * Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu)
  * High-Availability support
  * Can directly provision, or generate terraform manifests","`kOps` will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

<!--more--> 


AWS (Amazon Web Services) is currently officially supported, with DigitalOcean, GCE and OpenStack in beta support, and Azure in alpha.


`kOps` is an automated provisioning system:
  * Fully automated installation
  * Uses DNS to identify clusters
  * Self-healing: everything runs in Auto-Scaling Groups
  * Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu)
  * High-Availability support
  * Can directly provision, or generate terraform manifests","`kOps` will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

<!--more--> 


AWS (Amazon Web Services) is currently officially supported, with DigitalOcean, GCE and OpenStack in beta support, and Azure in alpha.


`kOps` is an automated provisioning system:
  * Fully automated installation
  * Uses DNS to identify clusters
  * Self-healing: everything runs in Auto-Scaling Groups
  * Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu)
  * High-Availability support
  * Can directly provision, or generate terraform manifests","`kOps` will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

<!--more--> 


AWS (Amazon Web Services) is currently officially supported, with DigitalOcean, GCE and OpenStack in beta support, and Azure in alpha.


`kOps` is an automated provisioning system:
  * Fully automated installation
  * Uses DNS to identify clusters
  * Self-healing: everything runs in Auto-Scaling Groups
  * Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu)
  * High-Availability support
  * Can directly provision, or generate terraform manifests","Container environment variables are name=value pairs that provide useful information into containers running in a 

<!--more-->

Container environment variables provide information that is required by the running containerized applications along with information about important related details to the . For example, file system details, information about the container itself, and other cluster resources such as service endpoints.","Container environment variables are name=value pairs that provide useful information into containers running in a 

<!--more-->

Container environment variables provide information that is required by the running containerized applications along with information about important related details to the . For example, file system details, information about the container itself, and other cluster resources such as service endpoints.","Container environment variables are name=value pairs that provide useful information into containers running in a 

<!--more-->

Container environment variables provide information that is required by the running containerized applications along with information about important related details to the . For example, file system details, information about the container itself, and other cluster resources such as service endpoints.","May refer to: code in the Kubernetes ecosystem that depends upon the core Kubernetes codebase or a forked repo.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *downstream* to mean the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to improve their functionality.
* In **GitHub** or **git**: The convention is to refer to a forked repo as *downstream*, whereas the source repo is considered *upstream*.","May refer to: code in the Kubernetes ecosystem that depends upon the core Kubernetes codebase or a forked repo.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *downstream* to mean the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to improve their functionality.
* In **GitHub** or **git**: The convention is to refer to a forked repo as *downstream*, whereas the source repo is considered *upstream*.","May refer to: code in the Kubernetes ecosystem that depends upon the core Kubernetes codebase or a forked repo.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *downstream* to mean the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to improve their functionality.
* In **GitHub** or **git**: The convention is to refer to a forked repo as *downstream*, whereas the source repo is considered *upstream*.","May refer to: code in the Kubernetes ecosystem that depends upon the core Kubernetes codebase or a forked repo.

<!--more--> 

* In the **Kubernetes Community**: Conversations often use *downstream* to mean the ecosystem, code, or third-party tools that rely on the core Kubernetes codebase. For example, a new feature in Kubernetes may be adopted by applications *downstream* to improve their functionality.
* In **GitHub** or **git**: The convention is to refer to a forked repo as *downstream*, whereas the source repo is considered *upstream*.","A specification of how groups of Pods are allowed to communicate with each other and with other network endpoints.

<!--more--> 

NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate,
and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use 
to select Pods and define rules which specify what traffic is allowed to the selected Pods.

NetworkPolicies are implemented by a supported network plugin provided by a network provider.
Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.","A specification of how groups of Pods are allowed to communicate with each other and with other network endpoints.

<!--more--> 

NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate,
and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use 
to select Pods and define rules which specify what traffic is allowed to the selected Pods.

NetworkPolicies are implemented by a supported network plugin provided by a network provider.
Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.","A specification of how groups of Pods are allowed to communicate with each other and with other network endpoints.

<!--more--> 

NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate,
and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use 
to select Pods and define rules which specify what traffic is allowed to the selected Pods.

NetworkPolicies are implemented by a supported network plugin provided by a network provider.
Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.","A specification of how groups of Pods are allowed to communicate with each other and with other network endpoints.

<!--more--> 

NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate,
and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use 
to select Pods and define rules which specify what traffic is allowed to the selected Pods.

NetworkPolicies are implemented by a supported network plugin provided by a network provider.
Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.","A specification of how groups of Pods are allowed to communicate with each other and with other network endpoints.

<!--more--> 

NetworkPolicies help you declaratively configure which Pods are allowed to connect to each other, which namespaces are allowed to communicate,
and more specifically which port numbers to enforce each policy on. NetworkPolicy objects use 
to select Pods and define rules which specify what traffic is allowed to the selected Pods.

NetworkPolicies are implemented by a supported network plugin provided by a network provider.
Be aware that creating a NetworkPolicy object without a controller to implement it will have no effect.","Pod Priority indicates the importance of a  relative to other Pods.

<!--more-->

[Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) gives the ability to set scheduling priority of a Pod to be higher and lower than other Pods — an important feature for production clusters workload.","Pod Priority indicates the importance of a  relative to other Pods.

<!--more-->

[Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) gives the ability to set scheduling priority of a Pod to be higher and lower than other Pods — an important feature for production clusters workload.","Pod Priority indicates the importance of a  relative to other Pods.

<!--more-->

[Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) gives the ability to set scheduling priority of a Pod to be higher and lower than other Pods — an important feature for production clusters workload.","Feature to let a kube-apiserver proxy a resource request to a different peer API server.

<!--more-->

When a cluster has multiple API servers running different versions of Kubernetes, this
feature enables 
requests to be served by the correct API server.

MVP is disabled by default and can be activated by enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `UnknownVersionInteroperabilityProxy` when 
the  is started.","Feature to let a kube-apiserver proxy a resource request to a different peer API server.

<!--more-->

When a cluster has multiple API servers running different versions of Kubernetes, this
feature enables 
requests to be served by the correct API server.

MVP is disabled by default and can be activated by enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `UnknownVersionInteroperabilityProxy` when 
the  is started.","Feature to let a kube-apiserver proxy a resource request to a different peer API server.

<!--more-->

When a cluster has multiple API servers running different versions of Kubernetes, this
feature enables 
requests to be served by the correct API server.

MVP is disabled by default and can be activated by enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `UnknownVersionInteroperabilityProxy` when 
the  is started.","Feature to let a kube-apiserver proxy a resource request to a different peer API server.

<!--more-->

When a cluster has multiple API servers running different versions of Kubernetes, this
feature enables 
requests to be served by the correct API server.

MVP is disabled by default and can be activated by enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `UnknownVersionInteroperabilityProxy` when 
the  is started.","Feature to let a kube-apiserver proxy a resource request to a different peer API server.

<!--more-->

When a cluster has multiple API servers running different versions of Kubernetes, this
feature enables 
requests to be served by the correct API server.

MVP is disabled by default and can be activated by enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `UnknownVersionInteroperabilityProxy` when 
the  is started.","Stored instance of a  that holds a set of software needed to run an application.

<!--more-->

A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information.","Stored instance of a  that holds a set of software needed to run an application.

<!--more-->

A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information.","Stored instance of a  that holds a set of software needed to run an application.

<!--more-->

A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information.","Stored instance of a  that holds a set of software needed to run an application.

<!--more-->

A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information.","Stored instance of a  that holds a set of software needed to run an application.

<!--more-->

A way of packaging software that allows it to be stored in a container registry, pulled to a local system, and run as an application. Meta data is included in the image that can indicate what executable to run, who built it, and other information.","Disruptions are events that lead to one or more
 going out of service.
A disruption has consequences for workload management ,
such as , that rely on the affected
Pods.

<!--more-->

If you, as cluster operator, destroy a Pod that belongs to an application,
Kubernetes terms that a _voluntary disruption_. If a Pod goes offline
because of a Node failure, or an outage affecting a wider failure zone,
Kubernetes terms that an _involuntary disruption_.

See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.","Disruptions are events that lead to one or more
 going out of service.
A disruption has consequences for workload management ,
such as , that rely on the affected
Pods.

<!--more-->

If you, as cluster operator, destroy a Pod that belongs to an application,
Kubernetes terms that a _voluntary disruption_. If a Pod goes offline
because of a Node failure, or an outage affecting a wider failure zone,
Kubernetes terms that an _involuntary disruption_.

See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.","Disruptions are events that lead to one or more
 going out of service.
A disruption has consequences for workload management ,
such as , that rely on the affected
Pods.

<!--more-->

If you, as cluster operator, destroy a Pod that belongs to an application,
Kubernetes terms that a _voluntary disruption_. If a Pod goes offline
because of a Node failure, or an outage affecting a wider failure zone,
Kubernetes terms that an _involuntary disruption_.

See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.","Disruptions are events that lead to one or more
 going out of service.
A disruption has consequences for workload management ,
such as , that rely on the affected
Pods.

<!--more-->

If you, as cluster operator, destroy a Pod that belongs to an application,
Kubernetes terms that a _voluntary disruption_. If a Pod goes offline
because of a Node failure, or an outage affecting a wider failure zone,
Kubernetes terms that an _involuntary disruption_.

See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.","Disruptions are events that lead to one or more
 going out of service.
A disruption has consequences for workload management ,
such as , that rely on the affected
Pods.

<!--more-->

If you, as cluster operator, destroy a Pod that belongs to an application,
Kubernetes terms that a _voluntary disruption_. If a Pod goes offline
because of a Node failure, or an outage affecting a wider failure zone,
Kubernetes terms that an _involuntary disruption_.

See [Disruptions](/docs/concepts/workloads/pods/disruptions/) for more information.","Device plugins run on worker
 and provide
 with access to
infrastructure ,
such as local hardware, that require vendor-specific initialization or setup
steps.

<!--more-->

Device plugins advertise resources to the
, so that workload
Pods can access hardware features that relate to the Node where that Pod is running.
You can deploy a device plugin as a ,
or install the device plugin software directly on each target Node.

See
[Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for more information.","Device plugins run on worker
 and provide
 with access to
infrastructure ,
such as local hardware, that require vendor-specific initialization or setup
steps.

<!--more-->

Device plugins advertise resources to the
, so that workload
Pods can access hardware features that relate to the Node where that Pod is running.
You can deploy a device plugin as a ,
or install the device plugin software directly on each target Node.

See
[Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for more information.","Device plugins run on worker
 and provide
 with access to
infrastructure ,
such as local hardware, that require vendor-specific initialization or setup
steps.

<!--more-->

Device plugins advertise resources to the
, so that workload
Pods can access hardware features that relate to the Node where that Pod is running.
You can deploy a device plugin as a ,
or install the device plugin software directly on each target Node.

See
[Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for more information.","Device plugins run on worker
 and provide
 with access to
infrastructure ,
such as local hardware, that require vendor-specific initialization or setup
steps.

<!--more-->

Device plugins advertise resources to the
, so that workload
Pods can access hardware features that relate to the Node where that Pod is running.
You can deploy a device plugin as a ,
or install the device plugin software directly on each target Node.

See
[Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for more information.","Device plugins run on worker
 and provide
 with access to
infrastructure ,
such as local hardware, that require vendor-specific initialization or setup
steps.

<!--more-->

Device plugins advertise resources to the
, so that workload
Pods can access hardware features that relate to the Node where that Pod is running.
You can deploy a device plugin as a ,
or install the device plugin software directly on each target Node.

See
[Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for more information.","A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.

<!--more--> 

We are often concerned with insulating different flows of requests
from each other, so that a high-intensity flow does not crowd out low-intensity flows.
A simple way to put requests into queues is to hash some
characteristics of the request, modulo the number of queues, to get
the index of the queue to use. The hash function uses as input
characteristics of the request that align with flows. For example, in
the Internet this is often the 5-tuple of source and destination
address, protocol, and source and destination port.

That simple hash-based scheme has the property that any high-intensity flow
will crowd out all the low-intensity flows that hash to the same queue.
Providing good insulation for a large number of flows requires a large
number of queues, which is problematic. Shuffle-sharding is a more
nimble technique that can do a better job of insulating the low-intensity
flows from the high-intensity flows. The terminology of shuffle-sharding uses
the metaphor of dealing a hand from a deck of cards; each queue is a
metaphorical card. The shuffle-sharding technique starts with hashing
the flow-identifying characteristics of the request, to produce a hash
value with dozens or more of bits. Then the hash value is used as a
source of entropy to shuffle the deck and deal a hand of cards
(queues). All the dealt queues are examined, and the request is put
into one of the examined queues with the shortest length. With a
modest hand size, it does not cost much to examine all the dealt cards
and a given low-intensity flow has a good chance to dodge the effects of a
given high-intensity flow. With a large hand size it is expensive to examine
the dealt queues and more difficult for the low-intensity flows to dodge the
collective effects of a set of high-intensity flows. Thus, the hand size
should be chosen judiciously.","A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.

<!--more--> 

We are often concerned with insulating different flows of requests
from each other, so that a high-intensity flow does not crowd out low-intensity flows.
A simple way to put requests into queues is to hash some
characteristics of the request, modulo the number of queues, to get
the index of the queue to use. The hash function uses as input
characteristics of the request that align with flows. For example, in
the Internet this is often the 5-tuple of source and destination
address, protocol, and source and destination port.

That simple hash-based scheme has the property that any high-intensity flow
will crowd out all the low-intensity flows that hash to the same queue.
Providing good insulation for a large number of flows requires a large
number of queues, which is problematic. Shuffle-sharding is a more
nimble technique that can do a better job of insulating the low-intensity
flows from the high-intensity flows. The terminology of shuffle-sharding uses
the metaphor of dealing a hand from a deck of cards; each queue is a
metaphorical card. The shuffle-sharding technique starts with hashing
the flow-identifying characteristics of the request, to produce a hash
value with dozens or more of bits. Then the hash value is used as a
source of entropy to shuffle the deck and deal a hand of cards
(queues). All the dealt queues are examined, and the request is put
into one of the examined queues with the shortest length. With a
modest hand size, it does not cost much to examine all the dealt cards
and a given low-intensity flow has a good chance to dodge the effects of a
given high-intensity flow. With a large hand size it is expensive to examine
the dealt queues and more difficult for the low-intensity flows to dodge the
collective effects of a set of high-intensity flows. Thus, the hand size
should be chosen judiciously.","A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.

<!--more--> 

We are often concerned with insulating different flows of requests
from each other, so that a high-intensity flow does not crowd out low-intensity flows.
A simple way to put requests into queues is to hash some
characteristics of the request, modulo the number of queues, to get
the index of the queue to use. The hash function uses as input
characteristics of the request that align with flows. For example, in
the Internet this is often the 5-tuple of source and destination
address, protocol, and source and destination port.

That simple hash-based scheme has the property that any high-intensity flow
will crowd out all the low-intensity flows that hash to the same queue.
Providing good insulation for a large number of flows requires a large
number of queues, which is problematic. Shuffle-sharding is a more
nimble technique that can do a better job of insulating the low-intensity
flows from the high-intensity flows. The terminology of shuffle-sharding uses
the metaphor of dealing a hand from a deck of cards; each queue is a
metaphorical card. The shuffle-sharding technique starts with hashing
the flow-identifying characteristics of the request, to produce a hash
value with dozens or more of bits. Then the hash value is used as a
source of entropy to shuffle the deck and deal a hand of cards
(queues). All the dealt queues are examined, and the request is put
into one of the examined queues with the shortest length. With a
modest hand size, it does not cost much to examine all the dealt cards
and a given low-intensity flow has a good chance to dodge the effects of a
given high-intensity flow. With a large hand size it is expensive to examine
the dealt queues and more difficult for the low-intensity flows to dodge the
collective effects of a set of high-intensity flows. Thus, the hand size
should be chosen judiciously.","A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.

<!--more--> 

We are often concerned with insulating different flows of requests
from each other, so that a high-intensity flow does not crowd out low-intensity flows.
A simple way to put requests into queues is to hash some
characteristics of the request, modulo the number of queues, to get
the index of the queue to use. The hash function uses as input
characteristics of the request that align with flows. For example, in
the Internet this is often the 5-tuple of source and destination
address, protocol, and source and destination port.

That simple hash-based scheme has the property that any high-intensity flow
will crowd out all the low-intensity flows that hash to the same queue.
Providing good insulation for a large number of flows requires a large
number of queues, which is problematic. Shuffle-sharding is a more
nimble technique that can do a better job of insulating the low-intensity
flows from the high-intensity flows. The terminology of shuffle-sharding uses
the metaphor of dealing a hand from a deck of cards; each queue is a
metaphorical card. The shuffle-sharding technique starts with hashing
the flow-identifying characteristics of the request, to produce a hash
value with dozens or more of bits. Then the hash value is used as a
source of entropy to shuffle the deck and deal a hand of cards
(queues). All the dealt queues are examined, and the request is put
into one of the examined queues with the shortest length. With a
modest hand size, it does not cost much to examine all the dealt cards
and a given low-intensity flow has a good chance to dodge the effects of a
given high-intensity flow. With a large hand size it is expensive to examine
the dealt queues and more difficult for the low-intensity flows to dodge the
collective effects of a set of high-intensity flows. Thus, the hand size
should be chosen judiciously.","A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.

<!--more--> 

We are often concerned with insulating different flows of requests
from each other, so that a high-intensity flow does not crowd out low-intensity flows.
A simple way to put requests into queues is to hash some
characteristics of the request, modulo the number of queues, to get
the index of the queue to use. The hash function uses as input
characteristics of the request that align with flows. For example, in
the Internet this is often the 5-tuple of source and destination
address, protocol, and source and destination port.

That simple hash-based scheme has the property that any high-intensity flow
will crowd out all the low-intensity flows that hash to the same queue.
Providing good insulation for a large number of flows requires a large
number of queues, which is problematic. Shuffle-sharding is a more
nimble technique that can do a better job of insulating the low-intensity
flows from the high-intensity flows. The terminology of shuffle-sharding uses
the metaphor of dealing a hand from a deck of cards; each queue is a
metaphorical card. The shuffle-sharding technique starts with hashing
the flow-identifying characteristics of the request, to produce a hash
value with dozens or more of bits. Then the hash value is used as a
source of entropy to shuffle the deck and deal a hand of cards
(queues). All the dealt queues are examined, and the request is put
into one of the examined queues with the shortest length. With a
modest hand size, it does not cost much to examine all the dealt cards
and a given low-intensity flow has a good chance to dodge the effects of a
given high-intensity flow. With a large hand size it is expensive to examine
the dealt queues and more difficult for the low-intensity flows to dodge the
collective effects of a set of high-intensity flows. Thus, the hand size
should be chosen judiciously.","A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching .

<!--more-->

Tolerations and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a . A toleration indicates that the  is allowed (but not required) to be scheduled on nodes or node groups with matching .","A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching .

<!--more-->

Tolerations and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a . A toleration indicates that the  is allowed (but not required) to be scheduled on nodes or node groups with matching .","A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching .

<!--more-->

Tolerations and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a . A toleration indicates that the  is allowed (but not required) to be scheduled on nodes or node groups with matching .","A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching .

<!--more-->

Tolerations and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a . A toleration indicates that the  is allowed (but not required) to be scheduled on nodes or node groups with matching .","A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have matching .

<!--more-->

Tolerations and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more tolerations are applied to a . A toleration indicates that the  is allowed (but not required) to be scheduled on nodes or node groups with matching .","A Kubernetes systems-generated string to uniquely identify objects.

<!--more--> 

Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.","A Kubernetes systems-generated string to uniquely identify objects.

<!--more--> 

Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.","A Kubernetes systems-generated string to uniquely identify objects.

<!--more--> 

Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.","A Kubernetes systems-generated string to uniquely identify objects.

<!--more--> 

Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.","A Kubernetes systems-generated string to uniquely identify objects.

<!--more--> 

Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.","[Pod disruption](/docs/concepts/workloads/pods/disruptions/) is the process by which 
Pods on Nodes are terminated either voluntarily or involuntarily. 

<!--more--> 

Voluntary disruptions are started intentionally by application owners or cluster 
administrators. Involuntary disruptions are unintentional and can be triggered by 
unavoidable issues like Nodes running out of ,
or by accidental deletions.","[Pod disruption](/docs/concepts/workloads/pods/disruptions/) is the process by which 
Pods on Nodes are terminated either voluntarily or involuntarily. 

<!--more--> 

Voluntary disruptions are started intentionally by application owners or cluster 
administrators. Involuntary disruptions are unintentional and can be triggered by 
unavoidable issues like Nodes running out of ,
or by accidental deletions.","[Pod disruption](/docs/concepts/workloads/pods/disruptions/) is the process by which 
Pods on Nodes are terminated either voluntarily or involuntarily. 

<!--more--> 

Voluntary disruptions are started intentionally by application owners or cluster 
administrators. Involuntary disruptions are unintentional and can be triggered by 
unavoidable issues like Nodes running out of ,
or by accidental deletions.","An  that automatically scales the number of  replicas,
based on targeted  utilization or custom metric targets.

<!--more--> 

HorizontalPodAutoscaler (HPA) is typically used with , or . It cannot be applied to objects that cannot be scaled, for example .","An  that automatically scales the number of  replicas,
based on targeted  utilization or custom metric targets.

<!--more--> 

HorizontalPodAutoscaler (HPA) is typically used with , or . It cannot be applied to objects that cannot be scaled, for example .","An  that automatically scales the number of  replicas,
based on targeted  utilization or custom metric targets.

<!--more--> 

HorizontalPodAutoscaler (HPA) is typically used with , or . It cannot be applied to objects that cannot be scaled, for example .","A business or other organization that offers a cloud computing platform.

<!--more-->

Cloud providers, sometimes called Cloud Service Providers (CSPs), offer
cloud computing platforms or services.

Many cloud providers offer managed infrastructure (also called
Infrastructure as a Service or IaaS).
With managed infrastructure the cloud provider is responsible for
servers, storage, and networking while you manage layers on top of that
such as running a Kubernetes cluster.

You can also find Kubernetes as a managed service; sometimes called
Platform as a Service, or PaaS. With managed Kubernetes, your
cloud provider is responsible for the Kubernetes control plane as well
as the  and the
infrastructure they rely on: networking, storage, and possibly other
elements such as load balancers.","A business or other organization that offers a cloud computing platform.

<!--more-->

Cloud providers, sometimes called Cloud Service Providers (CSPs), offer
cloud computing platforms or services.

Many cloud providers offer managed infrastructure (also called
Infrastructure as a Service or IaaS).
With managed infrastructure the cloud provider is responsible for
servers, storage, and networking while you manage layers on top of that
such as running a Kubernetes cluster.

You can also find Kubernetes as a managed service; sometimes called
Platform as a Service, or PaaS. With managed Kubernetes, your
cloud provider is responsible for the Kubernetes control plane as well
as the  and the
infrastructure they rely on: networking, storage, and possibly other
elements such as load balancers.","A business or other organization that offers a cloud computing platform.

<!--more-->

Cloud providers, sometimes called Cloud Service Providers (CSPs), offer
cloud computing platforms or services.

Many cloud providers offer managed infrastructure (also called
Infrastructure as a Service or IaaS).
With managed infrastructure the cloud provider is responsible for
servers, storage, and networking while you manage layers on top of that
such as running a Kubernetes cluster.

You can also find Kubernetes as a managed service; sometimes called
Platform as a Service, or PaaS. With managed Kubernetes, your
cloud provider is responsible for the Kubernetes control plane as well
as the  and the
infrastructure they rely on: networking, storage, and possibly other
elements such as load balancers.","A business or other organization that offers a cloud computing platform.

<!--more-->

Cloud providers, sometimes called Cloud Service Providers (CSPs), offer
cloud computing platforms or services.

Many cloud providers offer managed infrastructure (also called
Infrastructure as a Service or IaaS).
With managed infrastructure the cloud provider is responsible for
servers, storage, and networking while you manage layers on top of that
such as running a Kubernetes cluster.

You can also find Kubernetes as a managed service; sometimes called
Platform as a Service, or PaaS. With managed Kubernetes, your
cloud provider is responsible for the Kubernetes control plane as well
as the  and the
infrastructure they rely on: networking, storage, and possibly other
elements such as load balancers.","A business or other organization that offers a cloud computing platform.

<!--more-->

Cloud providers, sometimes called Cloud Service Providers (CSPs), offer
cloud computing platforms or services.

Many cloud providers offer managed infrastructure (also called
Infrastructure as a Service or IaaS).
With managed infrastructure the cloud provider is responsible for
servers, storage, and networking while you manage layers on top of that
such as running a Kubernetes cluster.

You can also find Kubernetes as a managed service; sometimes called
Platform as a Service, or PaaS. With managed Kubernetes, your
cloud provider is responsible for the Kubernetes control plane as well
as the  and the
infrastructure they rely on: networking, storage, and possibly other
elements such as load balancers.","The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.

<!--more--> 

Kubernetes resources and ""records of intent"" are all stored as API objects, and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly, or via tools like `kubectl`. The core Kubernetes API is flexible and can also be extended to support custom resources.","The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.

<!--more--> 

Kubernetes resources and ""records of intent"" are all stored as API objects, and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly, or via tools like `kubectl`. The core Kubernetes API is flexible and can also be extended to support custom resources.","The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.

<!--more--> 

Kubernetes resources and ""records of intent"" are all stored as API objects, and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly, or via tools like `kubectl`. The core Kubernetes API is flexible and can also be extended to support custom resources.","The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.

<!--more--> 

Kubernetes resources and ""records of intent"" are all stored as API objects, and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly, or via tools like `kubectl`. The core Kubernetes API is flexible and can also be extended to support custom resources.","The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.

<!--more--> 

Kubernetes resources and ""records of intent"" are all stored as API objects, and modified via RESTful calls to the API. The API allows configuration to be managed in a declarative way. Users can interact with the Kubernetes API directly, or via tools like `kubectl`. The core Kubernetes API is flexible and can also be extended to support custom resources.","A person who configures, controls, and monitors clusters.

<!--more--> 

Their primary responsibility is keeping a cluster up and running, which may involve periodic maintenance activities or upgrades.<br>


Cluster operators are different from the [Operator pattern](/docs/concepts/extend-kubernetes/operator/) that extends the Kubernetes API.","A person who configures, controls, and monitors clusters.

<!--more--> 

Their primary responsibility is keeping a cluster up and running, which may involve periodic maintenance activities or upgrades.<br>


Cluster operators are different from the [Operator pattern](/docs/concepts/extend-kubernetes/operator/) that extends the Kubernetes API.","A person who configures, controls, and monitors clusters.

<!--more--> 

Their primary responsibility is keeping a cluster up and running, which may involve periodic maintenance activities or upgrades.<br>


Cluster operators are different from the [Operator pattern](/docs/concepts/extend-kubernetes/operator/) that extends the Kubernetes API.","The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.

 <!--more--> 
 
 This layer is composed by many different components, such as (but not restricted to):

 * 
 * 
 * 
 * 
 * 

 These components can be run as traditional operating system services (daemons) or as containers. The hosts running these components were historically called .","The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.

 <!--more--> 
 
 This layer is composed by many different components, such as (but not restricted to):

 * 
 * 
 * 
 * 
 * 

 These components can be run as traditional operating system services (daemons) or as containers. The hosts running these components were historically called .","The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.

 <!--more--> 
 
 This layer is composed by many different components, such as (but not restricted to):

 * 
 * 
 * 
 * 
 * 

 These components can be run as traditional operating system services (daemons) or as containers. The hosts running these components were historically called .","kube-proxy is a network proxy that runs on each
 in your cluster,
implementing part of the Kubernetes
 concept.

<!--more-->

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.

kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.","kube-proxy is a network proxy that runs on each
 in your cluster,
implementing part of the Kubernetes
 concept.

<!--more-->

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.

kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.","kube-proxy is a network proxy that runs on each
 in your cluster,
implementing part of the Kubernetes
 concept.

<!--more-->

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.

kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.","kube-proxy is a network proxy that runs on each
 in your cluster,
implementing part of the Kubernetes
 concept.

<!--more-->

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.

kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.","kube-proxy is a network proxy that runs on each
 in your cluster,
implementing part of the Kubernetes
 concept.

<!--more-->

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.

kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.","A means of representing claims to be transferred between two parties.

<!--more-->

JWTs can be digitally signed and encrypted. Kubernetes uses JWTs as
authentication tokens to verify the identity of entities that want to perform
actions in a cluster.","A means of representing claims to be transferred between two parties.

<!--more-->

JWTs can be digitally signed and encrypted. Kubernetes uses JWTs as
authentication tokens to verify the identity of entities that want to perform
actions in a cluster.","A means of representing claims to be transferred between two parties.

<!--more-->

JWTs can be digitally signed and encrypted. Kubernetes uses JWTs as
authentication tokens to verify the identity of entities that want to perform
actions in a cluster.","Command line tool for communicating with a Kubernetes cluster's
,
using the Kubernetes API.

<!--more--> 

You can use `kubectl` to create, inspect, update, and delete Kubernetes objects.

<!-- localization note: OK to omit the rest of this entry -->
In English, `kubectl` is (officially) pronounced /kjuːb/ /kənˈtɹəʊl/ (like ""cube control"").","Command line tool for communicating with a Kubernetes cluster's
,
using the Kubernetes API.

<!--more--> 

You can use `kubectl` to create, inspect, update, and delete Kubernetes objects.

<!-- localization note: OK to omit the rest of this entry -->
In English, `kubectl` is (officially) pronounced /kjuːb/ /kənˈtɹəʊl/ (like ""cube control"").","Command line tool for communicating with a Kubernetes cluster's
,
using the Kubernetes API.

<!--more--> 

You can use `kubectl` to create, inspect, update, and delete Kubernetes objects.

<!-- localization note: OK to omit the rest of this entry -->
In English, `kubectl` is (officially) pronounced /kjuːb/ /kənˈtɹəʊl/ (like ""cube control"").","The sequence of states through which a Pod passes during its lifetime.

<!--more--> 

The [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/) is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core) `phase` field.","The sequence of states through which a Pod passes during its lifetime.

<!--more--> 

The [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/) is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core) `phase` field.","The sequence of states through which a Pod passes during its lifetime.

<!--more--> 

The [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/) is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core) `phase` field.","The sequence of states through which a Pod passes during its lifetime.

<!--more--> 

The [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/) is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core) `phase` field.","The sequence of states through which a Pod passes during its lifetime.

<!--more--> 

The [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/) is defined by the states or phases of a Pod. There are five possible Pod phases: Pending, Running, Succeeded, Failed, and Unknown. A high-level description of the Pod state is summarized in the [PodStatus](/docs/reference/generated/kubernetes-api//#podstatus-v1-core) `phase` field.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

<!--more-->

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

<!--more-->

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

<!--more-->

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

<!--more-->

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","Manages a [Job](/docs/concepts/workloads/controllers/job/) that runs on a periodic schedule.

<!--more-->

Similar to a line in a *crontab* file, a CronJob object specifies a schedule using the [cron](https://en.wikipedia.org/wiki/Cron) format.","A whole-number representation of small or large numbers using [SI](https://en.wikipedia.org/wiki/International_System_of_Units) suffixes.

<!--more-->

Quantities are representations of small or large numbers using a compact,
whole-number notation with SI suffixes.  Fractional numbers are represented
using milli units, while large numbers can be represented using kilo,
mega, or giga units.

For instance, the number `1.5` is represented as `1500m`, while the number `1000`
can be represented as `1k`, and `1000000` as `1M`. You can also specify
[binary-notation](https://en.wikipedia.org/wiki/Binary_prefix) suffixes; the number 2048 can be written as `2Ki`.

The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo,
intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta),
`E` (exa).

The accepted binary (power-of-2) units are `Ki` (kibi), `Mi` (mebi), `Gi` (gibi),
`Ti` (tebi), `Pi` (pebi), `Ei` (exbi).","A whole-number representation of small or large numbers using [SI](https://en.wikipedia.org/wiki/International_System_of_Units) suffixes.

<!--more-->

Quantities are representations of small or large numbers using a compact,
whole-number notation with SI suffixes.  Fractional numbers are represented
using milli units, while large numbers can be represented using kilo,
mega, or giga units.

For instance, the number `1.5` is represented as `1500m`, while the number `1000`
can be represented as `1k`, and `1000000` as `1M`. You can also specify
[binary-notation](https://en.wikipedia.org/wiki/Binary_prefix) suffixes; the number 2048 can be written as `2Ki`.

The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo,
intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta),
`E` (exa).

The accepted binary (power-of-2) units are `Ki` (kibi), `Mi` (mebi), `Gi` (gibi),
`Ti` (tebi), `Pi` (pebi), `Ei` (exbi).","A whole-number representation of small or large numbers using [SI](https://en.wikipedia.org/wiki/International_System_of_Units) suffixes.

<!--more-->

Quantities are representations of small or large numbers using a compact,
whole-number notation with SI suffixes.  Fractional numbers are represented
using milli units, while large numbers can be represented using kilo,
mega, or giga units.

For instance, the number `1.5` is represented as `1500m`, while the number `1000`
can be represented as `1k`, and `1000000` as `1M`. You can also specify
[binary-notation](https://en.wikipedia.org/wiki/Binary_prefix) suffixes; the number 2048 can be written as `2Ki`.

The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo,
intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta),
`E` (exa).

The accepted binary (power-of-2) units are `Ki` (kibi), `Mi` (mebi), `Gi` (gibi),
`Ti` (tebi), `Pi` (pebi), `Ei` (exbi).","A whole-number representation of small or large numbers using [SI](https://en.wikipedia.org/wiki/International_System_of_Units) suffixes.

<!--more-->

Quantities are representations of small or large numbers using a compact,
whole-number notation with SI suffixes.  Fractional numbers are represented
using milli units, while large numbers can be represented using kilo,
mega, or giga units.

For instance, the number `1.5` is represented as `1500m`, while the number `1000`
can be represented as `1k`, and `1000000` as `1M`. You can also specify
[binary-notation](https://en.wikipedia.org/wiki/Binary_prefix) suffixes; the number 2048 can be written as `2Ki`.

The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo,
intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta),
`E` (exa).

The accepted binary (power-of-2) units are `Ki` (kibi), `Mi` (mebi), `Gi` (gibi),
`Ti` (tebi), `Pi` (pebi), `Ei` (exbi).","A whole-number representation of small or large numbers using [SI](https://en.wikipedia.org/wiki/International_System_of_Units) suffixes.

<!--more-->

Quantities are representations of small or large numbers using a compact,
whole-number notation with SI suffixes.  Fractional numbers are represented
using milli units, while large numbers can be represented using kilo,
mega, or giga units.

For instance, the number `1.5` is represented as `1500m`, while the number `1000`
can be represented as `1k`, and `1000000` as `1M`. You can also specify
[binary-notation](https://en.wikipedia.org/wiki/Binary_prefix) suffixes; the number 2048 can be written as `2Ki`.

The accepted decimal (power-of-10) units are `m` (milli), `k` (kilo,
intentionally lowercase), `M` (mega), `G` (giga), `T` (tera), `P` (peta),
`E` (exa).

The accepted binary (power-of-2) units are `Ki` (kibi), `Mi` (mebi), `Gi` (gibi),
`Ti` (tebi), `Pi` (pebi), `Ei` (exbi).","A person who writes an application that runs in a Kubernetes cluster.

<!--more--> 

An application developer focuses on one part of an application. The scale of their focus may vary significantly in size.","A person who writes an application that runs in a Kubernetes cluster.

<!--more--> 

An application developer focuses on one part of an application. The scale of their focus may vary significantly in size.","A person who writes an application that runs in a Kubernetes cluster.

<!--more--> 

An application developer focuses on one part of an application. The scale of their focus may vary significantly in size.","The dockershim is a component of Kubernetes version 1.23 and earlier. It allows the 
to communicate with .

<!--more-->

Starting with version 1.24, dockershim has been removed from Kubernetes. For more information, see [Dockershim FAQ](/dockershim).","The dockershim is a component of Kubernetes version 1.23 and earlier. It allows the 
to communicate with .

<!--more-->

Starting with version 1.24, dockershim has been removed from Kubernetes. For more information, see [Dockershim FAQ](/dockershim).","The dockershim is a component of Kubernetes version 1.23 and earlier. It allows the 
to communicate with .

<!--more-->

Starting with version 1.24, dockershim has been removed from Kubernetes. For more information, see [Dockershim FAQ](/dockershim).","`sysctl` is a semi-standardized interface for reading or changing the
 attributes of the running Unix kernel.

<!--more-->

On Unix-like systems, `sysctl` is both the name of the tool that administrators
use to view and modify these settings, and also the system call that the tool
uses.

 runtimes and
network plugins may rely on `sysctl` values being set a certain way.","`sysctl` is a semi-standardized interface for reading or changing the
 attributes of the running Unix kernel.

<!--more-->

On Unix-like systems, `sysctl` is both the name of the tool that administrators
use to view and modify these settings, and also the system call that the tool
uses.

 runtimes and
network plugins may rely on `sysctl` values being set a certain way.","`sysctl` is a semi-standardized interface for reading or changing the
 attributes of the running Unix kernel.

<!--more-->

On Unix-like systems, `sysctl` is both the name of the tool that administrators
use to view and modify these settings, and also the system call that the tool
uses.

 runtimes and
network plugins may rely on `sysctl` values being set a certain way.","A node is a worker machine in Kubernetes.

<!--more-->

A worker node may be a VM or physical machine, depending on the cluster. It has local daemons or services necessary to run  and is managed by the control plane. The daemons on a node include , , and a container runtime implementing the  such as .

In early Kubernetes versions, Nodes were called ""Minions"".","A node is a worker machine in Kubernetes.

<!--more-->

A worker node may be a VM or physical machine, depending on the cluster. It has local daemons or services necessary to run  and is managed by the control plane. The daemons on a node include , , and a container runtime implementing the  such as .

In early Kubernetes versions, Nodes were called ""Minions"".","A node is a worker machine in Kubernetes.

<!--more-->

A worker node may be a VM or physical machine, depending on the cluster. It has local daemons or services necessary to run  and is managed by the control plane. The daemons on a node include , , and a container runtime implementing the  such as .

In early Kubernetes versions, Nodes were called ""Minions"".","A verb that is used to track changes to an object in Kubernetes as a stream.
It is used for the efficient detection of changes.

<!--more-->

A verb that is used to track changes to an object in Kubernetes as a stream. Watches allow
efficient detection of changes; for example, a
 that needs to know whenever a
ConfigMap has changed can use a watch rather than polling.

See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.","A verb that is used to track changes to an object in Kubernetes as a stream.
It is used for the efficient detection of changes.

<!--more-->

A verb that is used to track changes to an object in Kubernetes as a stream. Watches allow
efficient detection of changes; for example, a
 that needs to know whenever a
ConfigMap has changed can use a watch rather than polling.

See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.","A verb that is used to track changes to an object in Kubernetes as a stream.
It is used for the efficient detection of changes.

<!--more-->

A verb that is used to track changes to an object in Kubernetes as a stream. Watches allow
efficient detection of changes; for example, a
 that needs to know whenever a
ConfigMap has changed can use a watch rather than polling.

See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.","A verb that is used to track changes to an object in Kubernetes as a stream.
It is used for the efficient detection of changes.

<!--more-->

A verb that is used to track changes to an object in Kubernetes as a stream. Watches allow
efficient detection of changes; for example, a
 that needs to know whenever a
ConfigMap has changed can use a watch rather than polling.

See [Efficient Detection of Changes in API Concepts](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) for more information.","A workload management 
that manages a replicated application, ensuring that
a specific number of instances of a  are running.

<!--more-->

The control plane ensures that the defined number of Pods are running, even if some
Pods fail, if you delete Pods manually, or if too many are started by mistake.


ReplicationController is deprecated. See
, which is similar.","A workload management 
that manages a replicated application, ensuring that
a specific number of instances of a  are running.

<!--more-->

The control plane ensures that the defined number of Pods are running, even if some
Pods fail, if you delete Pods manually, or if too many are started by mistake.


ReplicationController is deprecated. See
, which is similar.","A workload management 
that manages a replicated application, ensuring that
a specific number of instances of a  are running.

<!--more-->

The control plane ensures that the defined number of Pods are running, even if some
Pods fail, if you delete Pods manually, or if too many are started by mistake.


ReplicationController is deprecated. See
, which is similar.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod
in that class.

<!--more-->

A [PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#how-to-use-priority-and-preemption)
is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is
specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from
-2147483648 to 1000000000 inclusive. Higher values indicate higher priority.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod
in that class.

<!--more-->

A [PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#how-to-use-priority-and-preemption)
is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is
specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from
-2147483648 to 1000000000 inclusive. Higher values indicate higher priority.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod
in that class.

<!--more-->

A [PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#how-to-use-priority-and-preemption)
is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is
specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from
-2147483648 to 1000000000 inclusive. Higher values indicate higher priority.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod
in that class.

<!--more-->

A [PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#how-to-use-priority-and-preemption)
is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is
specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from
-2147483648 to 1000000000 inclusive. Higher values indicate higher priority.","A PriorityClass is a named class for the scheduling priority that should be assigned to a Pod
in that class.

<!--more-->

A [PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#how-to-use-priority-and-preemption)
is a non-namespaced object mapping a name to an integer priority, used for a Pod. The name is
specified in the `metadata.name` field, and the priority value in the `value` field. Priorities range from
-2147483648 to 1000000000 inclusive. Higher values indicate higher priority.","FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The  is a newer interface that addresses several problems with FlexVolume.

<!--more--> 

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a  driver if possible since it addresses the limitations with FlexVolumes.

* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)","FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The  is a newer interface that addresses several problems with FlexVolume.

<!--more--> 

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a  driver if possible since it addresses the limitations with FlexVolumes.

* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)","FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The  is a newer interface that addresses several problems with FlexVolume.

<!--more--> 

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a  driver if possible since it addresses the limitations with FlexVolumes.

* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)","FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The  is a newer interface that addresses several problems with FlexVolume.

<!--more--> 

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a  driver if possible since it addresses the limitations with FlexVolumes.

* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)","FlexVolume is a deprecated interface for creating out-of-tree volume plugins. The  is a newer interface that addresses several problems with FlexVolume.

<!--more--> 

FlexVolumes enable users to write their own drivers and add support for their volumes in Kubernetes. FlexVolume driver binaries and dependencies must be installed on host machines. This requires root access. The Storage SIG suggests implementing a  driver if possible since it addresses the limitations with FlexVolumes.

* [FlexVolume in the Kubernetes documentation](/docs/concepts/storage/volumes/#flexvolume)
* [More information on FlexVolumes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)
* [Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)","Ensures a copy of a  is running across a set of nodes in a .

<!--more--> 

Used to deploy system daemons such as log collectors and monitoring agents that typically must run on every .","Ensures a copy of a  is running across a set of nodes in a .

<!--more--> 

Used to deploy system daemons such as log collectors and monitoring agents that typically must run on every .","Ensures a copy of a  is running across a set of nodes in a .

<!--more--> 

Used to deploy system daemons such as log collectors and monitoring agents that typically must run on every .","May refer to&#58; , , or .

<!--more--> 

This overloaded term may have different meanings depending on the context","May refer to&#58; , , or .

<!--more--> 

This overloaded term may have different meanings depending on the context","May refer to&#58; , , or .

<!--more--> 

This overloaded term may have different meanings depending on the context","Feature gates are a set of keys (opaque string values) that you can use to control which
Kubernetes features are enabled in your cluster.

<!--more-->

You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component.
Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.
The Kubernetes documentation lists all current 
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.","Feature gates are a set of keys (opaque string values) that you can use to control which
Kubernetes features are enabled in your cluster.

<!--more-->

You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component.
Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.
The Kubernetes documentation lists all current 
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.","Feature gates are a set of keys (opaque string values) that you can use to control which
Kubernetes features are enabled in your cluster.

<!--more-->

You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component.
Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.
The Kubernetes documentation lists all current 
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.","Feature gates are a set of keys (opaque string values) that you can use to control which
Kubernetes features are enabled in your cluster.

<!--more-->

You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component.
Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.
The Kubernetes documentation lists all current 
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.","Feature gates are a set of keys (opaque string values) that you can use to control which
Kubernetes features are enabled in your cluster.

<!--more-->

You can turn these features on or off using the `--feature-gates` command line flag on each Kubernetes component.
Each Kubernetes component lets you enable or disable a set of feature gates that are relevant to that component.
The Kubernetes documentation lists all current 
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) and what they control.","The [operator pattern](/docs/concepts/extend-kubernetes/operator/) is a system
design that links a  to one or more custom
resources.

<!--more-->

You can extend Kubernetes by adding controllers to your cluster, beyond the built-in
controllers that come as part of Kubernetes itself.

If a running application acts as a controller and has API access to carry out tasks
against a custom resource that's defined in the control plane, that's an example of
the Operator pattern.","The [operator pattern](/docs/concepts/extend-kubernetes/operator/) is a system
design that links a  to one or more custom
resources.

<!--more-->

You can extend Kubernetes by adding controllers to your cluster, beyond the built-in
controllers that come as part of Kubernetes itself.

If a running application acts as a controller and has API access to carry out tasks
against a custom resource that's defined in the control plane, that's an example of
the Operator pattern.","The [operator pattern](/docs/concepts/extend-kubernetes/operator/) is a system
design that links a  to one or more custom
resources.

<!--more-->

You can extend Kubernetes by adding controllers to your cluster, beyond the built-in
controllers that come as part of Kubernetes itself.

If a running application acts as a controller and has API access to carry out tasks
against a custom resource that's defined in the control plane, that's an example of
the Operator pattern.","The [operator pattern](/docs/concepts/extend-kubernetes/operator/) is a system
design that links a  to one or more custom
resources.

<!--more-->

You can extend Kubernetes by adding controllers to your cluster, beyond the built-in
controllers that come as part of Kubernetes itself.

If a running application acts as a controller and has API access to carry out tasks
against a custom resource that's defined in the control plane, that's an example of
the Operator pattern.","The [operator pattern](/docs/concepts/extend-kubernetes/operator/) is a system
design that links a  to one or more custom
resources.

<!--more-->

You can extend Kubernetes by adding controllers to your cluster, beyond the built-in
controllers that come as part of Kubernetes itself.

If a running application acts as a controller and has API access to carry out tasks
against a custom resource that's defined in the control plane, that's an example of
the Operator pattern.","The process of safely evicting  from a  to prepare it for maintenance or removal from a .

<!--more-->

The `kubectl drain` command is used to mark a  as going out of service. 
When executed, it evicts all  from the . 
If an eviction request is temporarily rejected, `kubectl drain` retries until all  are terminated or a configurable timeout is reached.","The process of safely evicting  from a  to prepare it for maintenance or removal from a .

<!--more-->

The `kubectl drain` command is used to mark a  as going out of service. 
When executed, it evicts all  from the . 
If an eviction request is temporarily rejected, `kubectl drain` retries until all  are terminated or a configurable timeout is reached.","The process of safely evicting  from a  to prepare it for maintenance or removal from a .

<!--more-->

The `kubectl drain` command is used to mark a  as going out of service. 
When executed, it evicts all  from the . 
If an eviction request is temporarily rejected, `kubectl drain` retries until all  are terminated or a configurable timeout is reached.","The process of safely evicting  from a  to prepare it for maintenance or removal from a .

<!--more-->

The `kubectl drain` command is used to mark a  as going out of service. 
When executed, it evicts all  from the . 
If an eviction request is temporarily rejected, `kubectl drain` retries until all  are terminated or a configurable timeout is reached.","The process of safely evicting  from a  to prepare it for maintenance or removal from a .

<!--more-->

The `kubectl drain` command is used to mark a  as going out of service. 
When executed, it evicts all  from the . 
If an eviction request is temporarily rejected, `kubectl drain` retries until all  are terminated or a configurable timeout is reached.","A client-provided string that refers to an object in a 
URL, such as `/api/v1/pods/some-name`.

<!--more--> 

Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.","A client-provided string that refers to an object in a 
URL, such as `/api/v1/pods/some-name`.

<!--more--> 

Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.","A client-provided string that refers to an object in a 
URL, such as `/api/v1/pods/some-name`.

<!--more--> 

Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.","A client-provided string that refers to an object in a 
URL, such as `/api/v1/pods/some-name`.

<!--more--> 

Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.","EndpointSlices track the IP addresses of Pods with matching  .

<!--more-->
EndpointSlices can be configured manually for  without selectors specified.","EndpointSlices track the IP addresses of Pods with matching  .

<!--more-->
EndpointSlices can be configured manually for  without selectors specified.","EndpointSlices track the IP addresses of Pods with matching  .

<!--more-->
EndpointSlices can be configured manually for  without selectors specified.","An abstraction used by Kubernetes to support isolation of groups of 
within a single .

<!--more--> 

Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources _(for example: Pods, Deployments, Services)_ and not for cluster-wide resources _(for example: StorageClasses, Nodes, PersistentVolumes)_.","An abstraction used by Kubernetes to support isolation of groups of 
within a single .

<!--more--> 

Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources _(for example: Pods, Deployments, Services)_ and not for cluster-wide resources _(for example: StorageClasses, Nodes, PersistentVolumes)_.","An abstraction used by Kubernetes to support isolation of groups of 
within a single .

<!--more--> 

Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources _(for example: Pods, Deployments, Services)_ and not for cluster-wide resources _(for example: StorageClasses, Nodes, PersistentVolumes)_.","An abstraction used by Kubernetes to support isolation of groups of 
within a single .

<!--more--> 

Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources _(for example: Pods, Deployments, Services)_ and not for cluster-wide resources _(for example: StorageClasses, Nodes, PersistentVolumes)_.","An abstraction used by Kubernetes to support isolation of groups of 
within a single .

<!--more--> 

Namespaces are used to organize objects in a cluster and provide a way to divide cluster resources. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced resources _(for example: Pods, Deployments, Services)_ and not for cluster-wide resources _(for example: StorageClasses, Nodes, PersistentVolumes)_.","Represents one or more infrastructure resources, such as
, that are attached to
nodes. Drivers create and manage ResourceSlices in the cluster. ResourceSlices
are used for
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/).

<!--more-->

When a  is
created, Kubernetes uses ResourceSlices to find nodes that have access to
resources that can satisfy the claim. Kubernetes allocates resources to the
ResourceClaim and schedules the Pod onto a node that can access the resources.","Represents one or more infrastructure resources, such as
, that are attached to
nodes. Drivers create and manage ResourceSlices in the cluster. ResourceSlices
are used for
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/).

<!--more-->

When a  is
created, Kubernetes uses ResourceSlices to find nodes that have access to
resources that can satisfy the claim. Kubernetes allocates resources to the
ResourceClaim and schedules the Pod onto a node that can access the resources.","Represents one or more infrastructure resources, such as
, that are attached to
nodes. Drivers create and manage ResourceSlices in the cluster. ResourceSlices
are used for
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/).

<!--more-->

When a  is
created, Kubernetes uses ResourceSlices to find nodes that have access to
resources that can satisfy the claim. Kubernetes allocates resources to the
ResourceClaim and schedules the Pod onto a node that can access the resources.","Represents one or more infrastructure resources, such as
, that are attached to
nodes. Drivers create and manage ResourceSlices in the cluster. ResourceSlices
are used for
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/).

<!--more-->

When a  is
created, Kubernetes uses ResourceSlices to find nodes that have access to
resources that can satisfy the claim. Kubernetes allocates resources to the
ResourceClaim and schedules the Pod onto a node that can access the resources.","Represents one or more infrastructure resources, such as
, that are attached to
nodes. Drivers create and manage ResourceSlices in the cluster. ResourceSlices
are used for
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/).

<!--more-->

When a  is
created, Kubernetes uses ResourceSlices to find nodes that have access to
resources that can satisfy the claim. Kubernetes allocates resources to the
ResourceClaim and schedules the Pod onto a node that can access the resources.","A package of pre-configured Kubernetes configurations that can be managed with the Helm tool.

<!--more--> 

Charts provide a reproducible way of creating and sharing Kubernetes applications.
A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","A package of pre-configured Kubernetes configurations that can be managed with the Helm tool.

<!--more--> 

Charts provide a reproducible way of creating and sharing Kubernetes applications.
A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","A package of pre-configured Kubernetes configurations that can be managed with the Helm tool.

<!--more--> 

Charts provide a reproducible way of creating and sharing Kubernetes applications.
A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","A package of pre-configured Kubernetes configurations that can be managed with the Helm tool.

<!--more--> 

Charts provide a reproducible way of creating and sharing Kubernetes applications.
A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","A package of pre-configured Kubernetes configurations that can be managed with the Helm tool.

<!--more--> 

Charts provide a reproducible way of creating and sharing Kubernetes applications.
A single chart can be used to deploy something simple, like a memcached Pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction.

<!--more--> 
QoS Class of a Pod is set at creation time based on its 
requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction.
Kubernetes can assign one of the following  QoS classes to a Pod: `Guaranteed`, `Burstable` or `BestEffort`.","QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction.

<!--more--> 
QoS Class of a Pod is set at creation time based on its 
requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction.
Kubernetes can assign one of the following  QoS classes to a Pod: `Guaranteed`, `Burstable` or `BestEffort`.","QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction.

<!--more--> 
QoS Class of a Pod is set at creation time based on its 
requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction.
Kubernetes can assign one of the following  QoS classes to a Pod: `Guaranteed`, `Burstable` or `BestEffort`.","QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction.

<!--more--> 
QoS Class of a Pod is set at creation time based on its 
requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction.
Kubernetes can assign one of the following  QoS classes to a Pod: `Guaranteed`, `Burstable` or `BestEffort`.","QoS Class (Quality of Service Class) provides a way for Kubernetes to classify Pods within the cluster into several classes and make decisions about scheduling and eviction.

<!--more--> 
QoS Class of a Pod is set at creation time based on its 
requests and limits settings. QoS classes are used to make decisions about Pods scheduling and eviction.
Kubernetes can assign one of the following  QoS classes to a Pod: `Guaranteed`, `Burstable` or `BestEffort`.","An API object that defines a template for creating .
The PodTemplate API is also embedded in API definitions for workload management, such as 
 or
.

<!--more--> 

Pod templates allow you to define common metadata (such as labels, or a template for the name of a
new Pod) as well as to specify a pod's desired state.
[Workload management](/docs/concepts/workloads/controllers/) controllers use Pod templates
(embedded into another object, such as a Deployment or StatefulSet)
to define and manage one or more .
When there can be multiple Pods based on the same template, these are called
.
Although you can create a PodTemplate object directly, you rarely need to do so.","An API object that defines a template for creating .
The PodTemplate API is also embedded in API definitions for workload management, such as 
 or
.

<!--more--> 

Pod templates allow you to define common metadata (such as labels, or a template for the name of a
new Pod) as well as to specify a pod's desired state.
[Workload management](/docs/concepts/workloads/controllers/) controllers use Pod templates
(embedded into another object, such as a Deployment or StatefulSet)
to define and manage one or more .
When there can be multiple Pods based on the same template, these are called
.
Although you can create a PodTemplate object directly, you rarely need to do so.","An API object that defines a template for creating .
The PodTemplate API is also embedded in API definitions for workload management, such as 
 or
.

<!--more--> 

Pod templates allow you to define common metadata (such as labels, or a template for the name of a
new Pod) as well as to specify a pod's desired state.
[Workload management](/docs/concepts/workloads/controllers/) controllers use Pod templates
(embedded into another object, such as a Deployment or StatefulSet)
to define and manage one or more .
When there can be multiple Pods based on the same template, these are called
.
Although you can create a PodTemplate object directly, you rarely need to do so.","An API object that defines a template for creating .
The PodTemplate API is also embedded in API definitions for workload management, such as 
 or
.

<!--more--> 

Pod templates allow you to define common metadata (such as labels, or a template for the name of a
new Pod) as well as to specify a pod's desired state.
[Workload management](/docs/concepts/workloads/controllers/) controllers use Pod templates
(embedded into another object, such as a Deployment or StatefulSet)
to define and manage one or more .
When there can be multiple Pods based on the same template, these are called
.
Although you can create a PodTemplate object directly, you rarely need to do so.","In Kubernetes, controllers are control loops that watch the state of your
, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.

<!--more-->

Controllers watch the shared state of your cluster through the
 (part of the
).

Some controllers also run inside the control plane, providing control loops that
are core to Kubernetes' operations. For example: the deployment controller, the
daemonset controller, the namespace controller, and the persistent volume
controller (and others) all run within the
.","In Kubernetes, controllers are control loops that watch the state of your
, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.

<!--more-->

Controllers watch the shared state of your cluster through the
 (part of the
).

Some controllers also run inside the control plane, providing control loops that
are core to Kubernetes' operations. For example: the deployment controller, the
daemonset controller, the namespace controller, and the persistent volume
controller (and others) all run within the
.","In Kubernetes, controllers are control loops that watch the state of your
, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.

<!--more-->

Controllers watch the shared state of your cluster through the
 (part of the
).

Some controllers also run inside the control plane, providing control loops that
are core to Kubernetes' operations. For example: the deployment controller, the
daemonset controller, the namespace controller, and the persistent volume
controller (and others) all run within the
.","In Kubernetes, controllers are control loops that watch the state of your
, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.

<!--more-->

Controllers watch the shared state of your cluster through the
 (part of the
).

Some controllers also run inside the control plane, providing control loops that
are core to Kubernetes' operations. For example: the deployment controller, the
daemonset controller, the namespace controller, and the persistent volume
controller (and others) all run within the
.","In Kubernetes, controllers are control loops that watch the state of your
, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.

<!--more-->

Controllers watch the shared state of your cluster through the
 (part of the
).

Some controllers also run inside the control plane, providing control loops that
are core to Kubernetes' operations. For example: the deployment controller, the
daemonset controller, the namespace controller, and the persistent volume
controller (and others) all run within the
.","Capabilities provided to one or more  (CPU, memory, GPUs, etc), and made available for consumption by
 running on those nodes.

Kubernetes also uses the term _resource_ to describe an .

<!--more-->
Computers provide fundamental hardware facilities: processing power, storage memory, network, etc.
These resources have finite capacity, measured in a unit applicable to that resource (number of CPUs, bytes of memory, etc).
Kubernetes abstracts common [resources](/docs/concepts/configuration/manage-resources-containers/)
for allocation to workloads and utilizes operating system primitives (for example, Linux ) to manage consumption by ).

You can also use [dynamic resource allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) to
manage complex resource allocations automatically.","Capabilities provided to one or more  (CPU, memory, GPUs, etc), and made available for consumption by
 running on those nodes.

Kubernetes also uses the term _resource_ to describe an .

<!--more-->
Computers provide fundamental hardware facilities: processing power, storage memory, network, etc.
These resources have finite capacity, measured in a unit applicable to that resource (number of CPUs, bytes of memory, etc).
Kubernetes abstracts common [resources](/docs/concepts/configuration/manage-resources-containers/)
for allocation to workloads and utilizes operating system primitives (for example, Linux ) to manage consumption by ).

You can also use [dynamic resource allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) to
manage complex resource allocations automatically.","Capabilities provided to one or more  (CPU, memory, GPUs, etc), and made available for consumption by
 running on those nodes.

Kubernetes also uses the term _resource_ to describe an .

<!--more-->
Computers provide fundamental hardware facilities: processing power, storage memory, network, etc.
These resources have finite capacity, measured in a unit applicable to that resource (number of CPUs, bytes of memory, etc).
Kubernetes abstracts common [resources](/docs/concepts/configuration/manage-resources-containers/)
for allocation to workloads and utilizes operating system primitives (for example, Linux ) to manage consumption by ).

You can also use [dynamic resource allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) to
manage complex resource allocations automatically.","Capabilities provided to one or more  (CPU, memory, GPUs, etc), and made available for consumption by
 running on those nodes.

Kubernetes also uses the term _resource_ to describe an .

<!--more-->
Computers provide fundamental hardware facilities: processing power, storage memory, network, etc.
These resources have finite capacity, measured in a unit applicable to that resource (number of CPUs, bytes of memory, etc).
Kubernetes abstracts common [resources](/docs/concepts/configuration/manage-resources-containers/)
for allocation to workloads and utilizes operating system primitives (for example, Linux ) to manage consumption by ).

You can also use [dynamic resource allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/) to
manage complex resource allocations automatically.","A tool for quickly installing Kubernetes and setting up a secure cluster.

<!--more--> 

You can use kubeadm to install both the control plane and the  components.","A tool for quickly installing Kubernetes and setting up a secure cluster.

<!--more--> 

You can use kubeadm to install both the control plane and the  components.","A tool for quickly installing Kubernetes and setting up a secure cluster.

<!--more--> 

You can use kubeadm to install both the control plane and the  components.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

<!--more--> 

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

<!--more--> 

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

<!--more--> 

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

<!--more--> 

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","An open platform (not Kubernetes-specific) that provides a uniform way to integrate microservices, manage traffic flow, enforce policies, and aggregate telemetry data.

<!--more--> 

Adding Istio does not require changing application code. It is a layer of infrastructure between a service and the network, which when combined with service deployments, is commonly referred to as a service mesh. Istio's control plane abstracts away the underlying cluster management platform, which may be Kubernetes, Mesosphere, etc.","A container runtime with an emphasis on simplicity, robustness and portability

<!--more-->

containerd is a  runtime
that runs as a daemon on Linux or Windows. containerd takes care of fetching and
storing container images, executing containers, providing network access, and more.","A container runtime with an emphasis on simplicity, robustness and portability

<!--more-->

containerd is a  runtime
that runs as a daemon on Linux or Windows. containerd takes care of fetching and
storing container images, executing containers, providing network access, and more.","A container runtime with an emphasis on simplicity, robustness and portability

<!--more-->

containerd is a  runtime
that runs as a daemon on Linux or Windows. containerd takes care of fetching and
storing container images, executing containers, providing network access, and more.","An agent that runs on each  in the cluster. It makes sure that  are running in a .

<!--more-->


The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) takes a set of PodSpecs that 
are provided through various mechanisms and ensures that the containers described in those 
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by 
Kubernetes.","An agent that runs on each  in the cluster. It makes sure that  are running in a .

<!--more-->


The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) takes a set of PodSpecs that 
are provided through various mechanisms and ensures that the containers described in those 
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by 
Kubernetes.","An agent that runs on each  in the cluster. It makes sure that  are running in a .

<!--more-->


The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) takes a set of PodSpecs that 
are provided through various mechanisms and ensures that the containers described in those 
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by 
Kubernetes.","An agent that runs on each  in the cluster. It makes sure that  are running in a .

<!--more-->


The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) takes a set of PodSpecs that 
are provided through various mechanisms and ensures that the containers described in those 
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by 
Kubernetes.","An agent that runs on each  in the cluster. It makes sure that  are running in a .

<!--more-->


The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) takes a set of PodSpecs that 
are provided through various mechanisms and ensures that the containers described in those 
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by 
Kubernetes.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. 

<!--more-->

Kubernetes uses garbage collection to clean up resources like
[unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images),
[failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection),
[objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/),
[completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources
that have expired or failed.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. 

<!--more-->

Kubernetes uses garbage collection to clean up resources like
[unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images),
[failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection),
[objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/),
[completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources
that have expired or failed.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. 

<!--more-->

Kubernetes uses garbage collection to clean up resources like
[unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images),
[failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection),
[objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/),
[completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources
that have expired or failed.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. 

<!--more-->

Kubernetes uses garbage collection to clean up resources like
[unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images),
[failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection),
[objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/),
[completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources
that have expired or failed.","Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. 

<!--more-->

Kubernetes uses garbage collection to clean up resources like
[unused containers and images](/docs/concepts/architecture/garbage-collection/#containers-images),
[failed Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection),
[objects owned by the targeted resource](/docs/concepts/overview/working-with-objects/owners-dependents/),
[completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/), and resources
that have expired or failed.","Allows users to request automatic creation of storage  .

<!--more--> 

Dynamic provisioning eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage by user request. Dynamic volume provisioning is based on an API object, , referring to a  that provisions a  and the set of parameters to pass to the Volume Plugin.","Allows users to request automatic creation of storage  .

<!--more--> 

Dynamic provisioning eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage by user request. Dynamic volume provisioning is based on an API object, , referring to a  that provisions a  and the set of parameters to pass to the Volume Plugin.","Allows users to request automatic creation of storage  .

<!--more--> 

Dynamic provisioning eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage by user request. Dynamic volume provisioning is based on an API object, , referring to a  that provisions a  and the set of parameters to pass to the Volume Plugin.","cAdvisor (Container Advisor) provides container users an understanding of the 
usage and performance characteristics of their running .

<!--more-->

It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.","cAdvisor (Container Advisor) provides container users an understanding of the 
usage and performance characteristics of their running .

<!--more-->

It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.","cAdvisor (Container Advisor) provides container users an understanding of the 
usage and performance characteristics of their running .

<!--more-->

It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.","cAdvisor (Container Advisor) provides container users an understanding of the 
usage and performance characteristics of their running .

<!--more-->

It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.","cAdvisor (Container Advisor) provides container users an understanding of the 
usage and performance characteristics of their running .

<!--more-->

It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.","An entity in the Kubernetes system. An object is an
 that the Kubernetes API
uses to represent the state of your cluster.
<!--more-->
A Kubernetes object is typically a “record of intent”—once you create the object, the Kubernetes
 works constantly to ensure
that the item it represents actually exists.
By creating an object, you're effectively telling the Kubernetes system what you want that part of
your cluster's workload to look like; this is your cluster's desired state.","An entity in the Kubernetes system. An object is an
 that the Kubernetes API
uses to represent the state of your cluster.
<!--more-->
A Kubernetes object is typically a “record of intent”—once you create the object, the Kubernetes
 works constantly to ensure
that the item it represents actually exists.
By creating an object, you're effectively telling the Kubernetes system what you want that part of
your cluster's workload to look like; this is your cluster's desired state.","An entity in the Kubernetes system. An object is an
 that the Kubernetes API
uses to represent the state of your cluster.
<!--more-->
A Kubernetes object is typically a “record of intent”—once you create the object, the Kubernetes
 works constantly to ensure
that the item it represents actually exists.
By creating an object, you're effectively telling the Kubernetes system what you want that part of
your cluster's workload to look like; this is your cluster's desired state.","An entity in the Kubernetes system. An object is an
 that the Kubernetes API
uses to represent the state of your cluster.
<!--more-->
A Kubernetes object is typically a “record of intent”—once you create the object, the Kubernetes
 works constantly to ensure
that the item it represents actually exists.
By creating an object, you're effectively telling the Kubernetes system what you want that part of
your cluster's workload to look like; this is your cluster's desired state.","An entity in the Kubernetes system. An object is an
 that the Kubernetes API
uses to represent the state of your cluster.
<!--more-->
A Kubernetes object is typically a “record of intent”—once you create the object, the Kubernetes
 works constantly to ensure
that the item it represents actually exists.
By creating an object, you're effectively telling the Kubernetes system what you want that part of
your cluster's workload to look like; this is your cluster's desired state.","A  type that you can temporarily run inside a .

<!--more-->

If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics.
Ephemeral containers have no  or scheduling guarantees,
and you should not use them to run any part of the workload itself.

Ephemeral containers are not supported by .","A  type that you can temporarily run inside a .

<!--more-->

If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics.
Ephemeral containers have no  or scheduling guarantees,
and you should not use them to run any part of the workload itself.

Ephemeral containers are not supported by .","A  type that you can temporarily run inside a .

<!--more-->

If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics.
Ephemeral containers have no  or scheduling guarantees,
and you should not use them to run any part of the workload itself.

Ephemeral containers are not supported by .","A  type that you can temporarily run inside a .

<!--more-->

If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics.
Ephemeral containers have no  or scheduling guarantees,
and you should not use them to run any part of the workload itself.

Ephemeral containers are not supported by .","A  type that you can temporarily run inside a .

<!--more-->

If you want to investigate a Pod that's running with problems, you can add an ephemeral container to that Pod and carry out diagnostics.
Ephemeral containers have no  or scheduling guarantees,
and you should not use them to run any part of the workload itself.

Ephemeral containers are not supported by .","A Volume Plugin enables integration of storage within a .

<!--more--> 

A Volume Plugin lets you attach and mount storage volumes for use by a . Volume plugins can be _in tree_ or _out of tree_. _In tree_ plugins are part of the Kubernetes code repository and follow its release cycle. _Out of tree_ plugins are developed independently.","A Volume Plugin enables integration of storage within a .

<!--more--> 

A Volume Plugin lets you attach and mount storage volumes for use by a . Volume plugins can be _in tree_ or _out of tree_. _In tree_ plugins are part of the Kubernetes code repository and follow its release cycle. _Out of tree_ plugins are developed independently.","A Volume Plugin enables integration of storage within a .

<!--more--> 

A Volume Plugin lets you attach and mount storage volumes for use by a . Volume plugins can be _in tree_ or _out of tree_. _In tree_ plugins are part of the Kubernetes code repository and follow its release cycle. _Out of tree_ plugins are developed independently.","Defines a template that Kubernetes uses to create
. 
ResourceClaimTemplates are used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide _per-Pod access to separate, similar resources_.

<!--more-->

When a ResourceClaimTemplate is referenced in a workload specification,
Kubernetes automatically creates ResourceClaim objects based on the template.
Each ResourceClaim is bound to a specific Pod. When the Pod terminates,
Kubernetes deletes the corresponding ResourceClaim.","Defines a template that Kubernetes uses to create
. 
ResourceClaimTemplates are used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide _per-Pod access to separate, similar resources_.

<!--more-->

When a ResourceClaimTemplate is referenced in a workload specification,
Kubernetes automatically creates ResourceClaim objects based on the template.
Each ResourceClaim is bound to a specific Pod. When the Pod terminates,
Kubernetes deletes the corresponding ResourceClaim.","Defines a template that Kubernetes uses to create
. 
ResourceClaimTemplates are used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide _per-Pod access to separate, similar resources_.

<!--more-->

When a ResourceClaimTemplate is referenced in a workload specification,
Kubernetes automatically creates ResourceClaim objects based on the template.
Each ResourceClaim is bound to a specific Pod. When the Pod terminates,
Kubernetes deletes the corresponding ResourceClaim.","Defines a template that Kubernetes uses to create
. 
ResourceClaimTemplates are used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide _per-Pod access to separate, similar resources_.

<!--more-->

When a ResourceClaimTemplate is referenced in a workload specification,
Kubernetes automatically creates ResourceClaim objects based on the template.
Each ResourceClaim is bound to a specific Pod. When the Pod terminates,
Kubernetes deletes the corresponding ResourceClaim.","Defines a template that Kubernetes uses to create
. 
ResourceClaimTemplates are used in
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
to provide _per-Pod access to separate, similar resources_.

<!--more-->

When a ResourceClaimTemplate is referenced in a workload specification,
Kubernetes automatically creates ResourceClaim objects based on the template.
Each ResourceClaim is bound to a specific Pod. When the Pod terminates,
Kubernetes deletes the corresponding ResourceClaim.","Specification of a Kubernetes API object in [JSON](https://www.json.org/json-en.html)
or [YAML](https://yaml.org/) format.

<!--more-->
A manifest specifies the desired state of an object that Kubernetes will maintain when you apply the manifest.
For YAML format, each file can contain multiple manifests.","Specification of a Kubernetes API object in [JSON](https://www.json.org/json-en.html)
or [YAML](https://yaml.org/) format.

<!--more-->
A manifest specifies the desired state of an object that Kubernetes will maintain when you apply the manifest.
For YAML format, each file can contain multiple manifests.","Specification of a Kubernetes API object in [JSON](https://www.json.org/json-en.html)
or [YAML](https://yaml.org/) format.

<!--more-->
A manifest specifies the desired state of an object that Kubernetes will maintain when you apply the manifest.
For YAML format, each file can contain multiple manifests.","Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes 
that are marked for deletion.
Finalizers alert 
to clean up resources the deleted object owned.

<!--more-->

When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control 
of resources. For example, you can define a finalizer to clean up related
 or infrastructure before the controller
deletes the object being finalized.","Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes 
that are marked for deletion.
Finalizers alert 
to clean up resources the deleted object owned.

<!--more-->

When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control 
of resources. For example, you can define a finalizer to clean up related
 or infrastructure before the controller
deletes the object being finalized.","Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes 
that are marked for deletion.
Finalizers alert 
to clean up resources the deleted object owned.

<!--more-->

When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control 
of resources. For example, you can define a finalizer to clean up related
 or infrastructure before the controller
deletes the object being finalized.","Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes 
that are marked for deletion.
Finalizers alert 
to clean up resources the deleted object owned.

<!--more-->

When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control 
of resources. For example, you can define a finalizer to clean up related
 or infrastructure before the controller
deletes the object being finalized.","Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes 
that are marked for deletion.
Finalizers alert 
to clean up resources the deleted object owned.

<!--more-->

When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP ""Accepted""). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete and deletes the object.

You can use finalizers to control 
of resources. For example, you can define a finalizer to clean up related
 or infrastructure before the controller
deletes the object being finalized.","A group of Linux processes with optional  isolation, accounting and limits.

<!--more--> 

cgroup is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network) for a collection of processes.","A group of Linux processes with optional  isolation, accounting and limits.

<!--more--> 

cgroup is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network) for a collection of processes.","A group of Linux processes with optional  isolation, accounting and limits.

<!--more--> 

cgroup is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network) for a collection of processes.","An API object that manages a replicated application, typically by running Pods with no local state.

<!--more--> 

Each replica is represented by a , and the Pods are distributed among the 
 of a cluster.
For workloads that do require local state, consider using a .","An API object that manages a replicated application, typically by running Pods with no local state.

<!--more--> 

Each replica is represented by a , and the Pods are distributed among the 
 of a cluster.
For workloads that do require local state, consider using a .","An API object that manages a replicated application, typically by running Pods with no local state.

<!--more--> 

Each replica is represented by a , and the Pods are distributed among the 
 of a cluster.
For workloads that do require local state, consider using a .","A cryptographically secure file used to validate access to the Kubernetes cluster.

<!--more--> 

Certificates enable applications within a Kubernetes cluster to access the Kubernetes API securely. Certificates validate that clients are allowed to access the API.","A cryptographically secure file used to validate access to the Kubernetes cluster.

<!--more--> 

Certificates enable applications within a Kubernetes cluster to access the Kubernetes API securely. Certificates validate that clients are allowed to access the API.","A cryptographically secure file used to validate access to the Kubernetes cluster.

<!--more--> 

Certificates enable applications within a Kubernetes cluster to access the Kubernetes API securely. Certificates validate that clients are allowed to access the API.",,,,,,"Facilitates the discussion and/or implementation of a short-lived, narrow, or decoupled project for a committee, , or cross-SIG effort.

<!--more-->

Working groups are a way of organizing people to accomplish a discrete task.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and working groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Facilitates the discussion and/or implementation of a short-lived, narrow, or decoupled project for a committee, , or cross-SIG effort.

<!--more-->

Working groups are a way of organizing people to accomplish a discrete task.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and working groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Facilitates the discussion and/or implementation of a short-lived, narrow, or decoupled project for a committee, , or cross-SIG effort.

<!--more-->

Working groups are a way of organizing people to accomplish a discrete task.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and working groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","Logs are the list of events that are logged by  or application.

<!--more--> 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Logs are the list of events that are logged by  or application.

<!--more--> 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Logs are the list of events that are logged by  or application.

<!--more--> 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Logs are the list of events that are logged by  or application.

<!--more--> 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Logs are the list of events that are logged by  or application.

<!--more--> 

Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.","Terms under which a  grants a license to an open source project for their contributions.

<!--more--> 

CLAs help resolve legal disputes involving contributed material and intellectual property (IP).","Terms under which a  grants a license to an open source project for their contributions.

<!--more--> 

CLAs help resolve legal disputes involving contributed material and intellectual property (IP).","Terms under which a  grants a license to an open source project for their contributions.

<!--more--> 

CLAs help resolve legal disputes involving contributed material and intellectual property (IP).","The infrastructure layer provides and maintains VMs, networking, security groups and others.","The infrastructure layer provides and maintains VMs, networking, security groups and others.","The infrastructure layer provides and maintains VMs, networking, security groups and others.","A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of  on  or node groups.

<!--more-->

Taints and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node. A node should only schedule a Pod with the matching tolerations for the configured taints.","A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of  on  or node groups.

<!--more-->

Taints and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node. A node should only schedule a Pod with the matching tolerations for the configured taints.","A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of  on  or node groups.

<!--more-->

Taints and  work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node. A node should only schedule a Pod with the matching tolerations for the configured taints.","A software offering maintained by a third-party provider.

<!--more--> 

Some examples of Managed Services are AWS EC2, Azure SQL Database, and
GCP Pub/Sub, but they can be any software offering that can be used by an application.","A software offering maintained by a third-party provider.

<!--more--> 

Some examples of Managed Services are AWS EC2, Azure SQL Database, and
GCP Pub/Sub, but they can be any software offering that can be used by an application.","A software offering maintained by a third-party provider.

<!--more--> 

Some examples of Managed Services are AWS EC2, Azure SQL Database, and
GCP Pub/Sub, but they can be any software offering that can be used by an application.","A  managed directly by the 
 daemon on a specific node,
<!--more-->

without the API server observing it.

Static Pods do not support .","A  managed directly by the 
 daemon on a specific node,
<!--more-->

without the API server observing it.

Static Pods do not support .","A  managed directly by the 
 daemon on a specific node,
<!--more-->

without the API server observing it.

Static Pods do not support .","Manages the deployment and scaling of a set of , *and provides guarantees about the ordering and uniqueness* of these Pods.

<!--more--> 

Like a , a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.

If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.","Manages the deployment and scaling of a set of , *and provides guarantees about the ordering and uniqueness* of these Pods.

<!--more--> 

Like a , a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.

If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.","Manages the deployment and scaling of a set of , *and provides guarantees about the ordering and uniqueness* of these Pods.

<!--more--> 

Like a , a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.

If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.","Manages the deployment and scaling of a set of , *and provides guarantees about the ordering and uniqueness* of these Pods.

<!--more--> 

Like a , a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.

If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.","Object that constrains aggregate resource
consumption, per .

<!--more-->

A ResourceQuota can either limits the quantity of 
that can be created in a namespace by type, or it can set a limit on the total amount of

that may be consumed on behalf of the namespace (and the objects within it).","Object that constrains aggregate resource
consumption, per .

<!--more-->

A ResourceQuota can either limits the quantity of 
that can be created in a namespace by type, or it can set a limit on the total amount of

that may be consumed on behalf of the namespace (and the objects within it).","Object that constrains aggregate resource
consumption, per .

<!--more-->

A ResourceQuota can either limits the quantity of 
that can be created in a namespace by type, or it can set a limit on the total amount of

that may be consumed on behalf of the namespace (and the objects within it).","A kernel feature to emulate root. Used for ""rootless containers"".

<!--more-->

User namespaces are a Linux kernel feature that allows a non-root user to
emulate superuser (""root"") privileges,
for example in order to run containers without being a superuser outside the container.

User namespace is effective for mitigating damage of potential container break-out attacks.

In the context of user namespaces, the namespace is a Linux kernel feature, and not a
 in the Kubernetes sense
of the term.

<!-- TODO: https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/ -->","A kernel feature to emulate root. Used for ""rootless containers"".

<!--more-->

User namespaces are a Linux kernel feature that allows a non-root user to
emulate superuser (""root"") privileges,
for example in order to run containers without being a superuser outside the container.

User namespace is effective for mitigating damage of potential container break-out attacks.

In the context of user namespaces, the namespace is a Linux kernel feature, and not a
 in the Kubernetes sense
of the term.

<!-- TODO: https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/ -->","A kernel feature to emulate root. Used for ""rootless containers"".

<!--more-->

User namespaces are a Linux kernel feature that allows a non-root user to
emulate superuser (""root"") privileges,
for example in order to run containers without being a superuser outside the container.

User namespace is effective for mitigating damage of potential container break-out attacks.

In the context of user namespaces, the namespace is a Linux kernel feature, and not a
 in the Kubernetes sense
of the term.

<!-- TODO: https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/ -->","A kernel feature to emulate root. Used for ""rootless containers"".

<!--more-->

User namespaces are a Linux kernel feature that allows a non-root user to
emulate superuser (""root"") privileges,
for example in order to run containers without being a superuser outside the container.

User namespace is effective for mitigating damage of potential container break-out attacks.

In the context of user namespaces, the namespace is a Linux kernel feature, and not a
 in the Kubernetes sense
of the term.

<!-- TODO: https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/ -->","A kernel feature to emulate root. Used for ""rootless containers"".

<!--more-->

User namespaces are a Linux kernel feature that allows a non-root user to
emulate superuser (""root"") privileges,
for example in order to run containers without being a superuser outside the container.

User namespace is effective for mitigating damage of potential container break-out attacks.

In the context of user namespaces, the namespace is a Linux kernel feature, and not a
 in the Kubernetes sense
of the term.

<!-- TODO: https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/ -->","One or more initialization  that must run to completion before any app containers run.

<!--more--> 

Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins.

Unlike , init containers do not remain running after Pod startup.

For more information, read [init containers](/docs/concepts/workloads/pods/init-containers/).","One or more initialization  that must run to completion before any app containers run.

<!--more--> 

Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins.

Unlike , init containers do not remain running after Pod startup.

For more information, read [init containers](/docs/concepts/workloads/pods/init-containers/).","One or more initialization  that must run to completion before any app containers run.

<!--more--> 

Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins.

Unlike , init containers do not remain running after Pod startup.

For more information, read [init containers](/docs/concepts/workloads/pods/init-containers/).","One or more initialization  that must run to completion before any app containers run.

<!--more--> 

Initialization (init) containers are like regular app containers, with one difference: init containers must run to completion before any app containers can start. Init containers run in series: each init container must run to completion before the next init container begins.

Unlike , init containers do not remain running after Pod startup.

For more information, read [init containers](/docs/concepts/workloads/pods/init-containers/).","A family of API kinds for modeling service networking in Kubernetes.

<!--more--> 

Gateway API provides a family of extensible, role-oriented, protocol-aware
API kinds for modeling service networking in Kubernetes.","A family of API kinds for modeling service networking in Kubernetes.

<!--more--> 

Gateway API provides a family of extensible, role-oriented, protocol-aware
API kinds for modeling service networking in Kubernetes.","A family of API kinds for modeling service networking in Kubernetes.

<!--more--> 

Gateway API provides a family of extensible, role-oriented, protocol-aware
API kinds for modeling service networking in Kubernetes.","A family of API kinds for modeling service networking in Kubernetes.

<!--more--> 

Gateway API provides a family of extensible, role-oriented, protocol-aware
API kinds for modeling service networking in Kubernetes.","A family of API kinds for modeling service networking in Kubernetes.

<!--more--> 

Gateway API provides a family of extensible, role-oriented, protocol-aware
API kinds for modeling service networking in Kubernetes.","A Kubernetes  that describes state changes
or notable occurrences in the cluster.

<!--more-->
Events have a limited retention time and triggers and messages may evolve with time.
Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger,
or the continued existence of events with that reason.

Events should be treated as informative, best-effort, supplemental data.

In Kubernetes, [auditing](/docs/tasks/debug/debug-cluster/audit/) generates a different kind of
Event record (API group `audit.k8s.io`).","A Kubernetes  that describes state changes
or notable occurrences in the cluster.

<!--more-->
Events have a limited retention time and triggers and messages may evolve with time.
Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger,
or the continued existence of events with that reason.

Events should be treated as informative, best-effort, supplemental data.

In Kubernetes, [auditing](/docs/tasks/debug/debug-cluster/audit/) generates a different kind of
Event record (API group `audit.k8s.io`).","A Kubernetes  that describes state changes
or notable occurrences in the cluster.

<!--more-->
Events have a limited retention time and triggers and messages may evolve with time.
Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger,
or the continued existence of events with that reason.

Events should be treated as informative, best-effort, supplemental data.

In Kubernetes, [auditing](/docs/tasks/debug/debug-cluster/audit/) generates a different kind of
Event record (API group `audit.k8s.io`).","A Kubernetes  that describes state changes
or notable occurrences in the cluster.

<!--more-->
Events have a limited retention time and triggers and messages may evolve with time.
Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger,
or the continued existence of events with that reason.

Events should be treated as informative, best-effort, supplemental data.

In Kubernetes, [auditing](/docs/tasks/debug/debug-cluster/audit/) generates a different kind of
Event record (API group `audit.k8s.io`).","A Kubernetes  that describes state changes
or notable occurrences in the cluster.

<!--more-->
Events have a limited retention time and triggers and messages may evolve with time.
Event consumers should not rely on the timing of an event with a given reason reflecting a consistent underlying trigger,
or the continued existence of events with that reason.

Events should be treated as informative, best-effort, supplemental data.

In Kubernetes, [auditing](/docs/tasks/debug/debug-cluster/audit/) generates a different kind of
Event record (API group `audit.k8s.io`).","A key-value pair that is used to attach arbitrary non-identifying metadata to objects.

<!--more--> 

The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by . Clients such as tools and libraries can retrieve this metadata.","A key-value pair that is used to attach arbitrary non-identifying metadata to objects.

<!--more--> 

The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by . Clients such as tools and libraries can retrieve this metadata.","A key-value pair that is used to attach arbitrary non-identifying metadata to objects.

<!--more--> 

The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by . Clients such as tools and libraries can retrieve this metadata.","A key-value pair that is used to attach arbitrary non-identifying metadata to objects.

<!--more--> 

The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by . Clients such as tools and libraries can retrieve this metadata.","A key-value pair that is used to attach arbitrary non-identifying metadata to objects.

<!--more--> 

The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by . Clients such as tools and libraries can retrieve this metadata.","API-initiated eviction is the process by which you use the [Eviction API](/docs/reference/generated/kubernetes-api//#create-eviction-pod-v1-core)
to create an `Eviction` object that triggers graceful pod termination.

<!--more-->

You can request eviction either by directly calling the Eviction API 
using a client of the kube-apiserver, like the `kubectl drain` command. 
When an `Eviction` object is created, the API server terminates the Pod. 

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)
and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).

API-initiated eviction is not the same as [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).

* See [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/) for more information.","API-initiated eviction is the process by which you use the [Eviction API](/docs/reference/generated/kubernetes-api//#create-eviction-pod-v1-core)
to create an `Eviction` object that triggers graceful pod termination.

<!--more-->

You can request eviction either by directly calling the Eviction API 
using a client of the kube-apiserver, like the `kubectl drain` command. 
When an `Eviction` object is created, the API server terminates the Pod. 

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)
and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).

API-initiated eviction is not the same as [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).

* See [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/) for more information.","API-initiated eviction is the process by which you use the [Eviction API](/docs/reference/generated/kubernetes-api//#create-eviction-pod-v1-core)
to create an `Eviction` object that triggers graceful pod termination.

<!--more-->

You can request eviction either by directly calling the Eviction API 
using a client of the kube-apiserver, like the `kubectl drain` command. 
When an `Eviction` object is created, the API server terminates the Pod. 

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)
and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).

API-initiated eviction is not the same as [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).

* See [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/) for more information.","API-initiated eviction is the process by which you use the [Eviction API](/docs/reference/generated/kubernetes-api//#create-eviction-pod-v1-core)
to create an `Eviction` object that triggers graceful pod termination.

<!--more-->

You can request eviction either by directly calling the Eviction API 
using a client of the kube-apiserver, like the `kubectl drain` command. 
When an `Eviction` object is created, the API server terminates the Pod. 

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)
and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).

API-initiated eviction is not the same as [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).

* See [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/) for more information.","A method for exposing a network application that is running as one or more
 in your cluster.

<!--more-->

The set of Pods targeted by a Service is (usually) determined by a
. If more Pods are added or removed,
the set of Pods matching the selector will change. The Service makes sure that network traffic
can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in
the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","A method for exposing a network application that is running as one or more
 in your cluster.

<!--more-->

The set of Pods targeted by a Service is (usually) determined by a
. If more Pods are added or removed,
the set of Pods matching the selector will change. The Service makes sure that network traffic
can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in
the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","A method for exposing a network application that is running as one or more
 in your cluster.

<!--more-->

The set of Pods targeted by a Service is (usually) determined by a
. If more Pods are added or removed,
the set of Pods matching the selector will change. The Service makes sure that network traffic
can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in
the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","A method for exposing a network application that is running as one or more
 in your cluster.

<!--more-->

The set of Pods targeted by a Service is (usually) determined by a
. If more Pods are added or removed,
the set of Pods matching the selector will change. The Service makes sure that network traffic
can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in
the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","A method for exposing a network application that is running as one or more
 in your cluster.

<!--more-->

The set of Pods targeted by a Service is (usually) determined by a
. If more Pods are added or removed,
the set of Pods matching the selector will change. The Service makes sure that network traffic
can be directed to the current set of Pods for the workload.

Kubernetes Services either use IP networking (IPv4, IPv6, or both), or reference an external name in
the Domain Name System (DNS).

The Service abstraction enables other mechanisms, such as Ingress and Gateway.","The `securityContext` field defines privilege and access control settings for
a  or
.

<!--more-->

In a `securityContext`, you can define: the user that processes run as,
the group that processes run as, and privilege settings.
You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","The `securityContext` field defines privilege and access control settings for
a  or
.

<!--more-->

In a `securityContext`, you can define: the user that processes run as,
the group that processes run as, and privilege settings.
You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","The `securityContext` field defines privilege and access control settings for
a  or
.

<!--more-->

In a `securityContext`, you can define: the user that processes run as,
the group that processes run as, and privilege settings.
You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","The `securityContext` field defines privilege and access control settings for
a  or
.

<!--more-->

In a `securityContext`, you can define: the user that processes run as,
the group that processes run as, and privilege settings.
You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","The `securityContext` field defines privilege and access control settings for
a  or
.

<!--more-->

In a `securityContext`, you can define: the user that processes run as,
the group that processes run as, and privilege settings.
You can also configure security policies (for example: SELinux, AppArmor or seccomp).

The `PodSpec.securityContext` setting applies to all containers in a Pod.","Eviction is the process of terminating one or more Pods on Nodes.

<!--more-->
There are two kinds of eviction:
* [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
* [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/)","Eviction is the process of terminating one or more Pods on Nodes.

<!--more-->
There are two kinds of eviction:
* [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
* [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/)","Eviction is the process of terminating one or more Pods on Nodes.

<!--more-->
There are two kinds of eviction:
* [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
* [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/)","A tool for running Kubernetes locally.

<!--more--> 

Minikube runs an all-in-one or a multi-node local Kubernetes cluster inside a VM on your computer.
You can use Minikube to
[try Kubernetes in a learning environment](/docs/tasks/tools/#minikube).","A tool for running Kubernetes locally.

<!--more--> 

Minikube runs an all-in-one or a multi-node local Kubernetes cluster inside a VM on your computer.
You can use Minikube to
[try Kubernetes in a learning environment](/docs/tasks/tools/#minikube).","A tool for running Kubernetes locally.

<!--more--> 

Minikube runs an all-in-one or a multi-node local Kubernetes cluster inside a VM on your computer.
You can use Minikube to
[try Kubernetes in a learning environment](/docs/tasks/tools/#minikube).","Defines how each object, like Pods or Services, should be configured and its desired state.

<!--more-->
Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the  to have: its desired state.

It varies for different objects like Pods, StatefulSets, and Services, detailing settings such as containers, volumes, replicas, ports,
and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined
object.","Defines how each object, like Pods or Services, should be configured and its desired state.

<!--more-->
Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the  to have: its desired state.

It varies for different objects like Pods, StatefulSets, and Services, detailing settings such as containers, volumes, replicas, ports,
and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined
object.","Defines how each object, like Pods or Services, should be configured and its desired state.

<!--more-->
Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the  to have: its desired state.

It varies for different objects like Pods, StatefulSets, and Services, detailing settings such as containers, volumes, replicas, ports,
and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined
object.","Defines how each object, like Pods or Services, should be configured and its desired state.

<!--more-->
Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the  to have: its desired state.

It varies for different objects like Pods, StatefulSets, and Services, detailing settings such as containers, volumes, replicas, ports,
and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined
object.","Defines how each object, like Pods or Services, should be configured and its desired state.

<!--more-->
Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the  to have: its desired state.

It varies for different objects like Pods, StatefulSets, and Services, detailing settings such as containers, volumes, replicas, ports,
and other specifications unique to each object type. This field encapsulates what state Kubernetes should maintain for the defined
object.","A person who designs infrastructure that involves one or more Kubernetes clusters.

<!--more--> 

Cluster architects are concerned with best practices for distributed systems, for example&#58; high availability and security.","A person who designs infrastructure that involves one or more Kubernetes clusters.

<!--more--> 

Cluster architects are concerned with best practices for distributed systems, for example&#58; high availability and security.","A person who designs infrastructure that involves one or more Kubernetes clusters.

<!--more--> 

Cluster architects are concerned with best practices for distributed systems, for example&#58; high availability and security.","A person who designs infrastructure that involves one or more Kubernetes clusters.

<!--more--> 

Cluster architects are concerned with best practices for distributed systems, for example&#58; high availability and security.","A person who designs infrastructure that involves one or more Kubernetes clusters.

<!--more--> 

Cluster architects are concerned with best practices for distributed systems, for example&#58; high availability and security.","A fundamental component that empowers Kubernetes to run containers effectively.
 It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.

<!--more-->

Kubernetes supports container runtimes such as
, ,
and any other implementation of the [Kubernetes CRI (Container Runtime
Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).","A fundamental component that empowers Kubernetes to run containers effectively.
 It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.

<!--more-->

Kubernetes supports container runtimes such as
, ,
and any other implementation of the [Kubernetes CRI (Container Runtime
Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).","A fundamental component that empowers Kubernetes to run containers effectively.
 It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.

<!--more-->

Kubernetes supports container runtimes such as
, ,
and any other implementation of the [Kubernetes CRI (Container Runtime
Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).","Manages authorization decisions, allowing admins to dynamically configure access policies through the .

<!--more--> 

RBAC utilizes four kinds of Kubernetes objects:

Role
: Defines permission rules in a specific namespace.

ClusterRole
: Defines permission rules cluster-wide.

RoleBinding
: Grants the permissions defined in a role to a set of users in a specific namespace.

ClusterRoleBinding
: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","Manages authorization decisions, allowing admins to dynamically configure access policies through the .

<!--more--> 

RBAC utilizes four kinds of Kubernetes objects:

Role
: Defines permission rules in a specific namespace.

ClusterRole
: Defines permission rules cluster-wide.

RoleBinding
: Grants the permissions defined in a role to a set of users in a specific namespace.

ClusterRoleBinding
: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","Manages authorization decisions, allowing admins to dynamically configure access policies through the .

<!--more--> 

RBAC utilizes four kinds of Kubernetes objects:

Role
: Defines permission rules in a specific namespace.

ClusterRole
: Defines permission rules cluster-wide.

RoleBinding
: Grants the permissions defined in a role to a set of users in a specific namespace.

ClusterRoleBinding
: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","Manages authorization decisions, allowing admins to dynamically configure access policies through the .

<!--more--> 

RBAC utilizes four kinds of Kubernetes objects:

Role
: Defines permission rules in a specific namespace.

ClusterRole
: Defines permission rules cluster-wide.

RoleBinding
: Grants the permissions defined in a role to a set of users in a specific namespace.

ClusterRoleBinding
: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","Manages authorization decisions, allowing admins to dynamically configure access policies through the .

<!--more--> 

RBAC utilizes four kinds of Kubernetes objects:

Role
: Defines permission rules in a specific namespace.

ClusterRole
: Defines permission rules cluster-wide.

RoleBinding
: Grants the permissions defined in a role to a set of users in a specific namespace.

ClusterRoleBinding
: Grants the permissions defined in a role to a set of users cluster-wide.

For more information, see [RBAC](/docs/reference/access-authn-authz/rbac/).","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods.

<!--more-->
There are two kinds of affinity:
* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes ,
and  specified in , 
and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods.

<!--more-->
There are two kinds of affinity:
* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes ,
and  specified in , 
and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods.

<!--more-->
There are two kinds of affinity:
* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes ,
and  specified in , 
and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods.

<!--more-->
There are two kinds of affinity:
* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes ,
and  specified in , 
and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","In Kubernetes, _affinity_ is a set of rules that give hints to the scheduler about where to place pods.

<!--more-->
There are two kinds of affinity:
* [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)
* [pod-to-pod affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)

The rules are defined using the Kubernetes ,
and  specified in , 
and they can be either required or preferred, depending on how strictly you want the scheduler to enforce them.","A former Kubernetes API that enforced security restrictions during  creation and updates.

<!--more--> 

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25.
As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.","A former Kubernetes API that enforced security restrictions during  creation and updates.

<!--more--> 

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25.
As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.","A former Kubernetes API that enforced security restrictions during  creation and updates.

<!--more--> 

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25.
As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.","A former Kubernetes API that enforced security restrictions during  creation and updates.

<!--more--> 

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25.
As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.","A former Kubernetes API that enforced security restrictions during  creation and updates.

<!--more--> 

PodSecurityPolicy was deprecated as of Kubernetes v1.21, and removed in v1.25.
As an alternative, use [Pod Security Admission](/docs/concepts/security/pod-security-admission/) or a 3rd party admission plugin.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an 
 application owner to create an object for a replicated application, that ensures 
 a certain number or percentage of 
 with an assigned label will not be voluntarily evicted at any point in time.

<!--more--> 

Involuntary disruptions cannot be prevented by PDBs; however they 
do count against the budget.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an 
 application owner to create an object for a replicated application, that ensures 
 a certain number or percentage of 
 with an assigned label will not be voluntarily evicted at any point in time.

<!--more--> 

Involuntary disruptions cannot be prevented by PDBs; however they 
do count against the budget.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an 
 application owner to create an object for a replicated application, that ensures 
 a certain number or percentage of 
 with an assigned label will not be voluntarily evicted at any point in time.

<!--more--> 

Involuntary disruptions cannot be prevented by PDBs; however they 
do count against the budget.","A [Pod Disruption Budget](/docs/concepts/workloads/pods/disruptions/) allows an 
 application owner to create an object for a replicated application, that ensures 
 a certain number or percentage of 
 with an assigned label will not be voluntarily evicted at any point in time.

<!--more--> 

Involuntary disruptions cannot be prevented by PDBs; however they 
do count against the budget.","A ReplicaSet (aims to) maintain a set of replica Pods running at any given time.

<!--more-->

Workload objects such as  make use of ReplicaSets
to ensure that the configured number of  are
running in your cluster, based on the spec of that ReplicaSet.","A ReplicaSet (aims to) maintain a set of replica Pods running at any given time.

<!--more-->

Workload objects such as  make use of ReplicaSets
to ensure that the configured number of  are
running in your cluster, based on the spec of that ReplicaSet.","A ReplicaSet (aims to) maintain a set of replica Pods running at any given time.

<!--more-->

Workload objects such as  make use of ReplicaSets
to ensure that the configured number of  are
running in your cluster, based on the spec of that ReplicaSet.","The smallest and simplest Kubernetes object. A Pod represents a set of running  on your cluster.

<!--more--> 

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a .","The smallest and simplest Kubernetes object. A Pod represents a set of running  on your cluster.

<!--more--> 

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a .","The smallest and simplest Kubernetes object. A Pod represents a set of running  on your cluster.

<!--more--> 

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a .","The smallest and simplest Kubernetes object. A Pod represents a set of running  on your cluster.

<!--more--> 

A Pod is typically set up to run a single primary container. It can also run optional sidecar containers that add supplementary features like logging. Pods are commonly managed by a .","A copy or duplicate of a  or
a set of pods. Replicas ensure high availability, scalability, and fault tolerance
by maintaining multiple identical instances of a pod.

<!--more-->
Replicas are commonly used in Kubernetes to achieve the desired application state and reliability.
They enable workload scaling and distribution across multiple nodes in a cluster.

By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that
the specified number of instances are running, automatically adjusting the count as needed.

Replica management allows for efficient load balancing, rolling updates, and
self-healing capabilities in a Kubernetes cluster.","A copy or duplicate of a  or
a set of pods. Replicas ensure high availability, scalability, and fault tolerance
by maintaining multiple identical instances of a pod.

<!--more-->
Replicas are commonly used in Kubernetes to achieve the desired application state and reliability.
They enable workload scaling and distribution across multiple nodes in a cluster.

By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that
the specified number of instances are running, automatically adjusting the count as needed.

Replica management allows for efficient load balancing, rolling updates, and
self-healing capabilities in a Kubernetes cluster.","A copy or duplicate of a  or
a set of pods. Replicas ensure high availability, scalability, and fault tolerance
by maintaining multiple identical instances of a pod.

<!--more-->
Replicas are commonly used in Kubernetes to achieve the desired application state and reliability.
They enable workload scaling and distribution across multiple nodes in a cluster.

By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that
the specified number of instances are running, automatically adjusting the count as needed.

Replica management allows for efficient load balancing, rolling updates, and
self-healing capabilities in a Kubernetes cluster.","A copy or duplicate of a  or
a set of pods. Replicas ensure high availability, scalability, and fault tolerance
by maintaining multiple identical instances of a pod.

<!--more-->
Replicas are commonly used in Kubernetes to achieve the desired application state and reliability.
They enable workload scaling and distribution across multiple nodes in a cluster.

By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that
the specified number of instances are running, automatically adjusting the count as needed.

Replica management allows for efficient load balancing, rolling updates, and
self-healing capabilities in a Kubernetes cluster.","A copy or duplicate of a  or
a set of pods. Replicas ensure high availability, scalability, and fault tolerance
by maintaining multiple identical instances of a pod.

<!--more-->
Replicas are commonly used in Kubernetes to achieve the desired application state and reliability.
They enable workload scaling and distribution across multiple nodes in a cluster.

By defining the number of replicas in a Deployment or ReplicaSet, Kubernetes ensures that
the specified number of instances are running, automatically adjusting the count as needed.

Replica management allows for efficient load balancing, rolling updates, and
self-healing capabilities in a Kubernetes cluster.","An entity in the Kubernetes type system, corresponding to an endpoint on the .
A resource typically represents an .
Some resources represent an operation on other objects, such as a permission check.
<!--more-->
Each resource represents an HTTP endpoint (URI) on the Kubernetes API server, defining the schema for the objects or operations on that resource.","An entity in the Kubernetes type system, corresponding to an endpoint on the .
A resource typically represents an .
Some resources represent an operation on other objects, such as a permission check.
<!--more-->
Each resource represents an HTTP endpoint (URI) on the Kubernetes API server, defining the schema for the objects or operations on that resource.","An entity in the Kubernetes type system, corresponding to an endpoint on the .
A resource typically represents an .
Some resources represent an operation on other objects, such as a permission check.
<!--more-->
Each resource represents an HTTP endpoint (URI) on the Kubernetes API server, defining the schema for the objects or operations on that resource.","An entity in the Kubernetes type system, corresponding to an endpoint on the .
A resource typically represents an .
Some resources represent an operation on other objects, such as a permission check.
<!--more-->
Each resource represents an HTTP endpoint (URI) on the Kubernetes API server, defining the schema for the objects or operations on that resource.","A continuously active  in the K8s community.

<!--more--> 

Members can have issues and PRs assigned to them and participate in  through GitHub teams. Pre-submit tests are automatically run for members' PRs. A member is expected to remain an active contributor to the community.","A continuously active  in the K8s community.

<!--more--> 

Members can have issues and PRs assigned to them and participate in  through GitHub teams. Pre-submit tests are automatically run for members' PRs. A member is expected to remain an active contributor to the community.","A continuously active  in the K8s community.

<!--more--> 

Members can have issues and PRs assigned to them and participate in  through GitHub teams. Pre-submit tests are automatically run for members' PRs. A member is expected to remain an active contributor to the community.","A Kubernetes  component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.

<!--more-->

By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.","A Kubernetes  component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.

<!--more-->

By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.","A Kubernetes  component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.

<!--more-->

By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.","Docker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as .

<!--more-->

Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).","Docker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as .

<!--more-->

Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).","Docker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as .

<!--more-->

Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).","A workload is an application running on Kubernetes.

<!--more--> 

Various core objects that represent different types or parts of a workload
include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects.

For example, a workload that has a web server and a database might run the
database in one  and the web server
in a .","A workload is an application running on Kubernetes.

<!--more--> 

Various core objects that represent different types or parts of a workload
include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects.

For example, a workload that has a web server and a database might run the
database in one  and the web server
in a .","A workload is an application running on Kubernetes.

<!--more--> 

Various core objects that represent different types or parts of a workload
include the DaemonSet, Deployment, Job, ReplicaSet, and StatefulSet objects.

For example, a workload that has a web server and a database might run the
database in one  and the web server
in a .","A general-purpose expression language that's designed to be fast, portable, and
safe to execute.

<!--more-->

In Kubernetes, CEL can be used to run queries and perform fine-grained
filtering. For example, you can use CEL expressions with
[dynamic admission control](/docs/reference/access-authn-authz/extensible-admission-controllers/)
to filter for specific fields in requests, and with
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation)
to select resources based on specific attributes.","A general-purpose expression language that's designed to be fast, portable, and
safe to execute.

<!--more-->

In Kubernetes, CEL can be used to run queries and perform fine-grained
filtering. For example, you can use CEL expressions with
[dynamic admission control](/docs/reference/access-authn-authz/extensible-admission-controllers/)
to filter for specific fields in requests, and with
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation)
to select resources based on specific attributes.","A general-purpose expression language that's designed to be fast, portable, and
safe to execute.

<!--more-->

In Kubernetes, CEL can be used to run queries and perform fine-grained
filtering. For example, you can use CEL expressions with
[dynamic admission control](/docs/reference/access-authn-authz/extensible-admission-controllers/)
to filter for specific fields in requests, and with
[dynamic resource allocation (DRA)](/docs/concepts/scheduling-eviction/dynamic-resource-allocation)
to select resources based on specific attributes.","The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network.","The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network.","The layer that provides capacity such as CPU, memory, network, and storage so that the containers can run and connect to a network.","Provides an identity for processes that run in a .

<!--more--> 

When processes inside Pods access the cluster, they are authenticated by the API server as a particular service account, for example, `default`. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same .","Provides an identity for processes that run in a .

<!--more--> 

When processes inside Pods access the cluster, they are authenticated by the API server as a particular service account, for example, `default`. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same .","Provides an identity for processes that run in a .

<!--more--> 

When processes inside Pods access the cluster, they are authenticated by the API server as a particular service account, for example, `default`. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same .","A finite or batch task that runs to completion.

<!--more--> 

Creates one or more  objects and ensures that a specified number of them successfully terminate. As Pods successfully complete, the Job tracks the successful completions.","A finite or batch task that runs to completion.

<!--more--> 

Creates one or more  objects and ensures that a specified number of them successfully terminate. As Pods successfully complete, the Job tracks the successful completions.","A finite or batch task that runs to completion.

<!--more--> 

Creates one or more  objects and ensures that a specified number of them successfully terminate. As Pods successfully complete, the Job tracks the successful completions.","Means of representing specific Kubernetes APIs uniquely.

<!--more-->

Group Version Resources (GVRs) specify the API group, API version, and _resource_ (name for the object kind as it appears in the URI) associated with accessing a particular id of object in Kubernetes.
GVRs let you define and distinguish different Kubernetes objects, and to specify a way of accessing
objects that is stable even as APIs change.

In this usage, _resource_ refers to an HTTP resource. Because some APIs are namespaced, a GVR may
not refer to a specific .","Means of representing specific Kubernetes APIs uniquely.

<!--more-->

Group Version Resources (GVRs) specify the API group, API version, and _resource_ (name for the object kind as it appears in the URI) associated with accessing a particular id of object in Kubernetes.
GVRs let you define and distinguish different Kubernetes objects, and to specify a way of accessing
objects that is stable even as APIs change.

In this usage, _resource_ refers to an HTTP resource. Because some APIs are namespaced, a GVR may
not refer to a specific .","Means of representing specific Kubernetes APIs uniquely.

<!--more-->

Group Version Resources (GVRs) specify the API group, API version, and _resource_ (name for the object kind as it appears in the URI) associated with accessing a particular id of object in Kubernetes.
GVRs let you define and distinguish different Kubernetes objects, and to specify a way of accessing
objects that is stable even as APIs change.

In this usage, _resource_ refers to an HTTP resource. Because some APIs are namespaced, a GVR may
not refer to a specific .","Means of representing specific Kubernetes APIs uniquely.

<!--more-->

Group Version Resources (GVRs) specify the API group, API version, and _resource_ (name for the object kind as it appears in the URI) associated with accessing a particular id of object in Kubernetes.
GVRs let you define and distinguish different Kubernetes objects, and to specify a way of accessing
objects that is stable even as APIs change.

In this usage, _resource_ refers to an HTTP resource. Because some APIs are namespaced, a GVR may
not refer to a specific .","Means of representing specific Kubernetes APIs uniquely.

<!--more-->

Group Version Resources (GVRs) specify the API group, API version, and _resource_ (name for the object kind as it appears in the URI) associated with accessing a particular id of object in Kubernetes.
GVRs let you define and distinguish different Kubernetes objects, and to specify a way of accessing
objects that is stable even as APIs change.

In this usage, _resource_ refers to an HTTP resource. Because some APIs are namespaced, a GVR may
not refer to a specific .","In computing, a proxy is a server that acts as an intermediary for a remote
service.

<!--more-->

A client interacts with the proxy; the proxy copies the client's data to the
actual server; the actual server replies to the proxy; the proxy sends the
actual server's reply to the client.

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes  concept.

You can run kube-proxy as a plain userland proxy service. If your operating
system supports it, you can instead run kube-proxy in a hybrid mode that
achieves the same overall effect using less system resources.","In computing, a proxy is a server that acts as an intermediary for a remote
service.

<!--more-->

A client interacts with the proxy; the proxy copies the client's data to the
actual server; the actual server replies to the proxy; the proxy sends the
actual server's reply to the client.

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes  concept.

You can run kube-proxy as a plain userland proxy service. If your operating
system supports it, you can instead run kube-proxy in a hybrid mode that
achieves the same overall effect using less system resources.","In computing, a proxy is a server that acts as an intermediary for a remote
service.

<!--more-->

A client interacts with the proxy; the proxy copies the client's data to the
actual server; the actual server replies to the proxy; the proxy sends the
actual server's reply to the client.

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes  concept.

You can run kube-proxy as a plain userland proxy service. If your operating
system supports it, you can instead run kube-proxy in a hybrid mode that
achieves the same overall effect using less system resources.","In computing, a proxy is a server that acts as an intermediary for a remote
service.

<!--more-->

A client interacts with the proxy; the proxy copies the client's data to the
actual server; the actual server replies to the proxy; the proxy sends the
actual server's reply to the client.

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes  concept.

You can run kube-proxy as a plain userland proxy service. If your operating
system supports it, you can instead run kube-proxy in a hybrid mode that
achieves the same overall effect using less system resources.","In computing, a proxy is a server that acts as an intermediary for a remote
service.

<!--more-->

A client interacts with the proxy; the proxy copies the client's data to the
actual server; the actual server replies to the proxy; the proxy sends the
actual server's reply to the client.

[kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes  concept.

You can run kube-proxy as a plain userland proxy service. If your operating
system supports it, you can instead run kube-proxy in a hybrid mode that
achieves the same overall effect using less system resources.","Control plane component that watches for newly created
 with no assigned
, and selects a node for them
to run on.

<!--more-->

Factors taken into account for scheduling decisions include:
individual and collective 
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.","Control plane component that watches for newly created
 with no assigned
, and selects a node for them
to run on.

<!--more-->

Factors taken into account for scheduling decisions include:
individual and collective 
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.","Control plane component that watches for newly created
 with no assigned
, and selects a node for them
to run on.

<!--more-->

Factors taken into account for scheduling decisions include:
individual and collective 
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.","Control plane component that watches for newly created
 with no assigned
, and selects a node for them
to run on.

<!--more-->

Factors taken into account for scheduling decisions include:
individual and collective 
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.","Control plane component that watches for newly created
 with no assigned
, and selects a node for them
to run on.

<!--more-->

Factors taken into account for scheduling decisions include:
individual and collective 
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.","An API object that manages external access to the services in a cluster, typically HTTP.

<!--more--> 

Ingress may provide load balancing, SSL termination and name-based virtual hosting.","An API object that manages external access to the services in a cluster, typically HTTP.

<!--more--> 

Ingress may provide load balancing, SSL termination and name-based virtual hosting.","An API object that manages external access to the services in a cluster, typically HTTP.

<!--more--> 

Ingress may provide load balancing, SSL termination and name-based virtual hosting.","An API object that manages external access to the services in a cluster, typically HTTP.

<!--more--> 

Ingress may provide load balancing, SSL termination and name-based virtual hosting.","An API object that manages external access to the services in a cluster, typically HTTP.

<!--more--> 

Ingress may provide load balancing, SSL termination and name-based virtual hosting.","A person who can review and approve Kubernetes code contributions.

<!--more--> 

While code review is focused on code quality and correctness, approval is focused on the holistic acceptance of a contribution. Holistic acceptance includes backwards/forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, and others. Approver status is scoped to a part of the codebase. Approvers were previously referred to as maintainers.","A person who can review and approve Kubernetes code contributions.

<!--more--> 

While code review is focused on code quality and correctness, approval is focused on the holistic acceptance of a contribution. Holistic acceptance includes backwards/forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, and others. Approver status is scoped to a part of the codebase. Approvers were previously referred to as maintainers.","A person who can review and approve Kubernetes code contributions.

<!--more--> 

While code review is focused on code quality and correctness, approval is focused on the holistic acceptance of a contribution. Holistic acceptance includes backwards/forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, and others. Approver status is scoped to a part of the codebase. Approvers were previously referred to as maintainers.","A person who can review and approve Kubernetes code contributions.

<!--more--> 

While code review is focused on code quality and correctness, approval is focused on the holistic acceptance of a contribution. Holistic acceptance includes backwards/forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, and others. Approver status is scoped to a part of the codebase. Approvers were previously referred to as maintainers.","Resources that extend the functionality of Kubernetes.

<!--more-->
[Installing addons](/docs/concepts/cluster-administration/addons/) explains more about using add-ons with your cluster, and lists some popular add-ons.","Resources that extend the functionality of Kubernetes.

<!--more-->
[Installing addons](/docs/concepts/cluster-administration/addons/) explains more about using add-ons with your cluster, and lists some popular add-ons.","Resources that extend the functionality of Kubernetes.

<!--more-->
[Installing addons](/docs/concepts/cluster-administration/addons/) explains more about using add-ons with your cluster, and lists some popular add-ons.","One or more

that are directly or indirectly attached to your
.

<!--more-->

Devices might be commercial products like GPUs, or custom hardware like
[ASIC boards](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit).
Attached devices usually require device drivers that let Kubernetes
 access the devices.","One or more

that are directly or indirectly attached to your
.

<!--more-->

Devices might be commercial products like GPUs, or custom hardware like
[ASIC boards](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit).
Attached devices usually require device drivers that let Kubernetes
 access the devices.","One or more

that are directly or indirectly attached to your
.

<!--more-->

Devices might be commercial products like GPUs, or custom hardware like
[ASIC boards](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit).
Attached devices usually require device drivers that let Kubernetes
 access the devices.","The lifecycle hooks expose events in the  management lifecycle and let the user run code when the events occur.

<!--more-->

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","The lifecycle hooks expose events in the  management lifecycle and let the user run code when the events occur.

<!--more-->

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","The lifecycle hooks expose events in the  management lifecycle and let the user run code when the events occur.

<!--more-->

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","The lifecycle hooks expose events in the  management lifecycle and let the user run code when the events occur.

<!--more-->

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","The lifecycle hooks expose events in the  management lifecycle and let the user run code when the events occur.

<!--more-->

Two hooks are exposed to Containers: PostStart which executes immediately after a container is created and PreStop which is blocking and is called immediately before a container is terminated.","A  object that a  uses
 to represent a 

<!--more--> 

When the kubelet finds a static pod in its configuration, it automatically tries to
create a Pod object on the Kubernetes API server for it. This means that the pod
will be visible on the API server, but cannot be controlled from there.

(For example, removing a mirror pod will not stop the kubelet daemon from running it).","A  object that a  uses
 to represent a 

<!--more--> 

When the kubelet finds a static pod in its configuration, it automatically tries to
create a Pod object on the Kubernetes API server for it. This means that the pod
will be visible on the API server, but cannot be controlled from there.

(For example, removing a mirror pod will not stop the kubelet daemon from running it).","A  object that a  uses
 to represent a 

<!--more--> 

When the kubelet finds a static pod in its configuration, it automatically tries to
create a Pod object on the Kubernetes API server for it. This means that the pod
will be visible on the API server, but cannot be controlled from there.

(For example, removing a mirror pod will not stop the kubelet daemon from running it).","Application containers (or app containers) are the  in a  that are started after any  have completed.

<!--more-->

An init container lets you separate initialization details that are important for the overall 
, and that don't need to keep running
once the application container has started.
If a pod doesn't have any init containers configured, all the containers in that pod are app containers.","Application containers (or app containers) are the  in a  that are started after any  have completed.

<!--more-->

An init container lets you separate initialization details that are important for the overall 
, and that don't need to keep running
once the application container has started.
If a pod doesn't have any init containers configured, all the containers in that pod are app containers.","Application containers (or app containers) are the  in a  that are started after any  have completed.

<!--more-->

An init container lets you separate initialization details that are important for the overall 
, and that don't need to keep running
once the application container has started.
If a pod doesn't have any init containers configured, all the containers in that pod are app containers.","A string value representing an amount of time.

<!--more-->

The format of a (Kubernetes) duration is based on the
[`time.Duration`](https://pkg.go.dev/time#Duration) type from the Go programming language.

In Kubernetes APIs that use durations, the value is expressed as series of a non-negative
integers combined with a time unit suffix. You can have more than one time quantity and
the duration is the sum of those time quantities.
The valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".

For example: `5s` represents a duration of five seconds, and `1m30s` represents a duration
of one minute and thirty seconds.","A string value representing an amount of time.

<!--more-->

The format of a (Kubernetes) duration is based on the
[`time.Duration`](https://pkg.go.dev/time#Duration) type from the Go programming language.

In Kubernetes APIs that use durations, the value is expressed as series of a non-negative
integers combined with a time unit suffix. You can have more than one time quantity and
the duration is the sum of those time quantities.
The valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".

For example: `5s` represents a duration of five seconds, and `1m30s` represents a duration
of one minute and thirty seconds.","A string value representing an amount of time.

<!--more-->

The format of a (Kubernetes) duration is based on the
[`time.Duration`](https://pkg.go.dev/time#Duration) type from the Go programming language.

In Kubernetes APIs that use durations, the value is expressed as series of a non-negative
integers combined with a time unit suffix. You can have more than one time quantity and
the duration is the sum of those time quantities.
The valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".

For example: `5s` represents a duration of five seconds, and `1m30s` represents a duration
of one minute and thirty seconds.","A string value representing an amount of time.

<!--more-->

The format of a (Kubernetes) duration is based on the
[`time.Duration`](https://pkg.go.dev/time#Duration) type from the Go programming language.

In Kubernetes APIs that use durations, the value is expressed as series of a non-negative
integers combined with a time unit suffix. You can have more than one time quantity and
the duration is the sum of those time quantities.
The valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".

For example: `5s` represents a duration of five seconds, and `1m30s` represents a duration
of one minute and thirty seconds.","A string value representing an amount of time.

<!--more-->

The format of a (Kubernetes) duration is based on the
[`time.Duration`](https://pkg.go.dev/time#Duration) type from the Go programming language.

In Kubernetes APIs that use durations, the value is expressed as series of a non-negative
integers combined with a time unit suffix. You can have more than one time quantity and
the duration is the sum of those time quantities.
The valid time units are ""ns"", ""µs"" (or ""us""), ""ms"", ""s"", ""m"", and ""h"".

For example: `5s` represents a duration of five seconds, and `1m30s` represents a duration
of one minute and thirty seconds.","A set of related paths in Kubernetes API.

<!--more-->

You can enable or disable each API group by changing the configuration of your API server. You can also disable or enable paths to specific
. An API group makes it easier to extend the Kubernetes API.
The API group is specified in a REST path and in the `apiVersion` field of a serialized .

* Read [API Group](/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) for more information.","A set of related paths in Kubernetes API.

<!--more-->

You can enable or disable each API group by changing the configuration of your API server. You can also disable or enable paths to specific
. An API group makes it easier to extend the Kubernetes API.
The API group is specified in a REST path and in the `apiVersion` field of a serialized .

* Read [API Group](/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) for more information.","A set of related paths in Kubernetes API.

<!--more-->

You can enable or disable each API group by changing the configuration of your API server. You can also disable or enable paths to specific
. An API group makes it easier to extend the Kubernetes API.
The API group is specified in a REST path and in the `apiVersion` field of a serialized .

* Read [API Group](/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) for more information.","Allows users to filter a list of 
 based on .

<!--more--> 

Selectors are applied when querying lists of resources to filter them by labels.","Allows users to filter a list of 
 based on .

<!--more--> 

Selectors are applied when querying lists of resources to filter them by labels.","Allows users to filter a list of 
 based on .

<!--more--> 

Selectors are applied when querying lists of resources to filter them by labels.","CIDR (Classless Inter-Domain Routing) is a notation for describing blocks of IP addresses and is used heavily in various networking configurations.

<!--more-->

In the context of Kubernetes, each  is assigned a range of IP addresses through the start address and a subnet mask using CIDR. This allows Nodes to assign each  a unique IP address. Although originally a concept for IPv4, CIDR has also been expanded to include IPv6.","CIDR (Classless Inter-Domain Routing) is a notation for describing blocks of IP addresses and is used heavily in various networking configurations.

<!--more-->

In the context of Kubernetes, each  is assigned a range of IP addresses through the start address and a subnet mask using CIDR. This allows Nodes to assign each  a unique IP address. Although originally a concept for IPv4, CIDR has also been expanded to include IPv6.","CIDR (Classless Inter-Domain Routing) is a notation for describing blocks of IP addresses and is used heavily in various networking configurations.

<!--more-->

In the context of Kubernetes, each  is assigned a range of IP addresses through the start address and a subnet mask using CIDR. This allows Nodes to assign each  a unique IP address. Although originally a concept for IPv4, CIDR has also been expanded to include IPv6.","CIDR (Classless Inter-Domain Routing) is a notation for describing blocks of IP addresses and is used heavily in various networking configurations.

<!--more-->

In the context of Kubernetes, each  is assigned a range of IP addresses through the start address and a subnet mask using CIDR. This allows Nodes to assign each  a unique IP address. Although originally a concept for IPv4, CIDR has also been expanded to include IPv6.","The main protocol for the communication between the  and Container Runtime.

<!--more-->

The Kubernetes Container Runtime Interface (CRI) defines the main
[gRPC](https://grpc.io) protocol for the communication between the
[node components](/docs/concepts/architecture/#node-components)
 and
.","The main protocol for the communication between the  and Container Runtime.

<!--more-->

The Kubernetes Container Runtime Interface (CRI) defines the main
[gRPC](https://grpc.io) protocol for the communication between the
[node components](/docs/concepts/architecture/#node-components)
 and
.","The main protocol for the communication between the  and Container Runtime.

<!--more-->

The Kubernetes Container Runtime Interface (CRI) defines the main
[gRPC](https://grpc.io) protocol for the communication between the
[node components](/docs/concepts/architecture/#node-components)
 and
.","The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.

<!--more--> 

CSI allows vendors to create custom storage plugins for Kubernetes without adding them to the Kubernetes repository (out-of-tree plugins). To use a CSI driver from a storage provider, you must first [deploy it to your cluster](https://kubernetes-csi.github.io/docs/deploying.html). You will then be able to create a  that uses that CSI driver.

* [CSI in the Kubernetes documentation](/docs/concepts/storage/volumes/#csi)
* [List of available CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html)","The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.

<!--more--> 

CSI allows vendors to create custom storage plugins for Kubernetes without adding them to the Kubernetes repository (out-of-tree plugins). To use a CSI driver from a storage provider, you must first [deploy it to your cluster](https://kubernetes-csi.github.io/docs/deploying.html). You will then be able to create a  that uses that CSI driver.

* [CSI in the Kubernetes documentation](/docs/concepts/storage/volumes/#csi)
* [List of available CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html)","The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.

<!--more--> 

CSI allows vendors to create custom storage plugins for Kubernetes without adding them to the Kubernetes repository (out-of-tree plugins). To use a CSI driver from a storage provider, you must first [deploy it to your cluster](https://kubernetes-csi.github.io/docs/deploying.html). You will then be able to create a  that uses that CSI driver.

* [CSI in the Kubernetes documentation](/docs/concepts/storage/volumes/#csi)
* [List of available CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html)","The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.

<!--more--> 

CSI allows vendors to create custom storage plugins for Kubernetes without adding them to the Kubernetes repository (out-of-tree plugins). To use a CSI driver from a storage provider, you must first [deploy it to your cluster](https://kubernetes-csi.github.io/docs/deploying.html). You will then be able to create a  that uses that CSI driver.

* [CSI in the Kubernetes documentation](/docs/concepts/storage/volumes/#csi)
* [List of available CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html)",The layer where various containerized applications run.,The layer where various containerized applications run.,The layer where various containerized applications run.,"A StorageClass provides a way for administrators to describe different available storage types.

<!--more--> 

StorageClasses can map to quality-of-service levels, backup policies, or to arbitrary policies determined by cluster administrators. Each StorageClass contains the fields `provisioner`, `parameters`, and `reclaimPolicy`, which are used when a  belonging to the class needs to be dynamically provisioned. Users can request a particular class using the name of a StorageClass object.","A StorageClass provides a way for administrators to describe different available storage types.

<!--more--> 

StorageClasses can map to quality-of-service levels, backup policies, or to arbitrary policies determined by cluster administrators. Each StorageClass contains the fields `provisioner`, `parameters`, and `reclaimPolicy`, which are used when a  belonging to the class needs to be dynamically provisioned. Users can request a particular class using the name of a StorageClass object.","A StorageClass provides a way for administrators to describe different available storage types.

<!--more--> 

StorageClasses can map to quality-of-service levels, backup policies, or to arbitrary policies determined by cluster administrators. Each StorageClass contains the fields `provisioner`, `parameters`, and `reclaimPolicy`, which are used when a  belonging to the class needs to be dynamically provisioned. Users can request a particular class using the name of a StorageClass object.","who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project.

<!--more--> 

Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.
SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project.

<!--more--> 

Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.
SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project.

<!--more--> 

Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.
SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project.

<!--more--> 

Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.
SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","who collectively manage an ongoing piece or aspect of the larger Kubernetes open source project.

<!--more--> 

Members within a SIG have a shared interest in advancing a specific area, such as architecture, API machinery, or documentation.
SIGs must follow the SIG [governance guidelines](https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance.md), but can have their own contribution policy and channels of communication.

For more information, see the [kubernetes/community](https://github.com/kubernetes/community) repo and the current list of [SIGs and Working Groups](https://github.com/kubernetes/community/blob/master/sig-list.md).","An API object that represents a piece of storage in the cluster. Representation of as a general, pluggable storage
 that can persist beyond the lifecycle of any
individual .

<!--more--> 

PersistentVolumes (PVs) provide an API that abstracts details of how storage is provided from how it is consumed.
PVs are used directly in scenarios where storage can be created ahead of time (static provisioning).
For scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead.","An API object that represents a piece of storage in the cluster. Representation of as a general, pluggable storage
 that can persist beyond the lifecycle of any
individual .

<!--more--> 

PersistentVolumes (PVs) provide an API that abstracts details of how storage is provided from how it is consumed.
PVs are used directly in scenarios where storage can be created ahead of time (static provisioning).
For scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead.","An API object that represents a piece of storage in the cluster. Representation of as a general, pluggable storage
 that can persist beyond the lifecycle of any
individual .

<!--more--> 

PersistentVolumes (PVs) provide an API that abstracts details of how storage is provided from how it is consumed.
PVs are used directly in scenarios where storage can be created ahead of time (static provisioning).
For scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead.","An API object that represents a piece of storage in the cluster. Representation of as a general, pluggable storage
 that can persist beyond the lifecycle of any
individual .

<!--more--> 

PersistentVolumes (PVs) provide an API that abstracts details of how storage is provided from how it is consumed.
PVs are used directly in scenarios where storage can be created ahead of time (static provisioning).
For scenarios that require on-demand storage (dynamic provisioning), PersistentVolumeClaims (PVCs) are used instead.","```bash
# All images running in a cluster
kubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'

# All images running in namespace: default, grouped by Pod
kubectl get pods --namespace default --output=custom-columns=""NAME:.metadata.name,IMAGE:.spec.containers[*].image""

 # All images excluding ""registry.k8s.io/coredns:1.6.2""
kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=""registry.k8s.io/coredns:1.6.2"")].image'

# All fields under metadata regardless of name
kubectl get pods -A -o=custom-columns='DATA:metadata.*'
```

More examples in the kubectl [reference documentation](/docs/reference/kubectl/#custom-columns).","```bash
# All images running in a cluster
kubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'

# All images running in namespace: default, grouped by Pod
kubectl get pods --namespace default --output=custom-columns=""NAME:.metadata.name,IMAGE:.spec.containers[*].image""

 # All images excluding ""registry.k8s.io/coredns:1.6.2""
kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=""registry.k8s.io/coredns:1.6.2"")].image'

# All fields under metadata regardless of name
kubectl get pods -A -o=custom-columns='DATA:metadata.*'
```

More examples in the kubectl [reference documentation](/docs/reference/kubectl/#custom-columns).","```bash
# All images running in a cluster
kubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'

# All images running in namespace: default, grouped by Pod
kubectl get pods --namespace default --output=custom-columns=""NAME:.metadata.name,IMAGE:.spec.containers[*].image""

 # All images excluding ""registry.k8s.io/coredns:1.6.2""
kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=""registry.k8s.io/coredns:1.6.2"")].image'

# All fields under metadata regardless of name
kubectl get pods -A -o=custom-columns='DATA:metadata.*'
```

More examples in the kubectl [reference documentation](/docs/reference/kubectl/#custom-columns).","```bash
# All images running in a cluster
kubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'

# All images running in namespace: default, grouped by Pod
kubectl get pods --namespace default --output=custom-columns=""NAME:.metadata.name,IMAGE:.spec.containers[*].image""

 # All images excluding ""registry.k8s.io/coredns:1.6.2""
kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=""registry.k8s.io/coredns:1.6.2"")].image'

# All fields under metadata regardless of name
kubectl get pods -A -o=custom-columns='DATA:metadata.*'
```

More examples in the kubectl [reference documentation](/docs/reference/kubectl/#custom-columns).","```shell
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{""\t""}{.status.startTime}{""\n""}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'
```

Or, with a ""my_pod"" and ""my_namespace"" (adjust these names to your environment):

```shell
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{@}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.metadata.name}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.status}'
```

{{< note >}}
On Windows, you must _double_ quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```
{{< /note >}}","```shell
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{""\t""}{.status.startTime}{""\n""}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'
```

Or, with a ""my_pod"" and ""my_namespace"" (adjust these names to your environment):

```shell
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{@}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.metadata.name}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.status}'
```

{{< note >}}
On Windows, you must _double_ quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```
{{< /note >}}","```shell
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{""\t""}{.status.startTime}{""\n""}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'
```

Or, with a ""my_pod"" and ""my_namespace"" (adjust these names to your environment):

```shell
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{@}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.metadata.name}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.status}'
```

{{< note >}}
On Windows, you must _double_ quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```
{{< /note >}}","```shell
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{""\t""}{.status.startTime}{""\n""}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'
```

Or, with a ""my_pod"" and ""my_namespace"" (adjust these names to your environment):

```shell
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{@}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.metadata.name}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.status}'
```

{{< note >}}
On Windows, you must _double_ quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```
{{< /note >}}","```shell
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
kubectl get pods -o=jsonpath=""{.items[*]['metadata.name', 'status.capacity']}""
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{""\t""}{.status.startTime}{""\n""}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'
```

Or, with a ""my_pod"" and ""my_namespace"" (adjust these names to your environment):

```shell
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{@}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.metadata.name}'
kubectl get pod/my_pod -n my_namespace -o=jsonpath='{.status}'
```

{{< note >}}
On Windows, you must _double_ quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:

```cmd
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{'\t'}{.status.startTime}{'\n'}{end}""
kubectl get pods -o=jsonpath=""{range .items[*]}{.metadata.name}{\""\t\""}{.status.startTime}{\""\n\""}{end}""
```
{{< /note >}}",,,,,,"kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/

```
kubectl [flags]
```","kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/

```
kubectl [flags]
```","kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/

```
kubectl [flags]
```",,,,,,,,,,,,,,"Update the user, group, or service account in a role binding or cluster role binding.

```
kubectl set subject (-f FILENAME | TYPE NAME) [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Update a cluster role binding for serviceaccount1
  kubectl set subject clusterrolebinding admin --serviceaccount=namespace:serviceaccount1
  
  # Update a role binding for user1, user2, and group1
  kubectl set subject rolebinding admin --user=user1 --user=user2 --group=group1
  
  # Print the result (in YAML format) of updating rolebinding subjects from a local, without hitting the server
  kubectl create rolebinding admin --role=admin --user=admin -o yaml --dry-run=client | kubectl set subject --local -f - --user=foo -o yaml
```","Update the user, group, or service account in a role binding or cluster role binding.

```
kubectl set subject (-f FILENAME | TYPE NAME) [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Update a cluster role binding for serviceaccount1
  kubectl set subject clusterrolebinding admin --serviceaccount=namespace:serviceaccount1
  
  # Update a role binding for user1, user2, and group1
  kubectl set subject rolebinding admin --user=user1 --user=user2 --group=group1
  
  # Print the result (in YAML format) of updating rolebinding subjects from a local, without hitting the server
  kubectl create rolebinding admin --role=admin --user=admin -o yaml --dry-run=client | kubectl set subject --local -f - --user=foo -o yaml
```","Update the user, group, or service account in a role binding or cluster role binding.

```
kubectl set subject (-f FILENAME | TYPE NAME) [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Update a cluster role binding for serviceaccount1
  kubectl set subject clusterrolebinding admin --serviceaccount=namespace:serviceaccount1
  
  # Update a role binding for user1, user2, and group1
  kubectl set subject rolebinding admin --user=user1 --user=user2 --group=group1
  
  # Print the result (in YAML format) of updating rolebinding subjects from a local, without hitting the server
  kubectl create rolebinding admin --role=admin --user=admin -o yaml --dry-run=client | kubectl set subject --local -f - --user=foo -o yaml
```","Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.

 A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.

```
kubectl set selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version]
```

```
  # Set the labels and selector before creating a deployment/service pair
  kubectl create service clusterip my-svc --clusterip=""None"" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -
  kubectl create deployment my-dep -o yaml --dry-run=client | kubectl label --local -f - environment=qa -o yaml | kubectl create -f -
```","Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.

 A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.

```
kubectl set selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version]
```

```
  # Set the labels and selector before creating a deployment/service pair
  kubectl create service clusterip my-svc --clusterip=""None"" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -
  kubectl create deployment my-dep -o yaml --dry-run=client | kubectl label --local -f - environment=qa -o yaml | kubectl create -f -
```","Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.

 A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.

```
kubectl set selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version]
```

```
  # Set the labels and selector before creating a deployment/service pair
  kubectl create service clusterip my-svc --clusterip=""None"" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -
  kubectl create deployment my-dep -o yaml --dry-run=client | kubectl label --local -f - environment=qa -o yaml | kubectl create -f -
```","Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.

 A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.

```
kubectl set selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version]
```

```
  # Set the labels and selector before creating a deployment/service pair
  kubectl create service clusterip my-svc --clusterip=""None"" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -
  kubectl create deployment my-dep -o yaml --dry-run=client | kubectl label --local -f - environment=qa -o yaml | kubectl create -f -
```","Set the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.

 A selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.

```
kubectl set selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version]
```

```
  # Set the labels and selector before creating a deployment/service pair
  kubectl create service clusterip my-svc --clusterip=""None"" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -
  kubectl create deployment my-dep -o yaml --dry-run=client | kubectl label --local -f - environment=qa -o yaml | kubectl create -f -
```","Update environment variables on a pod template.

 List environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.

 If ""--env -"" is passed, environment variables can be read from STDIN using the standard env syntax.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set env RESOURCE/NAME KEY_1=VAL_1 ... KEY_N=VAL_N
```

```
  # Update deployment 'registry' with a new environment variable
  kubectl set env deployment/registry STORAGE_DIR=/local
  
  # List the environment variables defined on a deployments 'sample-build'
  kubectl set env deployment/sample-build --list
  
  # List the environment variables defined on all pods
  kubectl set env pods --all --list
  
  # Output modified deployment in YAML, and does not alter the object on the server
  kubectl set env deployment/sample-build STORAGE_DIR=/data -o yaml
  
  # Update all containers in all replication controllers in the project to have ENV=prod
  kubectl set env rc --all ENV=prod
  
  # Import environment from a secret
  kubectl set env --from=secret/mysecret deployment/myapp
  
  # Import environment from a config map with a prefix
  kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
  
  # Import specific keys from a config map
  kubectl set env --keys=my-example-key --from=configmap/myconfigmap deployment/myapp
  
  # Remove the environment variable ENV from container 'c1' in all deployment configs
  kubectl set env deployments --all --containers=""c1"" ENV-
  
  # Remove the environment variable ENV from a deployment definition on disk and
  # update the deployment config on the server
  kubectl set env -f deploy.json ENV-
  
  # Set some of the local shell environment into a deployment config on the server
  env | grep RAILS_ | kubectl set env -e - deployment/registry
```","Update environment variables on a pod template.

 List environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.

 If ""--env -"" is passed, environment variables can be read from STDIN using the standard env syntax.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set env RESOURCE/NAME KEY_1=VAL_1 ... KEY_N=VAL_N
```

```
  # Update deployment 'registry' with a new environment variable
  kubectl set env deployment/registry STORAGE_DIR=/local
  
  # List the environment variables defined on a deployments 'sample-build'
  kubectl set env deployment/sample-build --list
  
  # List the environment variables defined on all pods
  kubectl set env pods --all --list
  
  # Output modified deployment in YAML, and does not alter the object on the server
  kubectl set env deployment/sample-build STORAGE_DIR=/data -o yaml
  
  # Update all containers in all replication controllers in the project to have ENV=prod
  kubectl set env rc --all ENV=prod
  
  # Import environment from a secret
  kubectl set env --from=secret/mysecret deployment/myapp
  
  # Import environment from a config map with a prefix
  kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
  
  # Import specific keys from a config map
  kubectl set env --keys=my-example-key --from=configmap/myconfigmap deployment/myapp
  
  # Remove the environment variable ENV from container 'c1' in all deployment configs
  kubectl set env deployments --all --containers=""c1"" ENV-
  
  # Remove the environment variable ENV from a deployment definition on disk and
  # update the deployment config on the server
  kubectl set env -f deploy.json ENV-
  
  # Set some of the local shell environment into a deployment config on the server
  env | grep RAILS_ | kubectl set env -e - deployment/registry
```","Update environment variables on a pod template.

 List environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.

 If ""--env -"" is passed, environment variables can be read from STDIN using the standard env syntax.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set env RESOURCE/NAME KEY_1=VAL_1 ... KEY_N=VAL_N
```

```
  # Update deployment 'registry' with a new environment variable
  kubectl set env deployment/registry STORAGE_DIR=/local
  
  # List the environment variables defined on a deployments 'sample-build'
  kubectl set env deployment/sample-build --list
  
  # List the environment variables defined on all pods
  kubectl set env pods --all --list
  
  # Output modified deployment in YAML, and does not alter the object on the server
  kubectl set env deployment/sample-build STORAGE_DIR=/data -o yaml
  
  # Update all containers in all replication controllers in the project to have ENV=prod
  kubectl set env rc --all ENV=prod
  
  # Import environment from a secret
  kubectl set env --from=secret/mysecret deployment/myapp
  
  # Import environment from a config map with a prefix
  kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
  
  # Import specific keys from a config map
  kubectl set env --keys=my-example-key --from=configmap/myconfigmap deployment/myapp
  
  # Remove the environment variable ENV from container 'c1' in all deployment configs
  kubectl set env deployments --all --containers=""c1"" ENV-
  
  # Remove the environment variable ENV from a deployment definition on disk and
  # update the deployment config on the server
  kubectl set env -f deploy.json ENV-
  
  # Set some of the local shell environment into a deployment config on the server
  env | grep RAILS_ | kubectl set env -e - deployment/registry
```","Update environment variables on a pod template.

 List environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.

 If ""--env -"" is passed, environment variables can be read from STDIN using the standard env syntax.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set env RESOURCE/NAME KEY_1=VAL_1 ... KEY_N=VAL_N
```

```
  # Update deployment 'registry' with a new environment variable
  kubectl set env deployment/registry STORAGE_DIR=/local
  
  # List the environment variables defined on a deployments 'sample-build'
  kubectl set env deployment/sample-build --list
  
  # List the environment variables defined on all pods
  kubectl set env pods --all --list
  
  # Output modified deployment in YAML, and does not alter the object on the server
  kubectl set env deployment/sample-build STORAGE_DIR=/data -o yaml
  
  # Update all containers in all replication controllers in the project to have ENV=prod
  kubectl set env rc --all ENV=prod
  
  # Import environment from a secret
  kubectl set env --from=secret/mysecret deployment/myapp
  
  # Import environment from a config map with a prefix
  kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
  
  # Import specific keys from a config map
  kubectl set env --keys=my-example-key --from=configmap/myconfigmap deployment/myapp
  
  # Remove the environment variable ENV from container 'c1' in all deployment configs
  kubectl set env deployments --all --containers=""c1"" ENV-
  
  # Remove the environment variable ENV from a deployment definition on disk and
  # update the deployment config on the server
  kubectl set env -f deploy.json ENV-
  
  # Set some of the local shell environment into a deployment config on the server
  env | grep RAILS_ | kubectl set env -e - deployment/registry
```","Update environment variables on a pod template.

 List environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.

 If ""--env -"" is passed, environment variables can be read from STDIN using the standard env syntax.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set env RESOURCE/NAME KEY_1=VAL_1 ... KEY_N=VAL_N
```

```
  # Update deployment 'registry' with a new environment variable
  kubectl set env deployment/registry STORAGE_DIR=/local
  
  # List the environment variables defined on a deployments 'sample-build'
  kubectl set env deployment/sample-build --list
  
  # List the environment variables defined on all pods
  kubectl set env pods --all --list
  
  # Output modified deployment in YAML, and does not alter the object on the server
  kubectl set env deployment/sample-build STORAGE_DIR=/data -o yaml
  
  # Update all containers in all replication controllers in the project to have ENV=prod
  kubectl set env rc --all ENV=prod
  
  # Import environment from a secret
  kubectl set env --from=secret/mysecret deployment/myapp
  
  # Import environment from a config map with a prefix
  kubectl set env --from=configmap/myconfigmap --prefix=MYSQL_ deployment/myapp
  
  # Import specific keys from a config map
  kubectl set env --keys=my-example-key --from=configmap/myconfigmap deployment/myapp
  
  # Remove the environment variable ENV from container 'c1' in all deployment configs
  kubectl set env deployments --all --containers=""c1"" ENV-
  
  # Remove the environment variable ENV from a deployment definition on disk and
  # update the deployment config on the server
  kubectl set env -f deploy.json ENV-
  
  # Set some of the local shell environment into a deployment config on the server
  env | grep RAILS_ | kubectl set env -e - deployment/registry
```","Update the service account of pod template resources.

 Possible resources (case insensitive) can be:

 replicationcontroller (rc), deployment (deploy), daemonset (ds), job, replicaset (rs), statefulset

```
kubectl set serviceaccount (-f FILENAME | TYPE NAME) SERVICE_ACCOUNT
```

```
  # Set deployment nginx-deployment's service account to serviceaccount1
  kubectl set serviceaccount deployment nginx-deployment serviceaccount1
  
  # Print the result (in YAML format) of updated nginx deployment with the service account from local file, without hitting the API server
  kubectl set sa -f nginx-deployment.yaml serviceaccount1 --local --dry-run=client -o yaml
```","Update the service account of pod template resources.

 Possible resources (case insensitive) can be:

 replicationcontroller (rc), deployment (deploy), daemonset (ds), job, replicaset (rs), statefulset

```
kubectl set serviceaccount (-f FILENAME | TYPE NAME) SERVICE_ACCOUNT
```

```
  # Set deployment nginx-deployment's service account to serviceaccount1
  kubectl set serviceaccount deployment nginx-deployment serviceaccount1
  
  # Print the result (in YAML format) of updated nginx deployment with the service account from local file, without hitting the API server
  kubectl set sa -f nginx-deployment.yaml serviceaccount1 --local --dry-run=client -o yaml
```","Update the service account of pod template resources.

 Possible resources (case insensitive) can be:

 replicationcontroller (rc), deployment (deploy), daemonset (ds), job, replicaset (rs), statefulset

```
kubectl set serviceaccount (-f FILENAME | TYPE NAME) SERVICE_ACCOUNT
```

```
  # Set deployment nginx-deployment's service account to serviceaccount1
  kubectl set serviceaccount deployment nginx-deployment serviceaccount1
  
  # Print the result (in YAML format) of updated nginx deployment with the service account from local file, without hitting the API server
  kubectl set sa -f nginx-deployment.yaml serviceaccount1 --local --dry-run=client -o yaml
```","Specify compute resource requirements (CPU, memory) for any resource that defines a pod template.  If a pod is successfully scheduled, it is guaranteed the amount of resource requested, but may burst up to its specified limits.

 For each compute resource, if a limit is specified and a request is omitted, the request will default to the limit.

 Possible resources include (case insensitive): Use ""kubectl api-resources"" for a complete list of supported resources..

```
kubectl set resources (-f FILENAME | TYPE NAME)  ([--limits=LIMITS & --requests=REQUESTS]
```

```
  # Set a deployments nginx container cpu limits to ""200m"" and memory to ""512Mi""
  kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi
  
  # Set the resource request and limits for all containers in nginx
  kubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
  
  # Remove the resource requests for resources on containers in nginx
  kubectl set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0
  
  # Print the result (in yaml format) of updating nginx container limits from a local, without hitting the server
  kubectl set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml
```","Specify compute resource requirements (CPU, memory) for any resource that defines a pod template.  If a pod is successfully scheduled, it is guaranteed the amount of resource requested, but may burst up to its specified limits.

 For each compute resource, if a limit is specified and a request is omitted, the request will default to the limit.

 Possible resources include (case insensitive): Use ""kubectl api-resources"" for a complete list of supported resources..

```
kubectl set resources (-f FILENAME | TYPE NAME)  ([--limits=LIMITS & --requests=REQUESTS]
```

```
  # Set a deployments nginx container cpu limits to ""200m"" and memory to ""512Mi""
  kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi
  
  # Set the resource request and limits for all containers in nginx
  kubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
  
  # Remove the resource requests for resources on containers in nginx
  kubectl set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0
  
  # Print the result (in yaml format) of updating nginx container limits from a local, without hitting the server
  kubectl set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml
```","Specify compute resource requirements (CPU, memory) for any resource that defines a pod template.  If a pod is successfully scheduled, it is guaranteed the amount of resource requested, but may burst up to its specified limits.

 For each compute resource, if a limit is specified and a request is omitted, the request will default to the limit.

 Possible resources include (case insensitive): Use ""kubectl api-resources"" for a complete list of supported resources..

```
kubectl set resources (-f FILENAME | TYPE NAME)  ([--limits=LIMITS & --requests=REQUESTS]
```

```
  # Set a deployments nginx container cpu limits to ""200m"" and memory to ""512Mi""
  kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi
  
  # Set the resource request and limits for all containers in nginx
  kubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
  
  # Remove the resource requests for resources on containers in nginx
  kubectl set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0
  
  # Print the result (in yaml format) of updating nginx container limits from a local, without hitting the server
  kubectl set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml
```","Specify compute resource requirements (CPU, memory) for any resource that defines a pod template.  If a pod is successfully scheduled, it is guaranteed the amount of resource requested, but may burst up to its specified limits.

 For each compute resource, if a limit is specified and a request is omitted, the request will default to the limit.

 Possible resources include (case insensitive): Use ""kubectl api-resources"" for a complete list of supported resources..

```
kubectl set resources (-f FILENAME | TYPE NAME)  ([--limits=LIMITS & --requests=REQUESTS]
```

```
  # Set a deployments nginx container cpu limits to ""200m"" and memory to ""512Mi""
  kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi
  
  # Set the resource request and limits for all containers in nginx
  kubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi
  
  # Remove the resource requests for resources on containers in nginx
  kubectl set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0
  
  # Print the result (in yaml format) of updating nginx container limits from a local, without hitting the server
  kubectl set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml
```","Update existing container image(s) of resources.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
```

```
  # Set a deployment's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'
  kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
  
  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  kubectl set image deployments,rc nginx=nginx:1.9.1 --all
  
  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  kubectl set image daemonset abc *=nginx:1.9.1
  
  # Print result (in yaml format) of updating nginx container image from local file, without hitting the server
  kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","Update existing container image(s) of resources.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
```

```
  # Set a deployment's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'
  kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
  
  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  kubectl set image deployments,rc nginx=nginx:1.9.1 --all
  
  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  kubectl set image daemonset abc *=nginx:1.9.1
  
  # Print result (in yaml format) of updating nginx container image from local file, without hitting the server
  kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","Update existing container image(s) of resources.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
```

```
  # Set a deployment's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'
  kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
  
  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  kubectl set image deployments,rc nginx=nginx:1.9.1 --all
  
  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  kubectl set image daemonset abc *=nginx:1.9.1
  
  # Print result (in yaml format) of updating nginx container image from local file, without hitting the server
  kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","Update existing container image(s) of resources.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
```

```
  # Set a deployment's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'
  kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
  
  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  kubectl set image deployments,rc nginx=nginx:1.9.1 --all
  
  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  kubectl set image daemonset abc *=nginx:1.9.1
  
  # Print result (in yaml format) of updating nginx container image from local file, without hitting the server
  kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","Update existing container image(s) of resources.

 Possible resources include (case insensitive):

        pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), statefulset (sts), cronjob (cj), replicaset (rs)

```
kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
```

```
  # Set a deployment's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'
  kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1
  
  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  kubectl set image deployments,rc nginx=nginx:1.9.1 --all
  
  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  kubectl set image daemonset abc *=nginx:1.9.1
  
  # Print result (in yaml format) of updating nginx container image from local file, without hitting the server
  kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml
```","Configure application resources.

 These commands help you make changes to existing application resources.

```
kubectl set SUBCOMMAND
```","Configure application resources.

 These commands help you make changes to existing application resources.

```
kubectl set SUBCOMMAND
```","Configure application resources.

 These commands help you make changes to existing application resources.

```
kubectl set SUBCOMMAND
```","Create and run a particular image in a pod.

```
kubectl run NAME --image=image [--env=""key=value""] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

```
  # Start a nginx pod
  kubectl run nginx --image=nginx
  
  # Start a hazelcast pod and let the container expose port 5701
  kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
  
  # Start a hazelcast pod and set environment variables ""DNS_DOMAIN=cluster"" and ""POD_NAMESPACE=default"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=""DNS_DOMAIN=cluster"" --env=""POD_NAMESPACE=default""
  
  # Start a hazelcast pod and set labels ""app=hazelcast"" and ""env=prod"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --labels=""app=hazelcast,env=prod""
  
  # Dry run; print the corresponding API objects without creating them
  kubectl run nginx --image=nginx --dry-run=client
  
  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
  
  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  kubectl run -i -t busybox --image=busybox --restart=Never
  
  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>
  
  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
```","Create and run a particular image in a pod.

```
kubectl run NAME --image=image [--env=""key=value""] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

```
  # Start a nginx pod
  kubectl run nginx --image=nginx
  
  # Start a hazelcast pod and let the container expose port 5701
  kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
  
  # Start a hazelcast pod and set environment variables ""DNS_DOMAIN=cluster"" and ""POD_NAMESPACE=default"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=""DNS_DOMAIN=cluster"" --env=""POD_NAMESPACE=default""
  
  # Start a hazelcast pod and set labels ""app=hazelcast"" and ""env=prod"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --labels=""app=hazelcast,env=prod""
  
  # Dry run; print the corresponding API objects without creating them
  kubectl run nginx --image=nginx --dry-run=client
  
  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
  
  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  kubectl run -i -t busybox --image=busybox --restart=Never
  
  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>
  
  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
```","Create and run a particular image in a pod.

```
kubectl run NAME --image=image [--env=""key=value""] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

```
  # Start a nginx pod
  kubectl run nginx --image=nginx
  
  # Start a hazelcast pod and let the container expose port 5701
  kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
  
  # Start a hazelcast pod and set environment variables ""DNS_DOMAIN=cluster"" and ""POD_NAMESPACE=default"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=""DNS_DOMAIN=cluster"" --env=""POD_NAMESPACE=default""
  
  # Start a hazelcast pod and set labels ""app=hazelcast"" and ""env=prod"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --labels=""app=hazelcast,env=prod""
  
  # Dry run; print the corresponding API objects without creating them
  kubectl run nginx --image=nginx --dry-run=client
  
  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
  
  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  kubectl run -i -t busybox --image=busybox --restart=Never
  
  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>
  
  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
```","Create and run a particular image in a pod.

```
kubectl run NAME --image=image [--env=""key=value""] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

```
  # Start a nginx pod
  kubectl run nginx --image=nginx
  
  # Start a hazelcast pod and let the container expose port 5701
  kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
  
  # Start a hazelcast pod and set environment variables ""DNS_DOMAIN=cluster"" and ""POD_NAMESPACE=default"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=""DNS_DOMAIN=cluster"" --env=""POD_NAMESPACE=default""
  
  # Start a hazelcast pod and set labels ""app=hazelcast"" and ""env=prod"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --labels=""app=hazelcast,env=prod""
  
  # Dry run; print the corresponding API objects without creating them
  kubectl run nginx --image=nginx --dry-run=client
  
  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
  
  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  kubectl run -i -t busybox --image=busybox --restart=Never
  
  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>
  
  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
```","Create and run a particular image in a pod.

```
kubectl run NAME --image=image [--env=""key=value""] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

```
  # Start a nginx pod
  kubectl run nginx --image=nginx
  
  # Start a hazelcast pod and let the container expose port 5701
  kubectl run hazelcast --image=hazelcast/hazelcast --port=5701
  
  # Start a hazelcast pod and set environment variables ""DNS_DOMAIN=cluster"" and ""POD_NAMESPACE=default"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=""DNS_DOMAIN=cluster"" --env=""POD_NAMESPACE=default""
  
  # Start a hazelcast pod and set labels ""app=hazelcast"" and ""env=prod"" in the container
  kubectl run hazelcast --image=hazelcast/hazelcast --labels=""app=hazelcast,env=prod""
  
  # Dry run; print the corresponding API objects without creating them
  kubectl run nginx --image=nginx --dry-run=client
  
  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  kubectl run nginx --image=nginx --overrides='{ ""apiVersion"": ""v1"", ""spec"": { ... } }'
  
  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  kubectl run -i -t busybox --image=busybox --restart=Never
  
  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>
  
  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
```","Print the supported API resources on the server.

```
kubectl api-resources [flags]
```

```
  # Print the supported API resources
  kubectl api-resources
  
  # Print the supported API resources with more information
  kubectl api-resources -o wide
  
  # Print the supported API resources sorted by a column
  kubectl api-resources --sort-by=name
  
  # Print the supported namespaced resources
  kubectl api-resources --namespaced=true
  
  # Print the supported non-namespaced resources
  kubectl api-resources --namespaced=false
  
  # Print the supported API resources with a specific APIGroup
  kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Print the supported API resources on the server.

```
kubectl api-resources [flags]
```

```
  # Print the supported API resources
  kubectl api-resources
  
  # Print the supported API resources with more information
  kubectl api-resources -o wide
  
  # Print the supported API resources sorted by a column
  kubectl api-resources --sort-by=name
  
  # Print the supported namespaced resources
  kubectl api-resources --namespaced=true
  
  # Print the supported non-namespaced resources
  kubectl api-resources --namespaced=false
  
  # Print the supported API resources with a specific APIGroup
  kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Print the supported API resources on the server.

```
kubectl api-resources [flags]
```

```
  # Print the supported API resources
  kubectl api-resources
  
  # Print the supported API resources with more information
  kubectl api-resources -o wide
  
  # Print the supported API resources sorted by a column
  kubectl api-resources --sort-by=name
  
  # Print the supported namespaced resources
  kubectl api-resources --namespaced=true
  
  # Print the supported non-namespaced resources
  kubectl api-resources --namespaced=false
  
  # Print the supported API resources with a specific APIGroup
  kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Print the supported API resources on the server.

```
kubectl api-resources [flags]
```

```
  # Print the supported API resources
  kubectl api-resources
  
  # Print the supported API resources with more information
  kubectl api-resources -o wide
  
  # Print the supported API resources sorted by a column
  kubectl api-resources --sort-by=name
  
  # Print the supported namespaced resources
  kubectl api-resources --namespaced=true
  
  # Print the supported non-namespaced resources
  kubectl api-resources --namespaced=false
  
  # Print the supported API resources with a specific APIGroup
  kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Print the supported API resources on the server.

```
kubectl api-resources [flags]
```

```
  # Print the supported API resources
  kubectl api-resources
  
  # Print the supported API resources with more information
  kubectl api-resources -o wide
  
  # Print the supported API resources sorted by a column
  kubectl api-resources --sort-by=name
  
  # Print the supported namespaced resources
  kubectl api-resources --namespaced=true
  
  # Print the supported non-namespaced resources
  kubectl api-resources --namespaced=false
  
  # Print the supported API resources with a specific APIGroup
  kubectl api-resources --api-group=rbac.authorization.k8s.io
```","Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'
  
  # Update a pod identified by type and name in ""pod.json""
  kubectl annotate -f pod.json description='my frontend'
  
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'
  
  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1
  
  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-
```","Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'
  
  # Update a pod identified by type and name in ""pod.json""
  kubectl annotate -f pod.json description='my frontend'
  
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'
  
  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1
  
  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-
```","Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'
  
  # Update a pod identified by type and name in ""pod.json""
  kubectl annotate -f pod.json description='my frontend'
  
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'
  
  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1
  
  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-
```","Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'
  
  # Update a pod identified by type and name in ""pod.json""
  kubectl annotate -f pod.json description='my frontend'
  
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'
  
  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1
  
  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-
```","Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'
  
  # Update a pod identified by type and name in ""pod.json""
  kubectl annotate -f pod.json description='my frontend'
  
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'
  
  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1
  
  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-
```","Dump cluster information out suitable for debugging and diagnosing cluster problems.  By default, dumps everything to stdout. You can optionally specify a directory with --output-directory.  If you specify a directory, Kubernetes will build a set of files in that directory.  By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.

 The command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.

```
kubectl cluster-info dump [flags]
```

```
  # Dump current cluster state to stdout
  kubectl cluster-info dump
  
  # Dump current cluster state to /path/to/cluster-state
  kubectl cluster-info dump --output-directory=/path/to/cluster-state
  
  # Dump all namespaces to stdout
  kubectl cluster-info dump --all-namespaces
  
  # Dump a set of namespaces to /path/to/cluster-state
  kubectl cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state
```","Dump cluster information out suitable for debugging and diagnosing cluster problems.  By default, dumps everything to stdout. You can optionally specify a directory with --output-directory.  If you specify a directory, Kubernetes will build a set of files in that directory.  By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.

 The command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.

```
kubectl cluster-info dump [flags]
```

```
  # Dump current cluster state to stdout
  kubectl cluster-info dump
  
  # Dump current cluster state to /path/to/cluster-state
  kubectl cluster-info dump --output-directory=/path/to/cluster-state
  
  # Dump all namespaces to stdout
  kubectl cluster-info dump --all-namespaces
  
  # Dump a set of namespaces to /path/to/cluster-state
  kubectl cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state
```","Dump cluster information out suitable for debugging and diagnosing cluster problems.  By default, dumps everything to stdout. You can optionally specify a directory with --output-directory.  If you specify a directory, Kubernetes will build a set of files in that directory.  By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.

 The command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.

```
kubectl cluster-info dump [flags]
```

```
  # Dump current cluster state to stdout
  kubectl cluster-info dump
  
  # Dump current cluster state to /path/to/cluster-state
  kubectl cluster-info dump --output-directory=/path/to/cluster-state
  
  # Dump all namespaces to stdout
  kubectl cluster-info dump --all-namespaces
  
  # Dump a set of namespaces to /path/to/cluster-state
  kubectl cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state
```","Dump cluster information out suitable for debugging and diagnosing cluster problems.  By default, dumps everything to stdout. You can optionally specify a directory with --output-directory.  If you specify a directory, Kubernetes will build a set of files in that directory.  By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.

 The command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.

```
kubectl cluster-info dump [flags]
```

```
  # Dump current cluster state to stdout
  kubectl cluster-info dump
  
  # Dump current cluster state to /path/to/cluster-state
  kubectl cluster-info dump --output-directory=/path/to/cluster-state
  
  # Dump all namespaces to stdout
  kubectl cluster-info dump --all-namespaces
  
  # Dump a set of namespaces to /path/to/cluster-state
  kubectl cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state
```","Dump cluster information out suitable for debugging and diagnosing cluster problems.  By default, dumps everything to stdout. You can optionally specify a directory with --output-directory.  If you specify a directory, Kubernetes will build a set of files in that directory.  By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.

 The command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.

```
kubectl cluster-info dump [flags]
```

```
  # Dump current cluster state to stdout
  kubectl cluster-info dump
  
  # Dump current cluster state to /path/to/cluster-state
  kubectl cluster-info dump --output-directory=/path/to/cluster-state
  
  # Dump all namespaces to stdout
  kubectl cluster-info dump --all-namespaces
  
  # Dump a set of namespaces to /path/to/cluster-state
  kubectl cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state
```","Display addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl cluster-info [flags]
```

```
  # Print the address of the control plane and cluster services
  kubectl cluster-info
```","Display addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl cluster-info [flags]
```

```
  # Print the address of the control plane and cluster services
  kubectl cluster-info
```","Display addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl cluster-info [flags]
```

```
  # Print the address of the control plane and cluster services
  kubectl cluster-info
```","Display addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl cluster-info [flags]
```

```
  # Print the address of the control plane and cluster services
  kubectl cluster-info
```","Display addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

```
kubectl cluster-info [flags]
```

```
  # Print the address of the control plane and cluster services
  kubectl cluster-info
```","Describe fields and structure of various resources.

 This command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:

        &lt;type&gt;.&lt;fieldName&gt;[.&lt;fieldName&gt;]
        
 Information about each field is retrieved from the server in OpenAPI format.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl explain TYPE [--recursive=FALSE|TRUE] [--api-version=api-version-group] [-o|--output=plaintext|plaintext-openapiv2]
```

```
  # Get the documentation of the resource and its fields
  kubectl explain pods
  
  # Get all the fields in the resource
  kubectl explain pods --recursive
  
  # Get the explanation for deployment in supported api versions
  kubectl explain deployments --api-version=apps/v1
  
  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers
  
  # Get the documentation of resources in different format
  kubectl explain deployment --output=plaintext-openapiv2
```","Describe fields and structure of various resources.

 This command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:

        &lt;type&gt;.&lt;fieldName&gt;[.&lt;fieldName&gt;]
        
 Information about each field is retrieved from the server in OpenAPI format.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl explain TYPE [--recursive=FALSE|TRUE] [--api-version=api-version-group] [-o|--output=plaintext|plaintext-openapiv2]
```

```
  # Get the documentation of the resource and its fields
  kubectl explain pods
  
  # Get all the fields in the resource
  kubectl explain pods --recursive
  
  # Get the explanation for deployment in supported api versions
  kubectl explain deployments --api-version=apps/v1
  
  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers
  
  # Get the documentation of resources in different format
  kubectl explain deployment --output=plaintext-openapiv2
```","Describe fields and structure of various resources.

 This command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:

        &lt;type&gt;.&lt;fieldName&gt;[.&lt;fieldName&gt;]
        
 Information about each field is retrieved from the server in OpenAPI format.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl explain TYPE [--recursive=FALSE|TRUE] [--api-version=api-version-group] [-o|--output=plaintext|plaintext-openapiv2]
```

```
  # Get the documentation of the resource and its fields
  kubectl explain pods
  
  # Get all the fields in the resource
  kubectl explain pods --recursive
  
  # Get the explanation for deployment in supported api versions
  kubectl explain deployments --api-version=apps/v1
  
  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers
  
  # Get the documentation of resources in different format
  kubectl explain deployment --output=plaintext-openapiv2
```","Describe fields and structure of various resources.

 This command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:

        &lt;type&gt;.&lt;fieldName&gt;[.&lt;fieldName&gt;]
        
 Information about each field is retrieved from the server in OpenAPI format.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl explain TYPE [--recursive=FALSE|TRUE] [--api-version=api-version-group] [-o|--output=plaintext|plaintext-openapiv2]
```

```
  # Get the documentation of the resource and its fields
  kubectl explain pods
  
  # Get all the fields in the resource
  kubectl explain pods --recursive
  
  # Get the explanation for deployment in supported api versions
  kubectl explain deployments --api-version=apps/v1
  
  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers
  
  # Get the documentation of resources in different format
  kubectl explain deployment --output=plaintext-openapiv2
```","Describe fields and structure of various resources.

 This command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:

        &lt;type&gt;.&lt;fieldName&gt;[.&lt;fieldName&gt;]
        
 Information about each field is retrieved from the server in OpenAPI format.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl explain TYPE [--recursive=FALSE|TRUE] [--api-version=api-version-group] [-o|--output=plaintext|plaintext-openapiv2]
```

```
  # Get the documentation of the resource and its fields
  kubectl explain pods
  
  # Get all the fields in the resource
  kubectl explain pods --recursive
  
  # Get the explanation for deployment in supported api versions
  kubectl explain deployments --api-version=apps/v1
  
  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers
  
  # Get the documentation of resources in different format
  kubectl explain deployment --output=plaintext-openapiv2
```","Show details of a specific resource or group of resources.

 Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:

        $ kubectl describe TYPE NAME_PREFIX
        
 will first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)
```

```
  # Describe a node
  kubectl describe nodes kubernetes-node-emt8.c.myproject.internal
  
  # Describe a pod
  kubectl describe pods/nginx
  
  # Describe a pod identified by type and name in ""pod.json""
  kubectl describe -f pod.json
  
  # Describe all pods
  kubectl describe pods
  
  # Describe pods by label name=myLabel
  kubectl describe pods -l name=myLabel
  
  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  kubectl describe pods frontend
```","Show details of a specific resource or group of resources.

 Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:

        $ kubectl describe TYPE NAME_PREFIX
        
 will first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)
```

```
  # Describe a node
  kubectl describe nodes kubernetes-node-emt8.c.myproject.internal
  
  # Describe a pod
  kubectl describe pods/nginx
  
  # Describe a pod identified by type and name in ""pod.json""
  kubectl describe -f pod.json
  
  # Describe all pods
  kubectl describe pods
  
  # Describe pods by label name=myLabel
  kubectl describe pods -l name=myLabel
  
  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  kubectl describe pods frontend
```","Show details of a specific resource or group of resources.

 Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:

        $ kubectl describe TYPE NAME_PREFIX
        
 will first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)
```

```
  # Describe a node
  kubectl describe nodes kubernetes-node-emt8.c.myproject.internal
  
  # Describe a pod
  kubectl describe pods/nginx
  
  # Describe a pod identified by type and name in ""pod.json""
  kubectl describe -f pod.json
  
  # Describe all pods
  kubectl describe pods
  
  # Describe pods by label name=myLabel
  kubectl describe pods -l name=myLabel
  
  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  kubectl describe pods frontend
```","Show details of a specific resource or group of resources.

 Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:

        $ kubectl describe TYPE NAME_PREFIX
        
 will first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)
```

```
  # Describe a node
  kubectl describe nodes kubernetes-node-emt8.c.myproject.internal
  
  # Describe a pod
  kubectl describe pods/nginx
  
  # Describe a pod identified by type and name in ""pod.json""
  kubectl describe -f pod.json
  
  # Describe all pods
  kubectl describe pods
  
  # Describe pods by label name=myLabel
  kubectl describe pods -l name=myLabel
  
  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  kubectl describe pods frontend
```","Show details of a specific resource or group of resources.

 Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:

        $ kubectl describe TYPE NAME_PREFIX
        
 will first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME)
```

```
  # Describe a node
  kubectl describe nodes kubernetes-node-emt8.c.myproject.internal
  
  # Describe a pod
  kubectl describe pods/nginx
  
  # Describe a pod identified by type and name in ""pod.json""
  kubectl describe -f pod.json
  
  # Describe all pods
  kubectl describe pods
  
  # Describe pods by label name=myLabel
  kubectl describe pods -l name=myLabel
  
  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  kubectl describe pods frontend
```","Edit a resource from the default editor.

 The edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. When attempting to open the editor, it will first attempt to use the shell that has been defined in the 'SHELL' environment variable. If this is not defined, the default shell will be used, which is '/bin/bash' for Linux or 'cmd' for Windows.

 You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 Editing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl edit (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the service named 'registry'
  kubectl edit svc/registry
  
  # Use an alternative editor
  KUBE_EDITOR=""nano"" kubectl edit svc/registry
  
  # Edit the job 'myjob' in JSON using the v1 API format
  kubectl edit job.v1.batch/myjob -o json
  
  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  kubectl edit deployment/mydeployment -o yaml --save-config
  
  # Edit the 'status' subresource for the 'mydeployment' deployment
  kubectl edit deployment mydeployment --subresource='status'
```","Edit a resource from the default editor.

 The edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. When attempting to open the editor, it will first attempt to use the shell that has been defined in the 'SHELL' environment variable. If this is not defined, the default shell will be used, which is '/bin/bash' for Linux or 'cmd' for Windows.

 You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 Editing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl edit (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the service named 'registry'
  kubectl edit svc/registry
  
  # Use an alternative editor
  KUBE_EDITOR=""nano"" kubectl edit svc/registry
  
  # Edit the job 'myjob' in JSON using the v1 API format
  kubectl edit job.v1.batch/myjob -o json
  
  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  kubectl edit deployment/mydeployment -o yaml --save-config
  
  # Edit the 'status' subresource for the 'mydeployment' deployment
  kubectl edit deployment mydeployment --subresource='status'
```","Edit a resource from the default editor.

 The edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. When attempting to open the editor, it will first attempt to use the shell that has been defined in the 'SHELL' environment variable. If this is not defined, the default shell will be used, which is '/bin/bash' for Linux or 'cmd' for Windows.

 You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 Editing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl edit (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the service named 'registry'
  kubectl edit svc/registry
  
  # Use an alternative editor
  KUBE_EDITOR=""nano"" kubectl edit svc/registry
  
  # Edit the job 'myjob' in JSON using the v1 API format
  kubectl edit job.v1.batch/myjob -o json
  
  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  kubectl edit deployment/mydeployment -o yaml --save-config
  
  # Edit the 'status' subresource for the 'mydeployment' deployment
  kubectl edit deployment mydeployment --subresource='status'
```","Edit a resource from the default editor.

 The edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. When attempting to open the editor, it will first attempt to use the shell that has been defined in the 'SHELL' environment variable. If this is not defined, the default shell will be used, which is '/bin/bash' for Linux or 'cmd' for Windows.

 You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 Editing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl edit (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the service named 'registry'
  kubectl edit svc/registry
  
  # Use an alternative editor
  KUBE_EDITOR=""nano"" kubectl edit svc/registry
  
  # Edit the job 'myjob' in JSON using the v1 API format
  kubectl edit job.v1.batch/myjob -o json
  
  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  kubectl edit deployment/mydeployment -o yaml --save-config
  
  # Edit the 'status' subresource for the 'mydeployment' deployment
  kubectl edit deployment mydeployment --subresource='status'
```","Edit a resource from the default editor.

 The edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. When attempting to open the editor, it will first attempt to use the shell that has been defined in the 'SHELL' environment variable. If this is not defined, the default shell will be used, which is '/bin/bash' for Linux or 'cmd' for Windows.

 You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 Editing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl edit (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the service named 'registry'
  kubectl edit svc/registry
  
  # Use an alternative editor
  KUBE_EDITOR=""nano"" kubectl edit svc/registry
  
  # Edit the job 'myjob' in JSON using the v1 API format
  kubectl edit job.v1.batch/myjob -o json
  
  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  kubectl edit deployment/mydeployment -o yaml --save-config
  
  # Edit the 'status' subresource for the 'mydeployment' deployment
  kubectl edit deployment mydeployment --subresource='status'
```","Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

```
kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]
```

```
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx
  
  # Return snapshot logs from pod nginx, prefixing each line with the source pod and container name
  kubectl logs nginx --prefix
  
  # Return snapshot logs from pod nginx, limiting output to 500 bytes
  kubectl logs nginx --limit-bytes=500
  
  # Return snapshot logs from pod nginx, waiting up to 20 seconds for it to start running.
  kubectl logs nginx --pod-running-timeout=20s
  
  # Return snapshot logs from pod nginx with multi containers
  kubectl logs nginx --all-containers=true
  
  # Return snapshot logs from all pods in the deployment nginx
  kubectl logs deployment/nginx --all-pods=true
  
  # Return snapshot logs from all containers in pods defined by label app=nginx
  kubectl logs -l app=nginx --all-containers=true
  
  # Return snapshot logs from all pods defined by label app=nginx, limiting concurrent log requests to 10 pods
  kubectl logs -l app=nginx --max-log-requests=10
  
  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1
  
  # Begin streaming the logs from pod nginx, continuing even if errors occur
  kubectl logs nginx -f --ignore-errors=true
  
  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1
  
  # Begin streaming the logs from all containers in pods defined by label app=nginx
  kubectl logs -f -l app=nginx --all-containers=true
  
  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx
  
  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
  
  # Show all logs with timestamps from pod nginx starting from August 30, 2024, at 06:00:00 UTC
  kubectl logs nginx --since-time=2024-08-30T06:00:00Z --timestamps=true
  
  # Show logs from a kubelet with an expired serving certificate
  kubectl logs --insecure-skip-tls-verify-backend nginx
  
  # Return snapshot logs from first container of a job named hello
  kubectl logs job/hello
  
  # Return snapshot logs from container nginx-1 of a deployment named nginx
  kubectl logs deployment/nginx -c nginx-1
```","Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

```
kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]
```

```
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx
  
  # Return snapshot logs from pod nginx, prefixing each line with the source pod and container name
  kubectl logs nginx --prefix
  
  # Return snapshot logs from pod nginx, limiting output to 500 bytes
  kubectl logs nginx --limit-bytes=500
  
  # Return snapshot logs from pod nginx, waiting up to 20 seconds for it to start running.
  kubectl logs nginx --pod-running-timeout=20s
  
  # Return snapshot logs from pod nginx with multi containers
  kubectl logs nginx --all-containers=true
  
  # Return snapshot logs from all pods in the deployment nginx
  kubectl logs deployment/nginx --all-pods=true
  
  # Return snapshot logs from all containers in pods defined by label app=nginx
  kubectl logs -l app=nginx --all-containers=true
  
  # Return snapshot logs from all pods defined by label app=nginx, limiting concurrent log requests to 10 pods
  kubectl logs -l app=nginx --max-log-requests=10
  
  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1
  
  # Begin streaming the logs from pod nginx, continuing even if errors occur
  kubectl logs nginx -f --ignore-errors=true
  
  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1
  
  # Begin streaming the logs from all containers in pods defined by label app=nginx
  kubectl logs -f -l app=nginx --all-containers=true
  
  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx
  
  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
  
  # Show all logs with timestamps from pod nginx starting from August 30, 2024, at 06:00:00 UTC
  kubectl logs nginx --since-time=2024-08-30T06:00:00Z --timestamps=true
  
  # Show logs from a kubelet with an expired serving certificate
  kubectl logs --insecure-skip-tls-verify-backend nginx
  
  # Return snapshot logs from first container of a job named hello
  kubectl logs job/hello
  
  # Return snapshot logs from container nginx-1 of a deployment named nginx
  kubectl logs deployment/nginx -c nginx-1
```","Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

```
kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]
```

```
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx
  
  # Return snapshot logs from pod nginx, prefixing each line with the source pod and container name
  kubectl logs nginx --prefix
  
  # Return snapshot logs from pod nginx, limiting output to 500 bytes
  kubectl logs nginx --limit-bytes=500
  
  # Return snapshot logs from pod nginx, waiting up to 20 seconds for it to start running.
  kubectl logs nginx --pod-running-timeout=20s
  
  # Return snapshot logs from pod nginx with multi containers
  kubectl logs nginx --all-containers=true
  
  # Return snapshot logs from all pods in the deployment nginx
  kubectl logs deployment/nginx --all-pods=true
  
  # Return snapshot logs from all containers in pods defined by label app=nginx
  kubectl logs -l app=nginx --all-containers=true
  
  # Return snapshot logs from all pods defined by label app=nginx, limiting concurrent log requests to 10 pods
  kubectl logs -l app=nginx --max-log-requests=10
  
  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1
  
  # Begin streaming the logs from pod nginx, continuing even if errors occur
  kubectl logs nginx -f --ignore-errors=true
  
  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1
  
  # Begin streaming the logs from all containers in pods defined by label app=nginx
  kubectl logs -f -l app=nginx --all-containers=true
  
  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx
  
  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
  
  # Show all logs with timestamps from pod nginx starting from August 30, 2024, at 06:00:00 UTC
  kubectl logs nginx --since-time=2024-08-30T06:00:00Z --timestamps=true
  
  # Show logs from a kubelet with an expired serving certificate
  kubectl logs --insecure-skip-tls-verify-backend nginx
  
  # Return snapshot logs from first container of a job named hello
  kubectl logs job/hello
  
  # Return snapshot logs from container nginx-1 of a deployment named nginx
  kubectl logs deployment/nginx -c nginx-1
```","Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

```
kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]
```

```
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx
  
  # Return snapshot logs from pod nginx, prefixing each line with the source pod and container name
  kubectl logs nginx --prefix
  
  # Return snapshot logs from pod nginx, limiting output to 500 bytes
  kubectl logs nginx --limit-bytes=500
  
  # Return snapshot logs from pod nginx, waiting up to 20 seconds for it to start running.
  kubectl logs nginx --pod-running-timeout=20s
  
  # Return snapshot logs from pod nginx with multi containers
  kubectl logs nginx --all-containers=true
  
  # Return snapshot logs from all pods in the deployment nginx
  kubectl logs deployment/nginx --all-pods=true
  
  # Return snapshot logs from all containers in pods defined by label app=nginx
  kubectl logs -l app=nginx --all-containers=true
  
  # Return snapshot logs from all pods defined by label app=nginx, limiting concurrent log requests to 10 pods
  kubectl logs -l app=nginx --max-log-requests=10
  
  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1
  
  # Begin streaming the logs from pod nginx, continuing even if errors occur
  kubectl logs nginx -f --ignore-errors=true
  
  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1
  
  # Begin streaming the logs from all containers in pods defined by label app=nginx
  kubectl logs -f -l app=nginx --all-containers=true
  
  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx
  
  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
  
  # Show all logs with timestamps from pod nginx starting from August 30, 2024, at 06:00:00 UTC
  kubectl logs nginx --since-time=2024-08-30T06:00:00Z --timestamps=true
  
  # Show logs from a kubelet with an expired serving certificate
  kubectl logs --insecure-skip-tls-verify-backend nginx
  
  # Return snapshot logs from first container of a job named hello
  kubectl logs job/hello
  
  # Return snapshot logs from container nginx-1 of a deployment named nginx
  kubectl logs deployment/nginx -c nginx-1
```","Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

```
kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]
```

```
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx
  
  # Return snapshot logs from pod nginx, prefixing each line with the source pod and container name
  kubectl logs nginx --prefix
  
  # Return snapshot logs from pod nginx, limiting output to 500 bytes
  kubectl logs nginx --limit-bytes=500
  
  # Return snapshot logs from pod nginx, waiting up to 20 seconds for it to start running.
  kubectl logs nginx --pod-running-timeout=20s
  
  # Return snapshot logs from pod nginx with multi containers
  kubectl logs nginx --all-containers=true
  
  # Return snapshot logs from all pods in the deployment nginx
  kubectl logs deployment/nginx --all-pods=true
  
  # Return snapshot logs from all containers in pods defined by label app=nginx
  kubectl logs -l app=nginx --all-containers=true
  
  # Return snapshot logs from all pods defined by label app=nginx, limiting concurrent log requests to 10 pods
  kubectl logs -l app=nginx --max-log-requests=10
  
  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1
  
  # Begin streaming the logs from pod nginx, continuing even if errors occur
  kubectl logs nginx -f --ignore-errors=true
  
  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1
  
  # Begin streaming the logs from all containers in pods defined by label app=nginx
  kubectl logs -f -l app=nginx --all-containers=true
  
  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx
  
  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
  
  # Show all logs with timestamps from pod nginx starting from August 30, 2024, at 06:00:00 UTC
  kubectl logs nginx --since-time=2024-08-30T06:00:00Z --timestamps=true
  
  # Show logs from a kubelet with an expired serving certificate
  kubectl logs --insecure-skip-tls-verify-backend nginx
  
  # Return snapshot logs from first container of a job named hello
  kubectl logs job/hello
  
  # Return snapshot logs from container nginx-1 of a deployment named nginx
  kubectl logs deployment/nginx -c nginx-1
```","Create a namespace with the specified name.

```
kubectl create namespace NAME [--dry-run=server|client|none]
```

```
  # Create a new namespace named my-namespace
  kubectl create namespace my-namespace
```","Create a namespace with the specified name.

```
kubectl create namespace NAME [--dry-run=server|client|none]
```

```
  # Create a new namespace named my-namespace
  kubectl create namespace my-namespace
```","Create a namespace with the specified name.

```
kubectl create namespace NAME [--dry-run=server|client|none]
```

```
  # Create a new namespace named my-namespace
  kubectl create namespace my-namespace
```","Request a service account token.

```
kubectl create token SERVICE_ACCOUNT_NAME
```

```
  # Request a token to authenticate to the kube-apiserver as the service account ""myapp"" in the current namespace
  kubectl create token myapp
  
  # Request a token for a service account in a custom namespace
  kubectl create token myapp --namespace myns
  
  # Request a token with a custom expiration
  kubectl create token myapp --duration 10m
  
  # Request a token with a custom audience
  kubectl create token myapp --audience https://example.com
  
  # Request a token bound to an instance of a Secret object
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
  
  # Request a token bound to an instance of a Secret object with a specific UID
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","Request a service account token.

```
kubectl create token SERVICE_ACCOUNT_NAME
```

```
  # Request a token to authenticate to the kube-apiserver as the service account ""myapp"" in the current namespace
  kubectl create token myapp
  
  # Request a token for a service account in a custom namespace
  kubectl create token myapp --namespace myns
  
  # Request a token with a custom expiration
  kubectl create token myapp --duration 10m
  
  # Request a token with a custom audience
  kubectl create token myapp --audience https://example.com
  
  # Request a token bound to an instance of a Secret object
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
  
  # Request a token bound to an instance of a Secret object with a specific UID
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","Request a service account token.

```
kubectl create token SERVICE_ACCOUNT_NAME
```

```
  # Request a token to authenticate to the kube-apiserver as the service account ""myapp"" in the current namespace
  kubectl create token myapp
  
  # Request a token for a service account in a custom namespace
  kubectl create token myapp --namespace myns
  
  # Request a token with a custom expiration
  kubectl create token myapp --duration 10m
  
  # Request a token with a custom audience
  kubectl create token myapp --audience https://example.com
  
  # Request a token bound to an instance of a Secret object
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
  
  # Request a token bound to an instance of a Secret object with a specific UID
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","Request a service account token.

```
kubectl create token SERVICE_ACCOUNT_NAME
```

```
  # Request a token to authenticate to the kube-apiserver as the service account ""myapp"" in the current namespace
  kubectl create token myapp
  
  # Request a token for a service account in a custom namespace
  kubectl create token myapp --namespace myns
  
  # Request a token with a custom expiration
  kubectl create token myapp --duration 10m
  
  # Request a token with a custom audience
  kubectl create token myapp --audience https://example.com
  
  # Request a token bound to an instance of a Secret object
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
  
  # Request a token bound to an instance of a Secret object with a specific UID
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","Request a service account token.

```
kubectl create token SERVICE_ACCOUNT_NAME
```

```
  # Request a token to authenticate to the kube-apiserver as the service account ""myapp"" in the current namespace
  kubectl create token myapp
  
  # Request a token for a service account in a custom namespace
  kubectl create token myapp --namespace myns
  
  # Request a token with a custom expiration
  kubectl create token myapp --duration 10m
  
  # Request a token with a custom audience
  kubectl create token myapp --audience https://example.com
  
  # Request a token bound to an instance of a Secret object
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret
  
  # Request a token bound to an instance of a Secret object with a specific UID
  kubectl create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc
```","Create an ingress with the specified name.

```
kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]] 
```

```
  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret ""my-cert""
  kubectl create ingress simple --rule=""foo.com/bar=svc1:8080,tls=my-cert""
  
  # Create a catch all ingress of ""/path"" pointing to service svc:port and Ingress Class as ""otheringress""
  kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
  
  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla
  
  # Create an ingress with the same host and multiple paths
  kubectl create ingress multipath --class=default \
  --rule=""foo.com/=svc:port"" \
  --rule=""foo.com/admin/=svcadmin:portadmin""
  
  # Create an ingress with multiple hosts and the pathType as Prefix
  kubectl create ingress ingress1 --class=default \
  --rule=""foo.com/path*=svc:8080"" \
  --rule=""bar.com/admin*=svc2:http""
  
  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  kubectl create ingress ingtls --class=default \
  --rule=""foo.com/=svc:https,tls"" \
  --rule=""foo.com/path/subpath*=othersvc:8080""
  
  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  kubectl create ingress ingsecret --class=default \
  --rule=""foo.com/*=svc:8080,tls=secret1""
  
  # Create an ingress with a default backend
  kubectl create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule=""foo.com/*=svc:8080,tls=secret1""
```","Create an ingress with the specified name.

```
kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]] 
```

```
  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret ""my-cert""
  kubectl create ingress simple --rule=""foo.com/bar=svc1:8080,tls=my-cert""
  
  # Create a catch all ingress of ""/path"" pointing to service svc:port and Ingress Class as ""otheringress""
  kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
  
  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla
  
  # Create an ingress with the same host and multiple paths
  kubectl create ingress multipath --class=default \
  --rule=""foo.com/=svc:port"" \
  --rule=""foo.com/admin/=svcadmin:portadmin""
  
  # Create an ingress with multiple hosts and the pathType as Prefix
  kubectl create ingress ingress1 --class=default \
  --rule=""foo.com/path*=svc:8080"" \
  --rule=""bar.com/admin*=svc2:http""
  
  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  kubectl create ingress ingtls --class=default \
  --rule=""foo.com/=svc:https,tls"" \
  --rule=""foo.com/path/subpath*=othersvc:8080""
  
  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  kubectl create ingress ingsecret --class=default \
  --rule=""foo.com/*=svc:8080,tls=secret1""
  
  # Create an ingress with a default backend
  kubectl create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule=""foo.com/*=svc:8080,tls=secret1""
```","Create an ingress with the specified name.

```
kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]] 
```

```
  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret ""my-cert""
  kubectl create ingress simple --rule=""foo.com/bar=svc1:8080,tls=my-cert""
  
  # Create a catch all ingress of ""/path"" pointing to service svc:port and Ingress Class as ""otheringress""
  kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
  
  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla
  
  # Create an ingress with the same host and multiple paths
  kubectl create ingress multipath --class=default \
  --rule=""foo.com/=svc:port"" \
  --rule=""foo.com/admin/=svcadmin:portadmin""
  
  # Create an ingress with multiple hosts and the pathType as Prefix
  kubectl create ingress ingress1 --class=default \
  --rule=""foo.com/path*=svc:8080"" \
  --rule=""bar.com/admin*=svc2:http""
  
  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  kubectl create ingress ingtls --class=default \
  --rule=""foo.com/=svc:https,tls"" \
  --rule=""foo.com/path/subpath*=othersvc:8080""
  
  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  kubectl create ingress ingsecret --class=default \
  --rule=""foo.com/*=svc:8080,tls=secret1""
  
  # Create an ingress with a default backend
  kubectl create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule=""foo.com/*=svc:8080,tls=secret1""
```","Create an ingress with the specified name.

```
kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]] 
```

```
  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret ""my-cert""
  kubectl create ingress simple --rule=""foo.com/bar=svc1:8080,tls=my-cert""
  
  # Create a catch all ingress of ""/path"" pointing to service svc:port and Ingress Class as ""otheringress""
  kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
  
  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla
  
  # Create an ingress with the same host and multiple paths
  kubectl create ingress multipath --class=default \
  --rule=""foo.com/=svc:port"" \
  --rule=""foo.com/admin/=svcadmin:portadmin""
  
  # Create an ingress with multiple hosts and the pathType as Prefix
  kubectl create ingress ingress1 --class=default \
  --rule=""foo.com/path*=svc:8080"" \
  --rule=""bar.com/admin*=svc2:http""
  
  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  kubectl create ingress ingtls --class=default \
  --rule=""foo.com/=svc:https,tls"" \
  --rule=""foo.com/path/subpath*=othersvc:8080""
  
  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  kubectl create ingress ingsecret --class=default \
  --rule=""foo.com/*=svc:8080,tls=secret1""
  
  # Create an ingress with a default backend
  kubectl create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule=""foo.com/*=svc:8080,tls=secret1""
```","Create an ingress with the specified name.

```
kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]] 
```

```
  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret ""my-cert""
  kubectl create ingress simple --rule=""foo.com/bar=svc1:8080,tls=my-cert""
  
  # Create a catch all ingress of ""/path"" pointing to service svc:port and Ingress Class as ""otheringress""
  kubectl create ingress catch-all --class=otheringress --rule=""/path=svc:port""
  
  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  kubectl create ingress annotated --class=default --rule=""foo.com/bar=svc:port"" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla
  
  # Create an ingress with the same host and multiple paths
  kubectl create ingress multipath --class=default \
  --rule=""foo.com/=svc:port"" \
  --rule=""foo.com/admin/=svcadmin:portadmin""
  
  # Create an ingress with multiple hosts and the pathType as Prefix
  kubectl create ingress ingress1 --class=default \
  --rule=""foo.com/path*=svc:8080"" \
  --rule=""bar.com/admin*=svc2:http""
  
  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  kubectl create ingress ingtls --class=default \
  --rule=""foo.com/=svc:https,tls"" \
  --rule=""foo.com/path/subpath*=othersvc:8080""
  
  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  kubectl create ingress ingsecret --class=default \
  --rule=""foo.com/*=svc:8080,tls=secret1""
  
  # Create an ingress with a default backend
  kubectl create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule=""foo.com/*=svc:8080,tls=secret1""
```","Create a deployment with the specified name.

```
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

```
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701
  
  # Create a deployment named my-dep that runs multiple containers
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","Create a deployment with the specified name.

```
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

```
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701
  
  # Create a deployment named my-dep that runs multiple containers
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","Create a deployment with the specified name.

```
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

```
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701
  
  # Create a deployment named my-dep that runs multiple containers
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","Create a deployment with the specified name.

```
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

```
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701
  
  # Create a deployment named my-dep that runs multiple containers
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","Create a deployment with the specified name.

```
kubectl create deployment NAME --image=image -- [COMMAND] [args...]
```

```
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701
  
  # Create a deployment named my-dep that runs multiple containers
  kubectl create deployment my-dep --image=busybox:latest --image=ubuntu:latest --image=nginx
```","Create a LoadBalancer service with the specified name.

```
kubectl create service loadbalancer NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new LoadBalancer service named my-lbs
  kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","Create a LoadBalancer service with the specified name.

```
kubectl create service loadbalancer NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new LoadBalancer service named my-lbs
  kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","Create a LoadBalancer service with the specified name.

```
kubectl create service loadbalancer NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new LoadBalancer service named my-lbs
  kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","Create a LoadBalancer service with the specified name.

```
kubectl create service loadbalancer NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new LoadBalancer service named my-lbs
  kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","Create a LoadBalancer service with the specified name.

```
kubectl create service loadbalancer NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new LoadBalancer service named my-lbs
  kubectl create service loadbalancer my-lbs --tcp=5678:8080
```","Create an ExternalName service with the specified name.

 ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.

```
kubectl create service externalname NAME --external-name external.name [--dry-run=server|client|none]
```

```
  # Create a new ExternalName service named my-ns
  kubectl create service externalname my-ns --external-name bar.com
```","Create an ExternalName service with the specified name.

 ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.

```
kubectl create service externalname NAME --external-name external.name [--dry-run=server|client|none]
```

```
  # Create a new ExternalName service named my-ns
  kubectl create service externalname my-ns --external-name bar.com
```","Create an ExternalName service with the specified name.

 ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.

```
kubectl create service externalname NAME --external-name external.name [--dry-run=server|client|none]
```

```
  # Create a new ExternalName service named my-ns
  kubectl create service externalname my-ns --external-name bar.com
```","Create an ExternalName service with the specified name.

 ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.

```
kubectl create service externalname NAME --external-name external.name [--dry-run=server|client|none]
```

```
  # Create a new ExternalName service named my-ns
  kubectl create service externalname my-ns --external-name bar.com
```","Create an ExternalName service with the specified name.

 ExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.

```
kubectl create service externalname NAME --external-name external.name [--dry-run=server|client|none]
```

```
  # Create a new ExternalName service named my-ns
  kubectl create service externalname my-ns --external-name bar.com
```","Create a secret with specified type.

 A docker-registry type secret is for accessing a container registry.

 A generic type secret indicate an Opaque secret type.

 A tls type secret holds TLS certificate and its associated key.

```
kubectl create secret (docker-registry | generic | tls)
```","Create a secret with specified type.

 A docker-registry type secret is for accessing a container registry.

 A generic type secret indicate an Opaque secret type.

 A tls type secret holds TLS certificate and its associated key.

```
kubectl create secret (docker-registry | generic | tls)
```","Create a secret with specified type.

 A docker-registry type secret is for accessing a container registry.

 A generic type secret indicate an Opaque secret type.

 A tls type secret holds TLS certificate and its associated key.

```
kubectl create secret (docker-registry | generic | tls)
```","Create a priority class with the specified name, value, globalDefault and description.

```
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL [--dry-run=server|client|none]
```

```
  # Create a priority class named high-priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority""
  
  # Create a priority class named default-priority that is considered as the global default priority
  kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
  
  # Create a priority class named high-priority that cannot preempt pods with lower priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority"" --preemption-policy=""Never""
```","Create a priority class with the specified name, value, globalDefault and description.

```
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL [--dry-run=server|client|none]
```

```
  # Create a priority class named high-priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority""
  
  # Create a priority class named default-priority that is considered as the global default priority
  kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
  
  # Create a priority class named high-priority that cannot preempt pods with lower priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority"" --preemption-policy=""Never""
```","Create a priority class with the specified name, value, globalDefault and description.

```
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL [--dry-run=server|client|none]
```

```
  # Create a priority class named high-priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority""
  
  # Create a priority class named default-priority that is considered as the global default priority
  kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
  
  # Create a priority class named high-priority that cannot preempt pods with lower priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority"" --preemption-policy=""Never""
```","Create a priority class with the specified name, value, globalDefault and description.

```
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL [--dry-run=server|client|none]
```

```
  # Create a priority class named high-priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority""
  
  # Create a priority class named default-priority that is considered as the global default priority
  kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
  
  # Create a priority class named high-priority that cannot preempt pods with lower priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority"" --preemption-policy=""Never""
```","Create a priority class with the specified name, value, globalDefault and description.

```
kubectl create priorityclass NAME --value=VALUE --global-default=BOOL [--dry-run=server|client|none]
```

```
  # Create a priority class named high-priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority""
  
  # Create a priority class named default-priority that is considered as the global default priority
  kubectl create priorityclass default-priority --value=1000 --global-default=true --description=""default priority""
  
  # Create a priority class named high-priority that cannot preempt pods with lower priority
  kubectl create priorityclass high-priority --value=1000 --description=""high priority"" --preemption-policy=""Never""
```","Create a pod disruption budget with the specified name, selector, and desired minimum available pods.

```
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N [--dry-run=server|client|none]
```

```
  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
  
  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  kubectl create pdb my-pdb --selector=app=nginx --min-available=50%
```","Create a pod disruption budget with the specified name, selector, and desired minimum available pods.

```
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N [--dry-run=server|client|none]
```

```
  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
  
  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  kubectl create pdb my-pdb --selector=app=nginx --min-available=50%
```","Create a pod disruption budget with the specified name, selector, and desired minimum available pods.

```
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N [--dry-run=server|client|none]
```

```
  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
  
  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  kubectl create pdb my-pdb --selector=app=nginx --min-available=50%
```","Create a pod disruption budget with the specified name, selector, and desired minimum available pods.

```
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N [--dry-run=server|client|none]
```

```
  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
  
  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  kubectl create pdb my-pdb --selector=app=nginx --min-available=50%
```","Create a pod disruption budget with the specified name, selector, and desired minimum available pods.

```
kubectl create poddisruptionbudget NAME --selector=SELECTOR --min-available=N [--dry-run=server|client|none]
```

```
  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  kubectl create poddisruptionbudget my-pdb --selector=app=rails --min-available=1
  
  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  kubectl create pdb my-pdb --selector=app=nginx --min-available=50%
```","Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create configmap NAME [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
  
  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a service using a specified subcommand.

```
kubectl create service [flags]
```","Create a service using a specified subcommand.

```
kubectl create service [flags]
```","Create a service using a specified subcommand.

```
kubectl create service [flags]
```","Create a cluster role binding for a particular cluster role.

```
kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1
```","Create a cluster role binding for a particular cluster role.

```
kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1
```","Create a cluster role binding for a particular cluster role.

```
kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1
```","Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar
  
  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
  
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
  
  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
  
  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar
  
  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
  
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
  
  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
  
  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar
  
  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
  
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
  
  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
  
  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar
  
  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
  
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
  
  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
  
  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

```
kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none]
```

```
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar
  
  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub
  
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
  
  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret
  
  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
```","Create a resource quota with the specified name, hard limits, and optional scopes.

```
kubectl create quota NAME [--hard=key1=value1,key2=value2] [--scopes=Scope1,Scope2] [--dry-run=server|client|none]
```

```
  # Create a new resource quota named my-quota
  kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
  
  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","Create a resource quota with the specified name, hard limits, and optional scopes.

```
kubectl create quota NAME [--hard=key1=value1,key2=value2] [--scopes=Scope1,Scope2] [--dry-run=server|client|none]
```

```
  # Create a new resource quota named my-quota
  kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
  
  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","Create a resource quota with the specified name, hard limits, and optional scopes.

```
kubectl create quota NAME [--hard=key1=value1,key2=value2] [--scopes=Scope1,Scope2] [--dry-run=server|client|none]
```

```
  # Create a new resource quota named my-quota
  kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
  
  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","Create a resource quota with the specified name, hard limits, and optional scopes.

```
kubectl create quota NAME [--hard=key1=value1,key2=value2] [--scopes=Scope1,Scope2] [--dry-run=server|client|none]
```

```
  # Create a new resource quota named my-quota
  kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
  
  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","Create a resource quota with the specified name, hard limits, and optional scopes.

```
kubectl create quota NAME [--hard=key1=value1,key2=value2] [--scopes=Scope1,Scope2] [--dry-run=server|client|none]
```

```
  # Create a new resource quota named my-quota
  kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10
  
  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
```","Create a TLS secret from the given public/private key pair.

 The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.

```
kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]
```

```
  # Create a new TLS secret named tls-secret with the given key pair
  kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","Create a TLS secret from the given public/private key pair.

 The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.

```
kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]
```

```
  # Create a new TLS secret named tls-secret with the given key pair
  kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","Create a TLS secret from the given public/private key pair.

 The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.

```
kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]
```

```
  # Create a new TLS secret named tls-secret with the given key pair
  kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","Create a TLS secret from the given public/private key pair.

 The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.

```
kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]
```

```
  # Create a new TLS secret named tls-secret with the given key pair
  kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","Create a TLS secret from the given public/private key pair.

 The public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.

```
kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]
```

```
  # Create a new TLS secret named tls-secret with the given key pair
  kubectl create secret tls tls-secret --cert=path/to/tls.crt --key=path/to/tls.key
```","Create a cron job with the specified name.

```
kubectl create cronjob NAME --image=image --schedule='0/5 * * * ?' -- [COMMAND] [args...] [flags]
```

```
  # Create a cron job
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *""
  
  # Create a cron job with a command
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```","Create a cron job with the specified name.

```
kubectl create cronjob NAME --image=image --schedule='0/5 * * * ?' -- [COMMAND] [args...] [flags]
```

```
  # Create a cron job
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *""
  
  # Create a cron job with a command
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```","Create a cron job with the specified name.

```
kubectl create cronjob NAME --image=image --schedule='0/5 * * * ?' -- [COMMAND] [args...] [flags]
```

```
  # Create a cron job
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *""
  
  # Create a cron job with a command
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```","Create a cron job with the specified name.

```
kubectl create cronjob NAME --image=image --schedule='0/5 * * * ?' -- [COMMAND] [args...] [flags]
```

```
  # Create a cron job
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *""
  
  # Create a cron job with a command
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```","Create a cron job with the specified name.

```
kubectl create cronjob NAME --image=image --schedule='0/5 * * * ?' -- [COMMAND] [args...] [flags]
```

```
  # Create a cron job
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *""
  
  # Create a cron job with a command
  kubectl create cronjob my-job --image=busybox --schedule=""*/1 * * * *"" -- date
```","Create a job with the specified name.

```
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```

```
  # Create a job
  kubectl create job my-job --image=busybox
  
  # Create a job with a command
  kubectl create job my-job --image=busybox -- date
  
  # Create a job from a cron job named ""a-cronjob""
  kubectl create job test-job --from=cronjob/a-cronjob
```","Create a job with the specified name.

```
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```

```
  # Create a job
  kubectl create job my-job --image=busybox
  
  # Create a job with a command
  kubectl create job my-job --image=busybox -- date
  
  # Create a job from a cron job named ""a-cronjob""
  kubectl create job test-job --from=cronjob/a-cronjob
```","Create a job with the specified name.

```
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```

```
  # Create a job
  kubectl create job my-job --image=busybox
  
  # Create a job with a command
  kubectl create job my-job --image=busybox -- date
  
  # Create a job from a cron job named ""a-cronjob""
  kubectl create job test-job --from=cronjob/a-cronjob
```","Create a job with the specified name.

```
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```

```
  # Create a job
  kubectl create job my-job --image=busybox
  
  # Create a job with a command
  kubectl create job my-job --image=busybox -- date
  
  # Create a job from a cron job named ""a-cronjob""
  kubectl create job test-job --from=cronjob/a-cronjob
```","Create a job with the specified name.

```
kubectl create job NAME --image=image [--from=cronjob/name] -- [COMMAND] [args...]
```

```
  # Create a job
  kubectl create job my-job --image=busybox
  
  # Create a job with a command
  kubectl create job my-job --image=busybox -- date
  
  # Create a job from a cron job named ""a-cronjob""
  kubectl create job test-job --from=cronjob/a-cronjob
```","Create a ClusterIP service with the specified name.

```
kubectl create service clusterip NAME [--tcp=<port>:<targetPort>] [--dry-run=server|client|none]
```

```
  # Create a new ClusterIP service named my-cs
  kubectl create service clusterip my-cs --tcp=5678:8080
  
  # Create a new ClusterIP service named my-cs (in headless mode)
  kubectl create service clusterip my-cs --clusterip=""None""
```","Create a ClusterIP service with the specified name.

```
kubectl create service clusterip NAME [--tcp=<port>:<targetPort>] [--dry-run=server|client|none]
```

```
  # Create a new ClusterIP service named my-cs
  kubectl create service clusterip my-cs --tcp=5678:8080
  
  # Create a new ClusterIP service named my-cs (in headless mode)
  kubectl create service clusterip my-cs --clusterip=""None""
```","Create a ClusterIP service with the specified name.

```
kubectl create service clusterip NAME [--tcp=<port>:<targetPort>] [--dry-run=server|client|none]
```

```
  # Create a new ClusterIP service named my-cs
  kubectl create service clusterip my-cs --tcp=5678:8080
  
  # Create a new ClusterIP service named my-cs (in headless mode)
  kubectl create service clusterip my-cs --clusterip=""None""
```","Create a role with single rule.

```
kubectl create role NAME --verb=verb --resource=resource.group/subresource [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
  
  # Create a role named ""pod-reader"" with ResourceName specified
  kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a role named ""foo"" with API Group specified
  kubectl create role foo --verb=get,list,watch --resource=rs.apps
  
  # Create a role named ""foo"" with SubResource specified
  kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
```","Create a role with single rule.

```
kubectl create role NAME --verb=verb --resource=resource.group/subresource [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
  
  # Create a role named ""pod-reader"" with ResourceName specified
  kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a role named ""foo"" with API Group specified
  kubectl create role foo --verb=get,list,watch --resource=rs.apps
  
  # Create a role named ""foo"" with SubResource specified
  kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
```","Create a role with single rule.

```
kubectl create role NAME --verb=verb --resource=resource.group/subresource [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
  
  # Create a role named ""pod-reader"" with ResourceName specified
  kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a role named ""foo"" with API Group specified
  kubectl create role foo --verb=get,list,watch --resource=rs.apps
  
  # Create a role named ""foo"" with SubResource specified
  kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
```","Create a role binding for a particular role or cluster role.

```
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
  
  # Create a role binding for service account monitoring:sa-dev using the admin role
  kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","Create a role binding for a particular role or cluster role.

```
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
  
  # Create a role binding for service account monitoring:sa-dev using the admin role
  kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","Create a role binding for a particular role or cluster role.

```
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
  
  # Create a role binding for service account monitoring:sa-dev using the admin role
  kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","Create a role binding for a particular role or cluster role.

```
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
  
  # Create a role binding for service account monitoring:sa-dev using the admin role
  kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","Create a role binding for a particular role or cluster role.

```
kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none]
```

```
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
  
  # Create a role binding for service account monitoring:sa-dev using the admin role
  kubectl create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev
```","Create a new secret for use with Docker registries.
        
        Dockercfg secrets are used to authenticate against Docker registries.
        
        When using the Docker command line to push images, you can authenticate to a given registry by running:
        '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.
        
 That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

        When creating applications, you may have a Docker registry that requires authentication.  In order for the
        nodes to pull images on your behalf, they must have the credentials.  You can provide this information
        by creating a dockercfg secret and attaching it to your service account.

```
kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] [--dry-run=server|client|none]
```

```
  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
  
  # Create a new secret named my-secret from ~/.docker/config.json
  kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","Create a new secret for use with Docker registries.
        
        Dockercfg secrets are used to authenticate against Docker registries.
        
        When using the Docker command line to push images, you can authenticate to a given registry by running:
        '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.
        
 That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

        When creating applications, you may have a Docker registry that requires authentication.  In order for the
        nodes to pull images on your behalf, they must have the credentials.  You can provide this information
        by creating a dockercfg secret and attaching it to your service account.

```
kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] [--dry-run=server|client|none]
```

```
  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
  
  # Create a new secret named my-secret from ~/.docker/config.json
  kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","Create a new secret for use with Docker registries.
        
        Dockercfg secrets are used to authenticate against Docker registries.
        
        When using the Docker command line to push images, you can authenticate to a given registry by running:
        '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.
        
 That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

        When creating applications, you may have a Docker registry that requires authentication.  In order for the
        nodes to pull images on your behalf, they must have the credentials.  You can provide this information
        by creating a dockercfg secret and attaching it to your service account.

```
kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] [--dry-run=server|client|none]
```

```
  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
  
  # Create a new secret named my-secret from ~/.docker/config.json
  kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","Create a new secret for use with Docker registries.
        
        Dockercfg secrets are used to authenticate against Docker registries.
        
        When using the Docker command line to push images, you can authenticate to a given registry by running:
        '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.
        
 That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

        When creating applications, you may have a Docker registry that requires authentication.  In order for the
        nodes to pull images on your behalf, they must have the credentials.  You can provide this information
        by creating a dockercfg secret and attaching it to your service account.

```
kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] [--dry-run=server|client|none]
```

```
  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
  
  # Create a new secret named my-secret from ~/.docker/config.json
  kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","Create a new secret for use with Docker registries.
        
        Dockercfg secrets are used to authenticate against Docker registries.
        
        When using the Docker command line to push images, you can authenticate to a given registry by running:
        '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.
        
 That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.

        When creating applications, you may have a Docker registry that requires authentication.  In order for the
        nodes to pull images on your behalf, they must have the credentials.  You can provide this information
        by creating a dockercfg secret and attaching it to your service account.

```
kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] [--dry-run=server|client|none]
```

```
  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
  
  # Create a new secret named my-secret from ~/.docker/config.json
  kubectl create secret docker-registry my-secret --from-file=path/to/.docker/config.json
```","Create a NodePort service with the specified name.

```
kubectl create service nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new NodePort service named my-ns
  kubectl create service nodeport my-ns --tcp=5678:8080
```","Create a NodePort service with the specified name.

```
kubectl create service nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new NodePort service named my-ns
  kubectl create service nodeport my-ns --tcp=5678:8080
```","Create a NodePort service with the specified name.

```
kubectl create service nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new NodePort service named my-ns
  kubectl create service nodeport my-ns --tcp=5678:8080
```","Create a NodePort service with the specified name.

```
kubectl create service nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new NodePort service named my-ns
  kubectl create service nodeport my-ns --tcp=5678:8080
```","Create a NodePort service with the specified name.

```
kubectl create service nodeport NAME [--tcp=port:targetPort] [--dry-run=server|client|none]
```

```
  # Create a new NodePort service named my-ns
  kubectl create service nodeport my-ns --tcp=5678:8080
```","Create a service account with the specified name.

```
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```

```
  # Create a new service account named my-service-account
  kubectl create serviceaccount my-service-account
```","Create a service account with the specified name.

```
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```

```
  # Create a new service account named my-service-account
  kubectl create serviceaccount my-service-account
```","Create a service account with the specified name.

```
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```

```
  # Create a new service account named my-service-account
  kubectl create serviceaccount my-service-account
```","Create a service account with the specified name.

```
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```

```
  # Create a new service account named my-service-account
  kubectl create serviceaccount my-service-account
```","Create a service account with the specified name.

```
kubectl create serviceaccount NAME [--dry-run=server|client|none]
```

```
  # Create a new service account named my-service-account
  kubectl create serviceaccount my-service-account
```","Create a cluster role.

```
kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a cluster role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  
  # Create a cluster role named ""pod-reader"" with ResourceName specified
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a cluster role named ""foo"" with API Group specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps
  
  # Create a cluster role named ""foo"" with SubResource specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  # Create a cluster role name ""foo"" with NonResourceURL specified
  kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
  
  # Create a cluster role name ""monitoring"" with AggregationRule specified
  kubectl create clusterrole monitoring --aggregation-rule=""rbac.example.com/aggregate-to-monitoring=true""
```","Create a cluster role.

```
kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a cluster role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  
  # Create a cluster role named ""pod-reader"" with ResourceName specified
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a cluster role named ""foo"" with API Group specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps
  
  # Create a cluster role named ""foo"" with SubResource specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  # Create a cluster role name ""foo"" with NonResourceURL specified
  kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
  
  # Create a cluster role name ""monitoring"" with AggregationRule specified
  kubectl create clusterrole monitoring --aggregation-rule=""rbac.example.com/aggregate-to-monitoring=true""
```","Create a cluster role.

```
kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a cluster role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  
  # Create a cluster role named ""pod-reader"" with ResourceName specified
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a cluster role named ""foo"" with API Group specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps
  
  # Create a cluster role named ""foo"" with SubResource specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  # Create a cluster role name ""foo"" with NonResourceURL specified
  kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
  
  # Create a cluster role name ""monitoring"" with AggregationRule specified
  kubectl create clusterrole monitoring --aggregation-rule=""rbac.example.com/aggregate-to-monitoring=true""
```","Create a cluster role.

```
kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a cluster role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  
  # Create a cluster role named ""pod-reader"" with ResourceName specified
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a cluster role named ""foo"" with API Group specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps
  
  # Create a cluster role named ""foo"" with SubResource specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  # Create a cluster role name ""foo"" with NonResourceURL specified
  kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
  
  # Create a cluster role name ""monitoring"" with AggregationRule specified
  kubectl create clusterrole monitoring --aggregation-rule=""rbac.example.com/aggregate-to-monitoring=true""
```","Create a cluster role.

```
kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] [--dry-run=server|client|none]
```

```
  # Create a cluster role named ""pod-reader"" that allows user to perform ""get"", ""watch"" and ""list"" on pods
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  
  # Create a cluster role named ""pod-reader"" with ResourceName specified
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  # Create a cluster role named ""foo"" with API Group specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=rs.apps
  
  # Create a cluster role named ""foo"" with SubResource specified
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  # Create a cluster role name ""foo"" with NonResourceURL specified
  kubectl create clusterrole ""foo"" --verb=get --non-resource-url=/logs/*
  
  # Create a cluster role name ""monitoring"" with AggregationRule specified
  kubectl create clusterrole monitoring --aggregation-rule=""rbac.example.com/aggregate-to-monitoring=true""
```","Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

```
kubectl create -f FILENAME
```

```
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json
```","Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

```
kubectl create -f FILENAME
```

```
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json
```","Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

```
kubectl create -f FILENAME
```

```
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json
```","Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

```
kubectl create -f FILENAME
```

```
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json
```","Print the supported API versions on the server, in the form of ""group/version"".

```
kubectl api-versions
```

```
  # Print the supported API versions
  kubectl api-versions
```","Print the supported API versions on the server, in the form of ""group/version"".

```
kubectl api-versions
```

```
  # Print the supported API versions
  kubectl api-versions
```","Print the supported API versions on the server, in the form of ""group/version"".

```
kubectl api-versions
```

```
  # Print the supported API versions
  kubectl api-versions
```","Execute a command in a container.

```
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]
```

```
  # Get output from running the 'date' command from pod mypod, using the first container by default
  kubectl exec mypod -- date
  
  # Get output from running the 'date' command in ruby-container from pod mypod
  kubectl exec mypod -c ruby-container -- date
  
  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  kubectl exec mypod -c ruby-container -i -t -- bash -il
  
  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not ""ls -t /usr"")
  kubectl exec mypod -i -t -- ls -t /usr
  
  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  kubectl exec deploy/mydeployment -- date
  
  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  kubectl exec svc/myservice -- date
```","Execute a command in a container.

```
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]
```

```
  # Get output from running the 'date' command from pod mypod, using the first container by default
  kubectl exec mypod -- date
  
  # Get output from running the 'date' command in ruby-container from pod mypod
  kubectl exec mypod -c ruby-container -- date
  
  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  kubectl exec mypod -c ruby-container -i -t -- bash -il
  
  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not ""ls -t /usr"")
  kubectl exec mypod -i -t -- ls -t /usr
  
  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  kubectl exec deploy/mydeployment -- date
  
  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  kubectl exec svc/myservice -- date
```","Execute a command in a container.

```
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]
```

```
  # Get output from running the 'date' command from pod mypod, using the first container by default
  kubectl exec mypod -- date
  
  # Get output from running the 'date' command in ruby-container from pod mypod
  kubectl exec mypod -c ruby-container -- date
  
  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  kubectl exec mypod -c ruby-container -i -t -- bash -il
  
  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not ""ls -t /usr"")
  kubectl exec mypod -i -t -- ls -t /usr
  
  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  kubectl exec deploy/mydeployment -- date
  
  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  kubectl exec svc/myservice -- date
```","Execute a command in a container.

```
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]
```

```
  # Get output from running the 'date' command from pod mypod, using the first container by default
  kubectl exec mypod -- date
  
  # Get output from running the 'date' command in ruby-container from pod mypod
  kubectl exec mypod -c ruby-container -- date
  
  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  kubectl exec mypod -c ruby-container -i -t -- bash -il
  
  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not ""ls -t /usr"")
  kubectl exec mypod -i -t -- ls -t /usr
  
  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  kubectl exec deploy/mydeployment -- date
  
  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  kubectl exec svc/myservice -- date
```","Execute a command in a container.

```
kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...]
```

```
  # Get output from running the 'date' command from pod mypod, using the first container by default
  kubectl exec mypod -- date
  
  # Get output from running the 'date' command in ruby-container from pod mypod
  kubectl exec mypod -c ruby-container -- date
  
  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  kubectl exec mypod -c ruby-container -i -t -- bash -il
  
  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not ""ls -t /usr"")
  kubectl exec mypod -i -t -- ls -t /usr
  
  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  kubectl exec deploy/mydeployment -- date
  
  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  kubectl exec svc/myservice -- date
```","Update the labels on a resource.

  *  A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.
  *  If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.

```
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  kubectl label pods foo unhealthy=true
  
  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  kubectl label --overwrite pods foo status=unhealthy
  
  # Update all pods in the namespace
  kubectl label pods --all status=unhealthy
  
  # Update a pod identified by the type and name in ""pod.json""
  kubectl label -f pod.json status=unhealthy
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl label pods foo status=unhealthy --resource-version=1
  
  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  kubectl label pods foo bar-
```","Update the labels on a resource.

  *  A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.
  *  If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.

```
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  kubectl label pods foo unhealthy=true
  
  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  kubectl label --overwrite pods foo status=unhealthy
  
  # Update all pods in the namespace
  kubectl label pods --all status=unhealthy
  
  # Update a pod identified by the type and name in ""pod.json""
  kubectl label -f pod.json status=unhealthy
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl label pods foo status=unhealthy --resource-version=1
  
  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  kubectl label pods foo bar-
```","Update the labels on a resource.

  *  A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.
  *  If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.

```
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  kubectl label pods foo unhealthy=true
  
  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  kubectl label --overwrite pods foo status=unhealthy
  
  # Update all pods in the namespace
  kubectl label pods --all status=unhealthy
  
  # Update a pod identified by the type and name in ""pod.json""
  kubectl label -f pod.json status=unhealthy
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl label pods foo status=unhealthy --resource-version=1
  
  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  kubectl label pods foo bar-
```","Update the labels on a resource.

  *  A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.
  *  If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.

```
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  kubectl label pods foo unhealthy=true
  
  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  kubectl label --overwrite pods foo status=unhealthy
  
  # Update all pods in the namespace
  kubectl label pods --all status=unhealthy
  
  # Update a pod identified by the type and name in ""pod.json""
  kubectl label -f pod.json status=unhealthy
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl label pods foo status=unhealthy --resource-version=1
  
  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  kubectl label pods foo bar-
```","Update the labels on a resource.

  *  A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.
  *  If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.

```
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
```

```
  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  kubectl label pods foo unhealthy=true
  
  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  kubectl label --overwrite pods foo status=unhealthy
  
  # Update all pods in the namespace
  kubectl label pods --all status=unhealthy
  
  # Update a pod identified by the type and name in ""pod.json""
  kubectl label -f pod.json status=unhealthy
  
  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl label pods foo status=unhealthy --resource-version=1
  
  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  kubectl label pods foo bar-
```","Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in the current namespace if you don't specify any namespace.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl get [(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
```

```
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the ""v1"" version of the ""apps"" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in ""pod.yaml"" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List the 'status' subresource for a single pod
  kubectl get pod web-pod-13je7 --subresource status
  
  # List all deployments in namespace 'backend'
  kubectl get deployments.apps --namespace backend
  
  # List all pods existing in all namespaces
  kubectl get pods --all-namespaces
```","Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in the current namespace if you don't specify any namespace.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl get [(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
```

```
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the ""v1"" version of the ""apps"" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in ""pod.yaml"" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List the 'status' subresource for a single pod
  kubectl get pod web-pod-13je7 --subresource status
  
  # List all deployments in namespace 'backend'
  kubectl get deployments.apps --namespace backend
  
  # List all pods existing in all namespaces
  kubectl get pods --all-namespaces
```","Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in the current namespace if you don't specify any namespace.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl get [(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
```

```
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the ""v1"" version of the ""apps"" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in ""pod.yaml"" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List the 'status' subresource for a single pod
  kubectl get pod web-pod-13je7 --subresource status
  
  # List all deployments in namespace 'backend'
  kubectl get deployments.apps --namespace backend
  
  # List all pods existing in all namespaces
  kubectl get pods --all-namespaces
```","Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in the current namespace if you don't specify any namespace.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl get [(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
```

```
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the ""v1"" version of the ""apps"" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in ""pod.yaml"" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List the 'status' subresource for a single pod
  kubectl get pod web-pod-13je7 --subresource status
  
  # List all deployments in namespace 'backend'
  kubectl get deployments.apps --namespace backend
  
  # List all pods existing in all namespaces
  kubectl get pods --all-namespaces
```","Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in the current namespace if you don't specify any namespace.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.

Use ""kubectl api-resources"" for a complete list of supported resources.

```
kubectl get [(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
```

```
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the ""v1"" version of the ""apps"" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in ""pod.yaml"" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List the 'status' subresource for a single pod
  kubectl get pod web-pod-13je7 --subresource status
  
  # List all deployments in namespace 'backend'
  kubectl get deployments.apps --namespace backend
  
  # List all pods existing in all namespaces
  kubectl get pods --all-namespaces
```","Delete resources by file names, stdin, resources and names, or by resources and label selector.

 JSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.

 Some resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.

 IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.

 Note that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.

 After a CustomResourceDefinition is deleted, invalidation of discovery cache may take up to 6 hours. If you don't want to wait, you might want to run ""kubectl api-resources"" to refresh the discovery cache.

```
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)])
```

```
  # Delete a pod using the type and name specified in pod.json
  kubectl delete -f ./pod.json
  
  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl delete -k dir
  
  # Delete resources from all files that end with '.json'
  kubectl delete -f '*.json'
  
  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | kubectl delete -f -
  
  # Delete pods and services with same names ""baz"" and ""foo""
  kubectl delete pod,service baz foo
  
  # Delete pods and services with label name=myLabel
  kubectl delete pods,services -l name=myLabel
  
  # Delete a pod with minimal delay
  kubectl delete pod foo --now
  
  # Force delete a pod on a dead node
  kubectl delete pod foo --force
  
  # Delete all pods
  kubectl delete pods --all
  
  # Delete all pods only if the user confirms the deletion
  kubectl delete pods --all --interactive
```","Delete resources by file names, stdin, resources and names, or by resources and label selector.

 JSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.

 Some resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.

 IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.

 Note that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.

 After a CustomResourceDefinition is deleted, invalidation of discovery cache may take up to 6 hours. If you don't want to wait, you might want to run ""kubectl api-resources"" to refresh the discovery cache.

```
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)])
```

```
  # Delete a pod using the type and name specified in pod.json
  kubectl delete -f ./pod.json
  
  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl delete -k dir
  
  # Delete resources from all files that end with '.json'
  kubectl delete -f '*.json'
  
  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | kubectl delete -f -
  
  # Delete pods and services with same names ""baz"" and ""foo""
  kubectl delete pod,service baz foo
  
  # Delete pods and services with label name=myLabel
  kubectl delete pods,services -l name=myLabel
  
  # Delete a pod with minimal delay
  kubectl delete pod foo --now
  
  # Force delete a pod on a dead node
  kubectl delete pod foo --force
  
  # Delete all pods
  kubectl delete pods --all
  
  # Delete all pods only if the user confirms the deletion
  kubectl delete pods --all --interactive
```","Delete resources by file names, stdin, resources and names, or by resources and label selector.

 JSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.

 Some resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.

 IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.

 Note that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.

 After a CustomResourceDefinition is deleted, invalidation of discovery cache may take up to 6 hours. If you don't want to wait, you might want to run ""kubectl api-resources"" to refresh the discovery cache.

```
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)])
```

```
  # Delete a pod using the type and name specified in pod.json
  kubectl delete -f ./pod.json
  
  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl delete -k dir
  
  # Delete resources from all files that end with '.json'
  kubectl delete -f '*.json'
  
  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | kubectl delete -f -
  
  # Delete pods and services with same names ""baz"" and ""foo""
  kubectl delete pod,service baz foo
  
  # Delete pods and services with label name=myLabel
  kubectl delete pods,services -l name=myLabel
  
  # Delete a pod with minimal delay
  kubectl delete pod foo --now
  
  # Force delete a pod on a dead node
  kubectl delete pod foo --force
  
  # Delete all pods
  kubectl delete pods --all
  
  # Delete all pods only if the user confirms the deletion
  kubectl delete pods --all --interactive
```","Delete resources by file names, stdin, resources and names, or by resources and label selector.

 JSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.

 Some resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.

 IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.

 Note that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.

 After a CustomResourceDefinition is deleted, invalidation of discovery cache may take up to 6 hours. If you don't want to wait, you might want to run ""kubectl api-resources"" to refresh the discovery cache.

```
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)])
```

```
  # Delete a pod using the type and name specified in pod.json
  kubectl delete -f ./pod.json
  
  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl delete -k dir
  
  # Delete resources from all files that end with '.json'
  kubectl delete -f '*.json'
  
  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | kubectl delete -f -
  
  # Delete pods and services with same names ""baz"" and ""foo""
  kubectl delete pod,service baz foo
  
  # Delete pods and services with label name=myLabel
  kubectl delete pods,services -l name=myLabel
  
  # Delete a pod with minimal delay
  kubectl delete pod foo --now
  
  # Force delete a pod on a dead node
  kubectl delete pod foo --force
  
  # Delete all pods
  kubectl delete pods --all
  
  # Delete all pods only if the user confirms the deletion
  kubectl delete pods --all --interactive
```","Delete resources by file names, stdin, resources and names, or by resources and label selector.

 JSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.

 Some resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.

 IMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.

 Note that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.

 After a CustomResourceDefinition is deleted, invalidation of discovery cache may take up to 6 hours. If you don't want to wait, you might want to run ""kubectl api-resources"" to refresh the discovery cache.

```
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)])
```

```
  # Delete a pod using the type and name specified in pod.json
  kubectl delete -f ./pod.json
  
  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl delete -k dir
  
  # Delete resources from all files that end with '.json'
  kubectl delete -f '*.json'
  
  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | kubectl delete -f -
  
  # Delete pods and services with same names ""baz"" and ""foo""
  kubectl delete pod,service baz foo
  
  # Delete pods and services with label name=myLabel
  kubectl delete pods,services -l name=myLabel
  
  # Delete a pod with minimal delay
  kubectl delete pod foo --now
  
  # Force delete a pod on a dead node
  kubectl delete pod foo --force
  
  # Delete all pods
  kubectl delete pods --all
  
  # Delete all pods only if the user confirms the deletion
  kubectl delete pods --all --interactive
```","Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```
  # Show metrics for all pods in the default namespace
  kubectl top pod
  
  # Show metrics for all pods in the given namespace
  kubectl top pod --namespace=NAMESPACE
  
  # Show metrics for a given pod and its containers
  kubectl top pod POD_NAME --containers
  
  # Show metrics for the pods defined by label name=myLabel
  kubectl top pod -l name=myLabel
```","Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```
  # Show metrics for all pods in the default namespace
  kubectl top pod
  
  # Show metrics for all pods in the given namespace
  kubectl top pod --namespace=NAMESPACE
  
  # Show metrics for a given pod and its containers
  kubectl top pod POD_NAME --containers
  
  # Show metrics for the pods defined by label name=myLabel
  kubectl top pod -l name=myLabel
```","Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```
  # Show metrics for all pods in the default namespace
  kubectl top pod
  
  # Show metrics for all pods in the given namespace
  kubectl top pod --namespace=NAMESPACE
  
  # Show metrics for a given pod and its containers
  kubectl top pod POD_NAME --containers
  
  # Show metrics for the pods defined by label name=myLabel
  kubectl top pod -l name=myLabel
```","Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```
  # Show metrics for all pods in the default namespace
  kubectl top pod
  
  # Show metrics for all pods in the given namespace
  kubectl top pod --namespace=NAMESPACE
  
  # Show metrics for a given pod and its containers
  kubectl top pod POD_NAME --containers
  
  # Show metrics for the pods defined by label name=myLabel
  kubectl top pod -l name=myLabel
```","Display resource (CPU/memory) usage of pods.

 The 'top pod' command allows you to see the resource consumption of pods.

 Due to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.

```
kubectl top pod [NAME | -l label]
```

```
  # Show metrics for all pods in the default namespace
  kubectl top pod
  
  # Show metrics for all pods in the given namespace
  kubectl top pod --namespace=NAMESPACE
  
  # Show metrics for a given pod and its containers
  kubectl top pod POD_NAME --containers
  
  # Show metrics for the pods defined by label name=myLabel
  kubectl top pod -l name=myLabel
```","Display resource (CPU/memory) usage of nodes.

 The top-node command allows you to see the resource consumption of nodes.

```
kubectl top node [NAME | -l label]
```

```
  # Show metrics for all nodes
  kubectl top node
  
  # Show metrics for a given node
  kubectl top node NODE_NAME
```","Display resource (CPU/memory) usage of nodes.

 The top-node command allows you to see the resource consumption of nodes.

```
kubectl top node [NAME | -l label]
```

```
  # Show metrics for all nodes
  kubectl top node
  
  # Show metrics for a given node
  kubectl top node NODE_NAME
```","Display resource (CPU/memory) usage of nodes.

 The top-node command allows you to see the resource consumption of nodes.

```
kubectl top node [NAME | -l label]
```

```
  # Show metrics for all nodes
  kubectl top node
  
  # Show metrics for a given node
  kubectl top node NODE_NAME
```","Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```","Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```","Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```","Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```","Display resource (CPU/memory) usage.

 This command provides a view of recent resource consumption for nodes and pods. It fetches metrics from the Metrics Server, which aggregates this data from the kubelet on each node. The Metrics Server must be installed and running in the cluster for this command to work.

 The metrics shown are specifically optimized for Kubernetes autoscaling decisions, such as those made by the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). Because of this, the values may not match those from standard OS tools like 'top', as the metrics are designed to provide a stable signal for autoscalers rather than for pinpoint accuracy.

 When to use this command:

  *  For on-the-fly spot-checks of resource usage (e.g. identify which pods are consuming the most resources at a glance, or get a quick sense of the load on your nodes)
  *  Understand current resource consumption patterns
  *  Validate the behavior of your HPA or VPA configurations by seeing the metrics they use for scaling decisions.

 It is not intended to be a replacement for full-featured monitoring solutions. Its primary design goal is to provide a low-overhead signal for autoscalers, not to be a perfectly accurate monitoring tool. For high-accuracy reporting, historical analysis, dashboarding, or alerting, you should use a dedicated monitoring solution.

```
kubectl top [flags]
```","Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.

```
kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

```
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in ""foo.yaml"" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web
```","Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.

```
kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

```
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in ""foo.yaml"" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web
```","Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.

```
kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

```
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in ""foo.yaml"" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web
```","Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.

```
kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

```
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in ""foo.yaml"" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web
```","Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.

```
kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

```
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in ""foo.yaml"" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/example1 rc/example2 rc/example3
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web
```","Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N
```

```
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  kubectl taint nodes foo dedicated=special-user:NoSchedule
  
  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  kubectl taint nodes foo dedicated:NoSchedule-
  
  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-
  
  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule
  
  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule
```","Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N
```

```
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  kubectl taint nodes foo dedicated=special-user:NoSchedule
  
  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  kubectl taint nodes foo dedicated:NoSchedule-
  
  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-
  
  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule
  
  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule
```","Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N
```

```
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  kubectl taint nodes foo dedicated=special-user:NoSchedule
  
  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  kubectl taint nodes foo dedicated:NoSchedule-
  
  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-
  
  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule
  
  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule
```","Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N
```

```
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  kubectl taint nodes foo dedicated=special-user:NoSchedule
  
  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  kubectl taint nodes foo dedicated:NoSchedule-
  
  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-
  
  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule
  
  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule
```","Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N
```

```
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  kubectl taint nodes foo dedicated=special-user:NoSchedule
  
  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  kubectl taint nodes foo dedicated:NoSchedule-
  
  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-
  
  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule
  
  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule
```","Roll back to a previous rollout.

```
kubectl rollout undo (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Roll back to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Roll back to daemonset revision 3
  kubectl rollout undo daemonset/abc --to-revision=3
  
  # Roll back to the previous deployment with dry-run
  kubectl rollout undo --dry-run=server deployment/abc
```","Roll back to a previous rollout.

```
kubectl rollout undo (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Roll back to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Roll back to daemonset revision 3
  kubectl rollout undo daemonset/abc --to-revision=3
  
  # Roll back to the previous deployment with dry-run
  kubectl rollout undo --dry-run=server deployment/abc
```","Roll back to a previous rollout.

```
kubectl rollout undo (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Roll back to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Roll back to daemonset revision 3
  kubectl rollout undo daemonset/abc --to-revision=3
  
  # Roll back to the previous deployment with dry-run
  kubectl rollout undo --dry-run=server deployment/abc
```","Resume a paused resource.

 Paused resources will not be reconciled by a controller. By resuming a resource, we allow it to be reconciled again. Currently only deployments support being resumed.

```
kubectl rollout resume RESOURCE
```

```
  # Resume an already paused deployment
  kubectl rollout resume deployment/nginx
```","Resume a paused resource.

 Paused resources will not be reconciled by a controller. By resuming a resource, we allow it to be reconciled again. Currently only deployments support being resumed.

```
kubectl rollout resume RESOURCE
```

```
  # Resume an already paused deployment
  kubectl rollout resume deployment/nginx
```","Resume a paused resource.

 Paused resources will not be reconciled by a controller. By resuming a resource, we allow it to be reconciled again. Currently only deployments support being resumed.

```
kubectl rollout resume RESOURCE
```

```
  # Resume an already paused deployment
  kubectl rollout resume deployment/nginx
```","Restart a resource.

        Resource rollout will be restarted.

```
kubectl rollout restart RESOURCE
```

```
  # Restart all deployments in the test-namespace namespace
  kubectl rollout restart deployment -n test-namespace
  
  # Restart a deployment
  kubectl rollout restart deployment/nginx
  
  # Restart a daemon set
  kubectl rollout restart daemonset/abc
  
  # Restart deployments with the app=nginx label
  kubectl rollout restart deployment --selector=app=nginx
```","Restart a resource.

        Resource rollout will be restarted.

```
kubectl rollout restart RESOURCE
```

```
  # Restart all deployments in the test-namespace namespace
  kubectl rollout restart deployment -n test-namespace
  
  # Restart a deployment
  kubectl rollout restart deployment/nginx
  
  # Restart a daemon set
  kubectl rollout restart daemonset/abc
  
  # Restart deployments with the app=nginx label
  kubectl rollout restart deployment --selector=app=nginx
```","Restart a resource.

        Resource rollout will be restarted.

```
kubectl rollout restart RESOURCE
```

```
  # Restart all deployments in the test-namespace namespace
  kubectl rollout restart deployment -n test-namespace
  
  # Restart a deployment
  kubectl rollout restart deployment/nginx
  
  # Restart a daemon set
  kubectl rollout restart daemonset/abc
  
  # Restart deployments with the app=nginx label
  kubectl rollout restart deployment --selector=app=nginx
```","Restart a resource.

        Resource rollout will be restarted.

```
kubectl rollout restart RESOURCE
```

```
  # Restart all deployments in the test-namespace namespace
  kubectl rollout restart deployment -n test-namespace
  
  # Restart a deployment
  kubectl rollout restart deployment/nginx
  
  # Restart a daemon set
  kubectl rollout restart daemonset/abc
  
  # Restart deployments with the app=nginx label
  kubectl rollout restart deployment --selector=app=nginx
```","Restart a resource.

        Resource rollout will be restarted.

```
kubectl rollout restart RESOURCE
```

```
  # Restart all deployments in the test-namespace namespace
  kubectl rollout restart deployment -n test-namespace
  
  # Restart a deployment
  kubectl rollout restart deployment/nginx
  
  # Restart a daemon set
  kubectl rollout restart daemonset/abc
  
  # Restart deployments with the app=nginx label
  kubectl rollout restart deployment --selector=app=nginx
```","Show the status of the rollout.

 By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Watch the rollout status of a deployment
  kubectl rollout status deployment/nginx
```","Show the status of the rollout.

 By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Watch the rollout status of a deployment
  kubectl rollout status deployment/nginx
```","Show the status of the rollout.

 By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Watch the rollout status of a deployment
  kubectl rollout status deployment/nginx
```","Show the status of the rollout.

 By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Watch the rollout status of a deployment
  kubectl rollout status deployment/nginx
```","Show the status of the rollout.

 By default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.

```
kubectl rollout status (TYPE NAME | TYPE/NAME) [flags]
```

```
  # Watch the rollout status of a deployment
  kubectl rollout status deployment/nginx
```","Mark the provided resource as paused.

 Paused resources will not be reconciled by a controller. Use ""kubectl rollout resume"" to resume a paused resource. Currently only deployments support being paused.

```
kubectl rollout pause RESOURCE
```

```
  # Mark the nginx deployment as paused
  # Any current state of the deployment will continue its function; new updates
  # to the deployment will not have an effect as long as the deployment is paused
  kubectl rollout pause deployment/nginx
```","Mark the provided resource as paused.

 Paused resources will not be reconciled by a controller. Use ""kubectl rollout resume"" to resume a paused resource. Currently only deployments support being paused.

```
kubectl rollout pause RESOURCE
```

```
  # Mark the nginx deployment as paused
  # Any current state of the deployment will continue its function; new updates
  # to the deployment will not have an effect as long as the deployment is paused
  kubectl rollout pause deployment/nginx
```","Mark the provided resource as paused.

 Paused resources will not be reconciled by a controller. Use ""kubectl rollout resume"" to resume a paused resource. Currently only deployments support being paused.

```
kubectl rollout pause RESOURCE
```

```
  # Mark the nginx deployment as paused
  # Any current state of the deployment will continue its function; new updates
  # to the deployment will not have an effect as long as the deployment is paused
  kubectl rollout pause deployment/nginx
```","Mark the provided resource as paused.

 Paused resources will not be reconciled by a controller. Use ""kubectl rollout resume"" to resume a paused resource. Currently only deployments support being paused.

```
kubectl rollout pause RESOURCE
```

```
  # Mark the nginx deployment as paused
  # Any current state of the deployment will continue its function; new updates
  # to the deployment will not have an effect as long as the deployment is paused
  kubectl rollout pause deployment/nginx
```","Mark the provided resource as paused.

 Paused resources will not be reconciled by a controller. Use ""kubectl rollout resume"" to resume a paused resource. Currently only deployments support being paused.

```
kubectl rollout pause RESOURCE
```

```
  # Mark the nginx deployment as paused
  # Any current state of the deployment will continue its function; new updates
  # to the deployment will not have an effect as long as the deployment is paused
  kubectl rollout pause deployment/nginx
```","Manage the rollout of one or many resources.
        
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

```
kubectl rollout SUBCOMMAND
```

```
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo
  
  # Restart a deployment
  kubectl rollout restart deployment/abc
  
  # Restart deployments with the 'app=nginx' label
  kubectl rollout restart deployment --selector=app=nginx
```","Manage the rollout of one or many resources.
        
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

```
kubectl rollout SUBCOMMAND
```

```
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo
  
  # Restart a deployment
  kubectl rollout restart deployment/abc
  
  # Restart deployments with the 'app=nginx' label
  kubectl rollout restart deployment --selector=app=nginx
```","Manage the rollout of one or many resources.
        
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

```
kubectl rollout SUBCOMMAND
```

```
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo
  
  # Restart a deployment
  kubectl rollout restart deployment/abc
  
  # Restart deployments with the 'app=nginx' label
  kubectl rollout restart deployment --selector=app=nginx
```","Manage the rollout of one or many resources.
        
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

```
kubectl rollout SUBCOMMAND
```

```
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo
  
  # Restart a deployment
  kubectl rollout restart deployment/abc
  
  # Restart deployments with the 'app=nginx' label
  kubectl rollout restart deployment --selector=app=nginx
```","Manage the rollout of one or many resources.
        
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

```
kubectl rollout SUBCOMMAND
```

```
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo
  
  # Restart a deployment
  kubectl rollout restart deployment/abc
  
  # Restart deployments with the 'app=nginx' label
  kubectl rollout restart deployment --selector=app=nginx
```","View previous rollout revisions and configurations.

```
kubectl rollout history (TYPE NAME | TYPE/NAME) [flags]
```

```
  # View the rollout history of a deployment
  kubectl rollout history deployment/abc
  
  # View the details of daemonset revision 3
  kubectl rollout history daemonset/abc --revision=3
```","View previous rollout revisions and configurations.

```
kubectl rollout history (TYPE NAME | TYPE/NAME) [flags]
```

```
  # View the rollout history of a deployment
  kubectl rollout history deployment/abc
  
  # View the details of daemonset revision 3
  kubectl rollout history daemonset/abc --revision=3
```","View previous rollout revisions and configurations.

```
kubectl rollout history (TYPE NAME | TYPE/NAME) [flags]
```

```
  # View the rollout history of a deployment
  kubectl rollout history deployment/abc
  
  # View the details of daemonset revision 3
  kubectl rollout history daemonset/abc --revision=3
```","Set the latest last-applied-configuration annotations by setting it to match the contents of a file. This results in the last-applied-configuration being updated as though 'kubectl apply -f&lt;file&gt; ' was run, without updating any other parts of the object.

```
kubectl apply set-last-applied -f FILENAME
```

```
  # Set the last-applied-configuration of a resource to match the contents of a file
  kubectl apply set-last-applied -f deploy.yaml
  
  # Execute set-last-applied against each configuration file in a directory
  kubectl apply set-last-applied -f path/
  
  # Set the last-applied-configuration of a resource to match the contents of a file; will create the annotation if it does not already exist
  kubectl apply set-last-applied -f deploy.yaml --create-annotation=true
```","Set the latest last-applied-configuration annotations by setting it to match the contents of a file. This results in the last-applied-configuration being updated as though 'kubectl apply -f&lt;file&gt; ' was run, without updating any other parts of the object.

```
kubectl apply set-last-applied -f FILENAME
```

```
  # Set the last-applied-configuration of a resource to match the contents of a file
  kubectl apply set-last-applied -f deploy.yaml
  
  # Execute set-last-applied against each configuration file in a directory
  kubectl apply set-last-applied -f path/
  
  # Set the last-applied-configuration of a resource to match the contents of a file; will create the annotation if it does not already exist
  kubectl apply set-last-applied -f deploy.yaml --create-annotation=true
```","Set the latest last-applied-configuration annotations by setting it to match the contents of a file. This results in the last-applied-configuration being updated as though 'kubectl apply -f&lt;file&gt; ' was run, without updating any other parts of the object.

```
kubectl apply set-last-applied -f FILENAME
```

```
  # Set the last-applied-configuration of a resource to match the contents of a file
  kubectl apply set-last-applied -f deploy.yaml
  
  # Execute set-last-applied against each configuration file in a directory
  kubectl apply set-last-applied -f path/
  
  # Set the last-applied-configuration of a resource to match the contents of a file; will create the annotation if it does not already exist
  kubectl apply set-last-applied -f deploy.yaml --create-annotation=true
```","Edit the latest last-applied-configuration annotations of resources from the default editor.

 The edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl apply edit-last-applied (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the last-applied-configuration annotations by type/name in YAML
  kubectl apply edit-last-applied deployment/nginx
  
  # Edit the last-applied-configuration annotations by file in JSON
  kubectl apply edit-last-applied -f deploy.yaml -o json
```","Edit the latest last-applied-configuration annotations of resources from the default editor.

 The edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl apply edit-last-applied (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the last-applied-configuration annotations by type/name in YAML
  kubectl apply edit-last-applied deployment/nginx
  
  # Edit the last-applied-configuration annotations by file in JSON
  kubectl apply edit-last-applied -f deploy.yaml -o json
```","Edit the latest last-applied-configuration annotations of resources from the default editor.

 The edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl apply edit-last-applied (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the last-applied-configuration annotations by type/name in YAML
  kubectl apply edit-last-applied deployment/nginx
  
  # Edit the last-applied-configuration annotations by file in JSON
  kubectl apply edit-last-applied -f deploy.yaml -o json
```","Edit the latest last-applied-configuration annotations of resources from the default editor.

 The edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl apply edit-last-applied (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the last-applied-configuration annotations by type/name in YAML
  kubectl apply edit-last-applied deployment/nginx
  
  # Edit the last-applied-configuration annotations by file in JSON
  kubectl apply edit-last-applied -f deploy.yaml -o json
```","Edit the latest last-applied-configuration annotations of resources from the default editor.

 The edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.

 The default format is YAML. To edit in JSON, specify ""-o json"".

 The flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.

 In the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.

```
kubectl apply edit-last-applied (RESOURCE/NAME | -f FILENAME)
```

```
  # Edit the last-applied-configuration annotations by type/name in YAML
  kubectl apply edit-last-applied deployment/nginx
  
  # Edit the last-applied-configuration annotations by file in JSON
  kubectl apply edit-last-applied -f deploy.yaml -o json
```","View the latest last-applied-configuration annotations by type/name or file.

 The default output will be printed to stdout in YAML format. You can use the -o option to change the output format.

```
kubectl apply view-last-applied (TYPE [NAME | -l label] | TYPE/NAME | -f FILENAME)
```

```
  # View the last-applied-configuration annotations by type/name in YAML
  kubectl apply view-last-applied deployment/nginx
  
  # View the last-applied-configuration annotations by file in JSON
  kubectl apply view-last-applied -f deploy.yaml -o json
```","View the latest last-applied-configuration annotations by type/name or file.

 The default output will be printed to stdout in YAML format. You can use the -o option to change the output format.

```
kubectl apply view-last-applied (TYPE [NAME | -l label] | TYPE/NAME | -f FILENAME)
```

```
  # View the last-applied-configuration annotations by type/name in YAML
  kubectl apply view-last-applied deployment/nginx
  
  # View the last-applied-configuration annotations by file in JSON
  kubectl apply view-last-applied -f deploy.yaml -o json
```","View the latest last-applied-configuration annotations by type/name or file.

 The default output will be printed to stdout in YAML format. You can use the -o option to change the output format.

```
kubectl apply view-last-applied (TYPE [NAME | -l label] | TYPE/NAME | -f FILENAME)
```

```
  # View the last-applied-configuration annotations by type/name in YAML
  kubectl apply view-last-applied deployment/nginx
  
  # View the last-applied-configuration annotations by file in JSON
  kubectl apply view-last-applied -f deploy.yaml -o json
```","View the latest last-applied-configuration annotations by type/name or file.

 The default output will be printed to stdout in YAML format. You can use the -o option to change the output format.

```
kubectl apply view-last-applied (TYPE [NAME | -l label] | TYPE/NAME | -f FILENAME)
```

```
  # View the last-applied-configuration annotations by type/name in YAML
  kubectl apply view-last-applied deployment/nginx
  
  # View the last-applied-configuration annotations by file in JSON
  kubectl apply view-last-applied -f deploy.yaml -o json
```","View the latest last-applied-configuration annotations by type/name or file.

 The default output will be printed to stdout in YAML format. You can use the -o option to change the output format.

```
kubectl apply view-last-applied (TYPE [NAME | -l label] | TYPE/NAME | -f FILENAME)
```

```
  # View the last-applied-configuration annotations by type/name in YAML
  kubectl apply view-last-applied deployment/nginx
  
  # View the last-applied-configuration annotations by file in JSON
  kubectl apply view-last-applied -f deploy.yaml -o json
```","Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.

 JSON and YAML formats are accepted.

 Alpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.

```
kubectl apply (-f FILENAME | -k DIRECTORY)
```

```
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'
  
  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  kubectl apply --prune -f manifest.yaml -l app=nginx
  
  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  kubectl apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap
```","Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.

 JSON and YAML formats are accepted.

 Alpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.

```
kubectl apply (-f FILENAME | -k DIRECTORY)
```

```
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'
  
  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  kubectl apply --prune -f manifest.yaml -l app=nginx
  
  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  kubectl apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap
```","Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.

 JSON and YAML formats are accepted.

 Alpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.

```
kubectl apply (-f FILENAME | -k DIRECTORY)
```

```
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'
  
  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  kubectl apply --prune -f manifest.yaml -l app=nginx
  
  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  kubectl apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap
```","Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.

 JSON and YAML formats are accepted.

 Alpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.

```
kubectl apply (-f FILENAME | -k DIRECTORY)
```

```
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'
  
  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  kubectl apply --prune -f manifest.yaml -l app=nginx
  
  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  kubectl apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap
```","Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.

 JSON and YAML formats are accepted.

 Alpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.

```
kubectl apply (-f FILENAME | -k DIRECTORY)
```

```
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'
  
  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  kubectl apply --prune -f manifest.yaml -l app=nginx
  
  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  kubectl apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap
```"
b021df6a,b021df6a,b021df6a,b021df6a,b021df6a,76d16796,76d16796,76d16796,0ad333e2,0ad333e2,0ad333e2,0ad333e2,6a399164,6a399164,6a399164,dc1d4e05,dc1d4e05,dc1d4e05,8e58cd87,8e58cd87,8e58cd87,8e58cd87,8e58cd87,99bc4b85,99bc4b85,99bc4b85,e21cb987,e21cb987,e21cb987,e21cb987,e21cb987,0bcfa281,0bcfa281,0bcfa281,0becb50d,0becb50d,0becb50d,0becb50d,0becb50d,3786473d,3786473d,3786473d,3786473d,3786473d,34171d17,34171d17,34171d17,34171d17,34171d17,23d21ad4,23d21ad4,23d21ad4,23d21ad4,23d21ad4,b8724da7,b8724da7,b8724da7,8e9db3d1,8e9db3d1,8e9db3d1,8e9db3d1,8e9db3d1,a941f8ad,a941f8ad,a941f8ad,5e2f5f3c,5e2f5f3c,5e2f5f3c,a226e9fb,a226e9fb,a226e9fb,f47b1769,f47b1769,f47b1769,f47b1769,f47b1769,b2ceaf1b,b2ceaf1b,b2ceaf1b,ae45514e,ae45514e,ae45514e,3d398179,3d398179,3d398179,3d398179,263d5008,263d5008,263d5008,263d5008,263d5008,bd7a9717,bd7a9717,bd7a9717,d5f30667,d5f30667,d5f30667,fecb13f1,fecb13f1,fecb13f1,fecb13f1,fecb13f1,b7144de2,b7144de2,b7144de2,0e7278cd,0e7278cd,0e7278cd,0e7278cd,0e7278cd,249694a4,249694a4,249694a4,ee043734,ee043734,ee043734,f03bde11,f03bde11,f03bde11,5f8b6080,5f8b6080,5f8b6080,63c0a62f,63c0a62f,63c0a62f,b06f5d40,b06f5d40,b06f5d40,b06f5d40,b06f5d40,1e6947ac,1e6947ac,1e6947ac,1e6947ac,1e6947ac,b3a7464b,b3a7464b,b3a7464b,b3a7464b,b3a7464b,4c9a5c67,4c9a5c67,4c9a5c67,4c9a5c67,4c9a5c67,7f69215e,7f69215e,7f69215e,7f69215e,7f69215e,1c93756c,1c93756c,1c93756c,1c93756c,1c93756c,c4e93a4e,c4e93a4e,c4e93a4e,9d828e3a,9d828e3a,9d828e3a,9d828e3a,bc2fdfa2,bc2fdfa2,bc2fdfa2,bc2fdfa2,bc2fdfa2,6a3918c8,6a3918c8,6a3918c8,56e11537,56e11537,56e11537,56e11537,56e11537,be53a054,be53a054,be53a054,be53a054,be53a054,f98c6e23,f98c6e23,f98c6e23,f98c6e23,f98c6e23,a68d46fa,a68d46fa,a68d46fa,a68d46fa,a68d46fa,f206f583,f206f583,f206f583,f206f583,f206f583,e6f4d685,e6f4d685,e6f4d685,e6f4d685,e6f4d685,e7d22294,e7d22294,e7d22294,e7d22294,e7d22294,3ea414e1,3ea414e1,3ea414e1,5b2c1651,5b2c1651,5b2c1651,d8613296,d8613296,d8613296,d8613296,d8613296,da0a2841,da0a2841,da0a2841,da0a2841,da0a2841,9aaf2d0a,9aaf2d0a,9aaf2d0a,7fd622e2,7fd622e2,7fd622e2,ea11065c,ea11065c,ea11065c,ea11065c,ea11065c,87b6269f,87b6269f,87b6269f,6f49e891,6f49e891,6f49e891,7641016e,7641016e,7641016e,7641016e,7641016e,63912595,63912595,63912595,63912595,63912595,694e8d1f,694e8d1f,694e8d1f,694e8d1f,694e8d1f,a83c0397,a83c0397,a83c0397,63e6413d,63e6413d,63e6413d,a2c88367,a2c88367,a2c88367,6c3a6944,6c3a6944,6c3a6944,f2065865,f2065865,f2065865,f2065865,00d3fcb5,00d3fcb5,00d3fcb5,69653f1f,69653f1f,69653f1f,69653f1f,69653f1f,045efe41,045efe41,045efe41,045efe41,045efe41,0d4eb61d,0d4eb61d,0d4eb61d,16492b66,16492b66,16492b66,1472974b,1472974b,1472974b,1472974b,1472974b,6833d8d1,6833d8d1,6833d8d1,6833d8d1,6833d8d1,0e37f131,0e37f131,0e37f131,0e37f131,0e37f131,49ee3087,49ee3087,49ee3087,49ee3087,a900e810,a900e810,a900e810,b3ba0fe9,b3ba0fe9,b3ba0fe9,b3ba0fe9,b3ba0fe9,70b67654,70b67654,70b67654,70b67654,70b67654,7d3f4658,7d3f4658,7d3f4658,7d3f4658,7d3f4658,4e4fa3af,4e4fa3af,4e4fa3af,4e4fa3af,4e4fa3af,3f4dbf8b,3f4dbf8b,3f4dbf8b,3f4dbf8b,9bbf3737,9bbf3737,9bbf3737,9bbf3737,9bbf3737,6df90e80,6df90e80,6df90e80,6df90e80,e1e3ce5e,e1e3ce5e,e1e3ce5e,82f0dea4,82f0dea4,82f0dea4,82f0dea4,82f0dea4,34c54dce,34c54dce,34c54dce,fd0bc66f,fd0bc66f,fd0bc66f,fd0bc66f,fd0bc66f,3cd533ca,3cd533ca,3cd533ca,3cd533ca,3cd533ca,fd3f34cc,fd3f34cc,fd3f34cc,6dd9ec20,6dd9ec20,6dd9ec20,6dd9ec20,6dd9ec20,49703179,49703179,49703179,49703179,49703179,42d83b79,42d83b79,42d83b79,42d83b79,42d83b79,5e80cac9,5e80cac9,5e80cac9,0794e996,0794e996,0794e996,0794e996,0794e996,76c3e002,76c3e002,76c3e002,d98b2083,d98b2083,d98b2083,d98b2083,d98b2083,f853338c,f853338c,f853338c,ea355214,ea355214,ea355214,eb0f48a1,eb0f48a1,eb0f48a1,31d109d1,31d109d1,31d109d1,31d109d1,31d109d1,f1f911cd,f1f911cd,f1f911cd,8db7df66,8db7df66,8db7df66,8db7df66,8db7df66,0ed59e46,0ed59e46,0ed59e46,b2f3b4e2,b2f3b4e2,b2f3b4e2,0f13555d,0f13555d,0f13555d,b083d43e,b083d43e,b083d43e,18b35c41,18b35c41,18b35c41,f202d113,f202d113,f202d113,f202d113,0b79951d,0b79951d,0b79951d,a1b02114,a1b02114,a1b02114,a1b02114,a1b02114,e02d70c6,e02d70c6,e02d70c6,e02d70c6,5c5ec427,5c5ec427,5c5ec427,5c5ec427,5c5ec427,a4ecfc70,a4ecfc70,a4ecfc70,a4ecfc70,a4ecfc70,55f6611c,55f6611c,55f6611c,55f6611c,55f6611c,602253aa,602253aa,602253aa,602253aa,c2ba7e78,c2ba7e78,c2ba7e78,c2ba7e78,c2ba7e78,1ac90aa9,1ac90aa9,1ac90aa9,1ac90aa9,1ac90aa9,ac22e8a2,ac22e8a2,ac22e8a2,43840b13,43840b13,43840b13,85a2ca75,85a2ca75,85a2ca75,85a2ca75,85a2ca75,09a35c88,09a35c88,09a35c88,09a35c88,09a35c88,9a3ee949,9a3ee949,9a3ee949,b530205f,b530205f,b530205f,b530205f,b530205f,ef5508a7,ef5508a7,ef5508a7,ef5508a7,ef5508a7,686b0c46,686b0c46,686b0c46,686b0c46,686b0c46,fad51769,fad51769,fad51769,fad51769,3f402161,3f402161,3f402161,8a0813c7,8a0813c7,8a0813c7,8a0813c7,f8ef73dd,f8ef73dd,f8ef73dd,f8ef73dd,f8ef73dd,0719b5ff,0719b5ff,0719b5ff,0719b5ff,858ba476,858ba476,858ba476,4629cd57,4629cd57,4629cd57,c5fd214c,c5fd214c,c5fd214c,68eaabb9,68eaabb9,68eaabb9,7b3b419f,7b3b419f,7b3b419f,19ee64a7,19ee64a7,19ee64a7,5c24ffd6,5c24ffd6,5c24ffd6,cf51066f,cf51066f,cf51066f,90223ebe,90223ebe,90223ebe,90223ebe,90223ebe,f56ec2ab,f56ec2ab,f56ec2ab,f56ec2ab,f56ec2ab,81e0da1d,81e0da1d,81e0da1d,81e0da1d,81e0da1d,7d05c708,7d05c708,7d05c708,7d05c708,7d05c708,8ac45c9b,8ac45c9b,8ac45c9b,8ac45c9b,2ed9fe33,2ed9fe33,2ed9fe33,e0ac20ad,e0ac20ad,e0ac20ad,73951e7d,73951e7d,73951e7d,73951e7d,73951e7d,e67ee101,e67ee101,e67ee101,45f2864b,45f2864b,45f2864b,e02d2ae0,e02d2ae0,e02d2ae0,e02d2ae0,e02d2ae0,2768dcc5,2768dcc5,2768dcc5,0c3cd33e,0c3cd33e,0c3cd33e,fb34bfeb,fb34bfeb,fb34bfeb,fb34bfeb,6a621a1c,6a621a1c,6a621a1c,e7862e4a,e7862e4a,e7862e4a,e7862e4a,1ddf333c,1ddf333c,1ddf333c,8d89aed0,8d89aed0,8d89aed0,bf5a3ef1,bf5a3ef1,bf5a3ef1,bf5a3ef1,bf5a3ef1,dc169184,dc169184,dc169184,dc169184,03465c70,03465c70,03465c70,03465c70,cdac89d1,cdac89d1,cdac89d1,cdac89d1,cdac89d1,aecc7cc0,aecc7cc0,aecc7cc0,aecc7cc0,aecc7cc0,0f12ee5c,0f12ee5c,0f12ee5c,96890108,96890108,96890108,8789a9d4,8789a9d4,8789a9d4,8789a9d4,8789a9d4,58e490dc,58e490dc,58e490dc,58e490dc,58e490dc,19ee0bd4,19ee0bd4,19ee0bd4,ac37f03f,ac37f03f,ac37f03f,ac37f03f,ac37f03f,33c47c3e,33c47c3e,33c47c3e,33c47c3e,33c47c3e,6a49db46,6a49db46,6a49db46,4ee82c41,4ee82c41,4ee82c41,4ee82c41,3576381d,3576381d,3576381d,3576381d,3576381d,ea602a47,ea602a47,ea602a47,647cbc0a,647cbc0a,647cbc0a,647cbc0a,647cbc0a,b703bead,b703bead,b703bead,b703bead,b703bead,3b9fa335,3b9fa335,3b9fa335,3b9fa335,3b9fa335,9ce0785e,9ce0785e,9ce0785e,9ce0785e,9ce0785e,7e05cfcc,7e05cfcc,7e05cfcc,7e05cfcc,7e05cfcc,25a6e17a,25a6e17a,25a6e17a,25a6e17a,25a6e17a,53f41626,53f41626,53f41626,53f41626,53f41626,890eaa78,890eaa78,890eaa78,890eaa78,890eaa78,66b92e95,66b92e95,66b92e95,66b92e95,66b92e95,e4221b52,e4221b52,e4221b52,a0890912,a0890912,a0890912,a0890912,a0890912,6ea305f1,6ea305f1,6ea305f1,6ea305f1,6ea305f1,dd580d9f,dd580d9f,dd580d9f,dd580d9f,dd580d9f,7646bf18,7646bf18,7646bf18,7646bf18,7646bf18,950f6633,950f6633,950f6633,950f6633,950f6633,d71a7259,d71a7259,d71a7259,7b55dce5,7b55dce5,7b55dce5,7b55dce5,7b55dce5,29a13236,29a13236,29a13236,29a13236,29a13236,a7ff9e0c,a7ff9e0c,a7ff9e0c,a7ff9e0c,a7ff9e0c,8ef67423,8ef67423,8ef67423,42a027ab,42a027ab,42a027ab,e6e242de,e6e242de,e6e242de,e6e242de,e6e242de,f1163f14,f1163f14,f1163f14,f1163f14,f1163f14,cc3bf0be,cc3bf0be,cc3bf0be,cc3bf0be,cc3bf0be,9a989473,9a989473,9a989473,9a989473,9a989473,15a5b01e,15a5b01e,15a5b01e,15a5b01e,15a5b01e,d40cfb15,d40cfb15,d40cfb15,2ce03fe9,2ce03fe9,2ce03fe9,b2569f93,b2569f93,b2569f93,b2569f93,b2569f93,64214de0,64214de0,64214de0,64214de0,64214de0,9f6949e2,9f6949e2,9f6949e2,9f6949e2,9f6949e2,8b2d0bc3,8b2d0bc3,8b2d0bc3,8b2d0bc3,8b2d0bc3,809b0671,809b0671,809b0671,809b0671,809b0671,d4d4ea5a,d4d4ea5a,d4d4ea5a,d4d4ea5a,96acd4b2,96acd4b2,96acd4b2,f8788db9,f8788db9,f8788db9,f8788db9,f8788db9,53dea42b,53dea42b,53dea42b,53dea42b,53dea42b,97e3c70d,97e3c70d,97e3c70d,97e3c70d,97e3c70d,3eb43484,3eb43484,3eb43484,3eb43484,3eb43484,24d99e73,24d99e73,24d99e73,24d99e73,24d99e73,a2652109,a2652109,a2652109,47158b98,47158b98,47158b98,47158b98,47158b98,711c3179,711c3179,711c3179,711c3179,711c3179,03f1623c,03f1623c,03f1623c,03f1623c,03f1623c,f5e66daa,f5e66daa,f5e66daa,78659d0e,78659d0e,78659d0e,28644f68,28644f68,28644f68,28644f68,28644f68,84913dcf,84913dcf,84913dcf,84913dcf,84913dcf,a197ed1e,a197ed1e,a197ed1e,a197ed1e,a197ed1e,e7d7c16b,e7d7c16b,e7d7c16b,e7d7c16b,e7d7c16b,cc9da943,cc9da943,cc9da943,850acdc7,850acdc7,850acdc7,f8072f1c,f8072f1c,f8072f1c,f8072f1c,f8072f1c,0ed2d5e8,0ed2d5e8,0ed2d5e8,0ed2d5e8,0ed2d5e8,8666ad26,8666ad26,8666ad26,8666ad26,8666ad26
What are labels used for in Kubernetes?,How are labels structured in Kubernetes?,What is the role of key/value pairs in labeling objects?,Why might users find labels meaningful and relevant?,Which Kubernetes objects can have labels attached to them?,What is the purpose of a check that runs periodically on a container in a pod?,How does the check influence the lifecycle of a container?,Where can you find more detailed information about container probes?,What is the primary purpose of a LimitRange in Kubernetes?,How does a LimitRange help manage resources within a namespace?,In what ways can a LimitRange restrict resources for containers or Pods?,Can a LimitRange affect the number of certain resource types created?,What is the purpose of the device classes defined by device drivers and cluster admins in Kubernetes?,How does Kubernetes handle allocation of devices to specific claims?,What role do hardware accelerators play in Kubernetes resource sharing among Pods?,What are some ways a platform developer can add new functionalities to their Kubernetes instance?,How might platform developers contribute to the broader Kubernetes community?,Can platform developers create extensions that are not shared publicly?,What role does a reviewer play in maintaining the quality of the codebase?,What qualifications or knowledge does a reviewer need to have?,Is a reviewer's responsibility limited to the entire codebase or specific parts?,Why might the scope of a reviewer be limited to just a part of the codebase?,How do reviewers contribute to the software development process?,What is the purpose of CustomResourceDefinitions in Kubernetes?,How do CustomResourceDefinitions help if the built-in Kubernetes functionality is insufficient?,Do CustomResourceDefinitions require building a new custom server to extend the Kubernetes API?,What does the term 'upstream' mean in the context of the Kubernetes Community?,How is the term 'upstream' used differently in GitHub or git compared to the Kubernetes Community?,Why might a community member want to move a feature upstream in Kubernetes?,"In git terminology, what is the relationship between an upstream and a downstream repository?",What does the Kubernetes Community mean by referring to other code or tools relying on the core Kubernetes codebase?,What was the purpose of the extension API mentioned in the Kubernetes documentation?,How did the extension API assist with using external managed software offerings?,What kind of services could Kubernetes clusters connect with using the extension API?,What role do admission controllers play when a request is made to the Kubernetes API server?,How do mutating admission controllers differ from validating admission controllers?,Is it possible for an admission controller to both modify and reject a request in Kubernetes?,What type of admission controller has the ability to modify objects before they are stored?,Can a validating admission controller change the objects they process in Kubernetes?,What are some examples of day-to-day operations involved in managing a Kubernetes cluster?,How can a cluster be scaled by adding resources?,What types of activities are involved in configuring cluster networking?,Why is implementing security controls important in Kubernetes cluster management?,What does managing cluster-wide observability entail in the context of Kubernetes?,What is the purpose of defining claims storage in Kubernetes?,What are the different ways storage can be accessed in Kubernetes?,How is storage reclaimed in Kubernetes once it is no longer needed?,In what Kubernetes object are the specifics of the storage described?,How is the storage claim used in relation to a volume in Kubernetes?,What are some specific ways a person can contribute to the Kubernetes project or community?,Does contributing to the Kubernetes project only involve donating code?,"Besides code contributions, what are other forms of participation in the Kubernetes community?",Can organizing events be considered a form of contribution to the Kubernetes project?,How can someone provide feedback as a contribution to the Kubernetes project?,What is an endpoint in the context of a Kubernetes Service?,How does the EndpointSlice controller function when a Service has a selector?,How can EndpointSlices be used for Services that do not have a selector?,"What defines immutable infrastructure, and why can't it be changed after deployment?",How does immutability help in identifying and mitigating security risks?,What role does automation play in enforcing immutability within an infrastructure?,How are containers an example of immutable infrastructure?,What is the relationship between immutable infrastructure and infrastructure as code?,What are the possible ways to consume a ConfigMap in Kubernetes?,How does a ConfigMap help in making applications portable?,What kind of data is stored in a ConfigMap?,What is the purpose of extensions in Kubernetes?,Why might many Kubernetes users not need to install extensions themselves?,Do most Kubernetes users need to create their own extensions?,What is the role of etcd in a Kubernetes cluster?,Why is it important to have a backup plan in place for etcd?,Where can I find more detailed information about etcd?,What is the purpose of using HostAliases in Kubernetes?,When are HostAliases applied to a Pod's hosts file?,Can HostAliases be used with hostNetwork Pods?,Where can you find more information about the HostAliases feature in Kubernetes?,Is it mandatory to specify HostAliases for a Pod?,What is the main purpose of CNI plugins in Kubernetes?,What specification do CNI plugins follow?,Where can more information about Kubernetes and CNI be found in the documentation?,What is the primary purpose of a sidecar container in Kubernetes?,How do sidecar containers differ from regular app containers in terms of their lifecycle?,Where can I find more detailed information about sidecar containers in Kubernetes?,What role do DeviceClasses play in dynamic resource allocation within a Kubernetes cluster?,Who is responsible for defining DeviceClasses in a Kubernetes environment?,How are devices claimed in a workload using DeviceClasses?,Where can one find more information about Dynamic Resource Allocation related to DeviceClasses?,What is the role of the API server in Kubernetes?,How does the Kubernetes API server scale to handle increased demand?,What is kube-apiserver and where can its main implementation be found?,How can traffic be managed when multiple instances of kube-apiserver are deployed?,What component acts as the front end for the Kubernetes control plane?,What happens to a Kubernetes volume when a container within the pod restarts?,How long does a Kubernetes volume exist in relation to the pod that contains it?,Where can one find more details about Kubernetes storage concepts?,What is the purpose of ResourceClaims in Kubernetes?,How do ResourceClaims facilitate dynamic resource allocation (DRA) for Pods?,Who can create ResourceClaims in a Kubernetes environment?,What is node-pressure eviction and how does it help manage resources on a Kubernetes node?,Which resources does the kubelet monitor to trigger node-pressure eviction?,How does node-pressure eviction differ from API-initiated eviction in Kubernetes?,What actions does the kubelet take when resource consumption reaches critical levels on a node?,Why is node-pressure eviction important for preventing resource starvation in a Kubernetes cluster?,What role does preemption play in scheduling pending Pods?,How does Kubernetes decide which Pods to evict when using preemption?,What happens if a Pod cannot initially be scheduled on any Node?,What is a container in the context of Kubernetes?,How do containers simplify application deployment across various environments?,What is the term used for applications running inside containers?,What does the process of containerization involve?,Why are containers considered lightweight and portable?,What is the minimum number of worker nodes required in a Kubernetes cluster?,What role do worker nodes play in managing application workloads?,How does the control plane contribute to fault-tolerance and high availability in a Kubernetes cluster?,What is the purpose of the aggregation layer in a Kubernetes cluster?,How can you extend your Kubernetes cluster to include additional APIs?,What is an `APIService` object used for in relation to the aggregation layer?,What does the legacy term 'hosting the' refer to in Kubernetes documentation?,How is the legacy term still being used by provisioning tools and managed services?,What role does the 'kubernetes.io/role' label play in the usage of the legacy term?,What roles does a person involved in developing Kubernetes typically undertake?,Is contributing code the only way a developer can participate in the Kubernetes community?,What does it mean to be an active participant in the Kubernetes ecosystem?,What is the purpose of compiling separate processes into a single binary in Kubernetes?,Why is it beneficial to run multiple processes as a single process in the control plane component?,How does running processes as a single binary affect the complexity of Kubernetes components?,What is the purpose of CRI-O within the Kubernetes ecosystem?,How does CRI-O enable Kubernetes to interact with container runtimes?,"Can CRI-O work with container images from remote sources, and if so, how?",What specification must the container runtimes adhere to for compatibility with CRI-O?,Why might someone choose CRI-O when deploying Kubernetes?,What are some examples of sensitive information that can be managed by Kubernetes Secrets?,How are Secret values encoded and stored by default in Kubernetes?,What are the methods a Pod can utilize to access a Secret in Kubernetes?,What is the main difference between Secrets and ConfigMaps in terms of data confidentiality?,What configuration change can be applied to enhance the security of stored Secrets in Kubernetes?,What is the purpose of the Kubernetes downward API in relation to containers?,Why might a container need information about itself without directly accessing the Kubernetes API?,What are the two methods used by Kubernetes to provide Pod and container field values to containers?,How does the downward API help applications running in containers with accessing their context within a Kubernetes cluster?,Why is it beneficial for applications in containers to access field values without acting as a Kubernetes API client?,What role does the CNCF play in relation to container orchestration projects?,How is Kubernetes related to the CNCF?,What is the main goal of the CNCF?,Under which larger organization does the CNCF operate?,What is the connection between CNCF projects and microservices architecture?,What is the role of an architect in the design of an application?,How does an architect ensure the scalability and maintainability of an application?,What are some examples of surrounding components an application must interact with?,Why is it important for an app's implementation to interact effectively with its surrounding components?,What aspects of application design does an architect focus on?,"What are the main cloud providers currently supported by kOps, and what are their support statuses?",How does kOps handle system failures within a Kubernetes cluster?,What role does DNS play in the functioning of kOps when managing Kubernetes clusters?,What are the operating systems that kOps supports for its Kubernetes installations?,In what ways can kOps set up the cloud infrastructure necessary for a Kubernetes cluster?,What are container environment variables in Kubernetes and what do they provide to running containers?,What kind of information can container environment variables include for containerized applications?,How do container environment variables relate to other cluster resources such as service endpoints in Kubernetes?,What does the term 'downstream' refer to in the context of the Kubernetes Community?,How is the term 'downstream' used differently in GitHub or git compared to the Kubernetes Community?,What does it mean for an application to adopt a new feature 'downstream' in Kubernetes?,What is the relationship between upstream and downstream repositories in git conventions?,What is the purpose of NetworkPolicies in Kubernetes?,How do NetworkPolicies define which Pods can communicate with each other?,What components are necessary for a NetworkPolicy to be effective in controlling network traffic?,What role do network plugins play in the implementation of NetworkPolicies?,Can NetworkPolicies be used to specify communication rules across different namespaces?,What is the purpose of Pod Priority in Kubernetes?,How does Pod Priority impact the scheduling of Pods in a cluster?,Why is Pod Priority considered important for workloads in production clusters?,What is the purpose of allowing a kube-apiserver to proxy resource requests to a different peer API server?,How does this feature benefit clusters with multiple API servers running different Kubernetes versions?,What must be done to activate the feature that allows the kube-apiserver to proxy requests?,Which feature gate needs to be enabled to use the kube-apiserver proxy feature?,Is the kube-apiserver proxy feature enabled by default in Kubernetes?,What does a stored instance of software contain that's necessary for running an application?,How is software packaged for storage and execution in a container within Kubernetes?,What type of information is included in the metadata of a container image?,Where can software packaged as a container be stored before it is pulled to a local system?,What kind of information does the metadata in a container image provide about the executable and its creator?,What is a disruption in Kubernetes and what is its impact on workload management?,How does Kubernetes define a voluntary disruption?,What is considered an involuntary disruption in Kubernetes?,What are some potential causes of involuntary disruptions in a Kubernetes cluster?,Where can I find more information about disruptions in Kubernetes?,What are device plugins used for in Kubernetes?,How do device plugins inform about available resources?,What are the methods to deploy a device plugin in Kubernetes?,Why might a device plugin require vendor-specific initialization?,Where can you find more detailed information on device plugins?,What is the main problem with using a simple hash-based scheme for assigning requests to queues?,How does shuffle-sharding improve insulation between low-intensity and high-intensity request flows?,"What metaphor is used to explain the shuffle-sharding technique, and how does it apply to queue selection?",Why is the choice of hand size important in the shuffle-sharding technique?,What characteristics of a request are typically used in a hash function to determine the queue index in a simple hash-based scheme?,What are the three required properties of a core object in Kubernetes?,How do tolerations affect pod scheduling on nodes?,Can a pod with a specific toleration be scheduled on a node without a matching property?,What role do tolerations play in preventing pods from being scheduled on unsuitable nodes?,Is it necessary for a pod to be scheduled on a node if its toleration matches the node's properties?,What is the purpose of a UID in Kubernetes?,How does Kubernetes ensure each object has a unique identity?,Why is it important for every object in a Kubernetes cluster to have a distinct UID?,Can UIDs help in distinguishing between similar entities over time in Kubernetes?,What role does a UID play in managing the lifecycle of objects in a Kubernetes cluster?,What are the two types of pod disruptions mentioned in Kubernetes?,Who typically initiates voluntary pod disruptions?,What are some causes of involuntary pod disruptions in Kubernetes?,What is the primary function of a HorizontalPodAutoscaler (HPA) in Kubernetes?,To what types of Kubernetes objects can HorizontalPodAutoscaler be applied?,Why might a HorizontalPodAutoscaler not be suitable for certain Kubernetes objects?,What is the difference between Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) in the context of cloud providers?,What responsibilities does a cloud provider take on when offering a managed infrastructure service?,"In a managed Kubernetes service, what components is the cloud provider responsible for overseeing?",How does the role of a cloud provider change when offering Kubernetes as a managed service compared to a managed infrastructure service?,What elements besides networking and storage might a cloud provider manage in a Platform as a Service offering?,What type of interface does the Kubernetes application use to serve functionality and store the state of the cluster?,"In what format are Kubernetes resources and records of intent stored, and how can they be modified?",What does configuring Kubernetes through the API in a declarative way allow users to do?,"Besides direct interaction, what tool can users use to interact with the Kubernetes API?",How can the core Kubernetes API be extended?,What are the main responsibilities of a cluster operator in Kubernetes?,How does a cluster operator's role differ from the Operator pattern in Kubernetes?,What tasks might a cluster operator perform to ensure a cluster remains operational?,What is the role of the container orchestration layer in Kubernetes?,What forms can the components of the orchestration layer take when running on a host?,How were the hosts running the orchestration layer components historically referred to?,What is the primary role of kube-proxy in a Kubernetes cluster?,How does kube-proxy facilitate communication to Pods within the cluster?,What mechanism does kube-proxy use if the operating system packet filtering layer is unavailable?,Does kube-proxy work with both internal and external network sessions?,Where can I find more detailed information about kube-proxy's command-line tools?,What is a JWT and why is it used in Kubernetes?,How does Kubernetes utilize JWTs for authentication?,Can JWTs be encrypted as well as digitally signed in Kubernetes?,What actions can the kubectl command line tool perform on Kubernetes objects?,How is the term 'kubectl' officially pronounced?,What does the kubectl tool use to interact with a Kubernetes cluster?,What are the different phases that a Pod can go through during its lifecycle?,How is the current state of a Pod described in Kubernetes?,Where can one find a high-level summary of a Pod's state?,Why might a Pod's state be described as 'Unknown'?,What is the significance of the PodStatus 'phase' field in Kubernetes?,What is the purpose of a CronJob in Kubernetes?,How does a CronJob determine when to execute a task?,What format does a CronJob use to specify its schedule?,In what way is a CronJob similar to a crontab file?,Where can I find additional information about how a Job functions within Kubernetes?,How are fractional numbers represented using SI suffixes in Kubernetes?,What is the representation of the number 1000 using SI suffixes?,Which units are considered power-of-10 and how are they represented?,What is the equivalent of the binary number 2048 using binary-notation suffixes?,How does the `k` in kilo differ in case compared to other SI units in Kubernetes?,What is the primary focus of an application developer in a Kubernetes cluster?,How might the scale of an application developer's focus in a Kubernetes cluster differ?,Who is responsible for writing applications that run within a Kubernetes cluster?,What is dockershim and which versions of Kubernetes include it?,When was dockershim removed from Kubernetes?,Where can I find more information about the removal of dockershim?,What is the purpose of the sysctl interface in Unix-like systems?,In what two forms does sysctl exist on Unix-like systems?,Why might some runtimes and network plugins depend on specific sysctl values?,What types of machines can a Kubernetes worker node be?,Who manages the worker nodes in a Kubernetes cluster?,What were nodes called in the early versions of Kubernetes?,What is the purpose of using a verb in Kubernetes to track changes to an object?,How does 'watch' contribute to detecting changes in Kubernetes?,What is an example of a situation where a watch would be more efficient than polling in Kubernetes?,Where can one find more information about efficient change detection in the Kubernetes API?,What is the purpose of workload management in Kubernetes?,How does the control plane maintain the correct number of Pods running?,Why is ReplicationController no longer recommended in Kubernetes?,What is a PriorityClass in Kubernetes and why is it important for scheduling Pods?,"How does a PriorityClass map a name to an integer priority, and what fields are used to specify these?","What is the range of integer values for priority, and what does a higher value indicate in terms of priority?","Is a PriorityClass a namespaced object or not, and what does that imply?",Where can the name of a PriorityClass be specified within its configuration?,What is the primary purpose of FlexVolumes in Kubernetes?,Why does implementing FlexVolume drivers require root access?,What does the Storage SIG recommend as an alternative to FlexVolumes?,What are some potential limitations associated with FlexVolumes?,Where can users find more detailed information about FlexVolume usage and development?,What purpose does ensuring a copy of something running across multiple nodes serve?,Why are system daemons like log collectors and monitoring agents deployed in this manner?,What is typically required to run on every node according to the text?,What are some contexts in which the term can have different meanings?,Why is the term described as overloaded in the documentation?,How should one approach understanding a term with multiple meanings in Kubernetes?,What are feature gates in Kubernetes and what is their purpose?,How can you enable or disable feature gates in a Kubernetes cluster?,Are feature gates specific to each Kubernetes component or are they universal across all components?,Where can you find a list of current feature gates and their functions in Kubernetes?,What does the '--feature-gates' command line flag do in the context of Kubernetes components?,What is the purpose of the operator pattern in Kubernetes?,How can Kubernetes be extended beyond its built-in controllers?,What role does a running application play in the operator pattern?,What is required for an application to be considered an example of the Operator pattern?,How does a controller perform tasks against a custom resource in Kubernetes?,What is the purpose of using the 'kubectl drain' command in a Kubernetes cluster?,How does 'kubectl drain' handle eviction requests that are temporarily rejected?,What happens to the pods on a node when 'kubectl drain' is executed?,What does it mean for a node to be marked as going out of service in Kubernetes?,"Is there a timeout for the 'kubectl drain' command, and what happens when it is reached?",What is the format of a URL that refers to a Kubernetes object?,Can two objects of the same kind have the same name simultaneously?,What happens if you delete an object with a specific name?,Can you create a new object with the same name as a deleted object?,What is the primary purpose of EndpointSlices in Kubernetes?,Can EndpointSlices be manually configured even if they don't have specified selectors?,How do EndpointSlices relate to the IP addresses of Pods?,What is the purpose of namespaces in a Kubernetes cluster?,How does namespace-based scoping affect resource names in Kubernetes?,Which types of resources does namespace-based scoping apply to in Kubernetes?,Can you use namespaces to organize cluster-wide resources in Kubernetes?,Why do resource names need to be unique within a namespace?,What are ResourceSlices and what role do they play in dynamic resource allocation within a Kubernetes cluster?,How does Kubernetes utilize ResourceSlices to allocate resources to a ResourceClaim?,What happens when a is created in a Kubernetes cluster in terms of resource allocation and scheduling?,Who or what is responsible for creating and managing ResourceSlices in a Kubernetes cluster?,How do ResourceSlices contribute to the process of scheduling a Pod onto a node?,What role do Helm charts play in managing Kubernetes applications?,How does a single Helm chart vary in terms of deployment complexity?,What are some examples of applications that can be deployed using a Helm chart?,What is a practical use of charts in the Kubernetes ecosystem?,How does Helm simplify the sharing of Kubernetes applications?,What factors determine the QoS Class assigned to a Pod in Kubernetes?,At what point is the QoS Class of a Pod determined?,What are the different QoS classes that Kubernetes can assign to a Pod?,How does Kubernetes use QoS classes in managing Pods?,What potential actions are influenced by a Pod's QoS Class in a Kubernetes cluster?,What is the purpose of a PodTemplate in Kubernetes?,How do workload management controllers use Pod templates?,In what situation would you have multiple Pods based on the same template?,Why might it be unnecessary to create a PodTemplate object directly?,What is the primary function of controllers in Kubernetes?,How do controllers in Kubernetes monitor the state of the cluster?,Which components are examples of controllers that run inside the control plane in Kubernetes?,What is the relationship between the desired state and the current state in the context of Kubernetes controllers?,Why are control loops crucial to the operations of Kubernetes?,What fundamental hardware facilities do computers provide as resources in Kubernetes?,How does Kubernetes abstract resources for allocation to workloads?,What does Kubernetes utilize to manage resource consumption on nodes?,What is the purpose of dynamic resource allocation in Kubernetes?,What is the main purpose of the tool mentioned in the text?,What can kubeadm install in a Kubernetes setup?,How does the tool contribute to the security of a Kubernetes cluster?,What role does Istio play in managing traffic and integrating microservices?,How does Istio's position in the network affect application code?,What does the term 'service mesh' refer to in the context of Istio?,How does Istio's control plane interact with different cluster management platforms like Kubernetes?,What are the primary functions Istio provides in terms of policy enforcement and telemetry data aggregation?,What operating systems can containerd run on as a daemon?,What are some of the main functions of containerd?,How does containerd handle container images?,What is the purpose of the agent that runs on each node in the Kubernetes cluster?,How does the kubelet ensure that the containers described in PodSpecs remain healthy?,Through what means does the kubelet receive the PodSpecs?,Does the kubelet manage containers that were not initiated by Kubernetes?,What is the consequence if a container was not created by Kubernetes in terms of management by the kubelet?,What types of cluster resources does Kubernetes garbage collection clean up?,How does Kubernetes handle unused containers and images during garbage collection?,What happens to failed Pods under the Kubernetes garbage collection process?,How does Kubernetes manage completed Jobs through garbage collection?,What role does the concept of ownership play in Kubernetes garbage collection?,What advantage does dynamic provisioning offer to cluster administrators regarding storage management?,How does dynamic provisioning of storage work in Kubernetes?,What is the role of the API object in dynamic volume provisioning?,What is the primary function of cAdvisor in Kubernetes?,How does cAdvisor collect and manage data for each container?,What specific types of information does cAdvisor keep track of for containers?,Does cAdvisor provide data on a machine-wide level or only at the container level?,What kind of historical data does cAdvisor maintain for containers?,What role do objects play in the Kubernetes API system?,How does a Kubernetes object reflect a user's desired state for a cluster?,What does Kubernetes do to maintain the state represented by an object after it's created?,In what way does creating a Kubernetes object communicate with the system?,Why is a Kubernetes object described as a 'record of intent'?,What is the purpose of adding an ephemeral container to a Pod?,Why are ephemeral containers not suitable for running part of the main workload?,What limitations do ephemeral containers have in terms of scheduling and guarantees?,Under what circumstances might one use an ephemeral container in Kubernetes?,Are there any Kubernetes environments where ephemeral containers are not supported?,What is the main function of a Volume Plugin in Kubernetes?,What are the two types of volume plugins mentioned in the text?,How do in tree volume plugins differ from out of tree plugins in terms of development and release cycle?,What is the purpose of a ResourceClaimTemplate in Kubernetes?,How does Kubernetes use ResourceClaimTemplates in dynamic resource allocation?,What happens to a ResourceClaim object when the associated Pod terminates in Kubernetes?,"How does Kubernetes ensure that each Pod accesses a separate, similar resource using ResourceClaimTemplates?",In what scenario does Kubernetes automatically create ResourceClaim objects based on a template?,What is the purpose of a manifest in Kubernetes?,In what formats can a Kubernetes API object be specified?,How many manifests can a single YAML file contain in Kubernetes?,What role do finalizers play in the deletion process of Kubernetes objects?,How does Kubernetes indicate that an object is marked for deletion when it has finalizers?,What happens to an object in Kubernetes after the finalizers' conditions are met?,How can finalizers be used to manage resource deletion in Kubernetes?,What status code does Kubernetes return when a delete request is accepted for an object with finalizers?,What is the primary function of a cgroup in Linux?,Which resources can be managed by a cgroup?,How does a cgroup differ from individual process management in Linux?,What does an API object do in Kubernetes with regards to managing applications?,How are replicas represented and distributed in a Kubernetes cluster?,What should you consider using if your application workloads require local state?,What is the purpose of certificates within a Kubernetes cluster?,How do certificates ensure secure access to the Kubernetes API?,Why is it necessary for applications inside the cluster to have validated certificates?,What is the main purpose of Kubernetes in the context of containerized applications?,How does Kubernetes ensure that containerized applications are deployed and managed consistently across different environments?,What role do Kubernetes objects play in the management of an application's lifecycle?,How does Kubernetes handle scaling of applications to accommodate varying loads?,What are some benefits of using Kubernetes for managing microservices architectures?,What is the purpose of working groups within the Kubernetes community?,How do working groups differ from long-term organizational structures in Kubernetes?,Where can I find more details about the existing working groups and SIGs in Kubernetes?,What are logs used for in a Kubernetes cluster?,How can logs assist in debugging within a Kubernetes environment?,Why are logs important for monitoring activities in a cluster?,What types of logs can be found in a Kubernetes cluster?,How do logs contribute to understanding cluster events?,What is the purpose of a Contributor License Agreement (CLA) in the context of open source projects?,How do CLAs assist in addressing potential legal issues related to contributions?,In what way do CLAs relate to intellectual property in open source projects?,What are some components that the infrastructure layer is responsible for in Kubernetes?,How does the infrastructure layer contribute to networking in a Kubernetes environment?,In what way does the infrastructure layer enhance security within Kubernetes?,What are the three essential properties of a core object related to scheduling in Kubernetes?,How do taints and tolerations function together to manage pod scheduling on nodes?,Why are taints applied to nodes in Kubernetes?,What is meant by a 'Managed Service' in the context of software offerings?,Can you give some examples of Managed Services provided by third-party providers?,"Are Managed Services limited to certain types of software, or can they be any software offering?",What is a static pod in Kubernetes and how is it managed?,How does the API server interact with static pods in Kubernetes?,What features or functionalities are not supported by static pods?,How does a StatefulSet ensure that each Pod maintains a unique identity even if rescheduled?,What is one advantage of using a StatefulSet when dealing with persistent storage volumes?,In what way does a StatefulSet differ from a Deployment in managing Pods?,Why might StatefulSet be a preferred choice when a sticky identity for Pods is required?,What is the primary function of a ResourceQuota in a Kubernetes namespace?,How does a ResourceQuota limit the creation of resources by type within a namespace?,In what way can a ResourceQuota manage overall resource consumption in a namespace?,What are user namespaces in the context of the Linux kernel?,How do user namespaces help enhance container security?,What distinction is made between namespaces in Linux and Kubernetes with regards to user namespaces?,Why might a user choose to use user namespaces when running containers?,How do user namespaces mitigate the risks of potential container break-out attacks?,What is the main function of an init container in a Kubernetes pod?,How do init containers differ in execution order compared to regular app containers?,Do init containers stay running after the Pod has started?,Where can I find more detailed information about init containers in Kubernetes?,What is the purpose of the Gateway API in Kubernetes?,How does the Gateway API differ from other API kinds in Kubernetes?,What features make the Gateway API protocol-aware?,How does the Gateway API support the different roles?,In what aspect is the Gateway API described as extensible?,What is the purpose of events in a Kubernetes cluster?,Why should event consumers not depend on the timing or consistency of events?,How does Kubernetes auditing differ from regular events?,What is the recommended way to use events in Kubernetes?,What is the retention policy for events in Kubernetes clusters?,What is the purpose of using key-value pairs in Kubernetes annotations?,What kind of data can be included in Kubernetes annotations?,Who can access the metadata stored in Kubernetes annotations?,Are there any restrictions on the size or format of data in Kubernetes annotations?,Can characters that are usually not permitted be included in the metadata of Kubernetes annotations?,What is the role of the Eviction API in the process of API-initiated eviction?,How can you request an eviction using the kube-apiserver?,What are two factors that API-initiated evictions take into account when terminating a pod?,How does API-initiated eviction differ from node-pressure eviction?,What is the purpose of a Service in a Kubernetes cluster?,How does a Service determine which Pods to target?,What happens to the network traffic when the set of Pods changes?,What networking methods can Kubernetes Services use?,How does the Service abstraction relate to mechanisms like Ingress and Gateway?,What is the purpose of the 'securityContext' field in Kubernetes?,Can you list some specific configurations you can define in a 'securityContext'?,How does the 'PodSpec.securityContext' setting affect containers within a pod?,What are some examples of security policies that can be configured in a 'securityContext'?,Does the 'securityContext' allow for setting the user and group for running processes?,What is the main purpose of eviction in Kubernetes?,How many types of eviction are there in Kubernetes?,What are the two kinds of eviction mentioned in the Kubernetes documentation?,What is Minikube used for in the context of Kubernetes?,How does Minikube run a local Kubernetes cluster on your computer?,In what kind of environment can you use Minikube to try Kubernetes?,What are the two nested object fields that govern the configuration of a Kubernetes object?,"When configuring the 'spec' field of a Kubernetes object, what information must be provided?","How does the 'spec' field differ among various Kubernetes objects like Pods, StatefulSets, and Services?",What kind of settings are included in a Kubernetes object's 'spec' field for it to reach its desired state?,How does Kubernetes use the 'spec' field to maintain the state of an object?,What role does a cluster architect play in designing Kubernetes infrastructure?,What best practices do cluster architects focus on concerning distributed systems?,How does high availability factor into the responsibilities of a cluster architect?,In what ways do cluster architects ensure security within Kubernetes clusters?,Why might multiple Kubernetes clusters be deployed in a single infrastructure design?,What is a core function of Kubernetes in relation to container management?,Which container runtimes are compatible with Kubernetes?,What interface standard does Kubernetes rely on for container runtime implementations?,What is the difference between a Role and a ClusterRole in Kubernetes?,How does a RoleBinding differ from a ClusterRoleBinding in terms of scope and permissions?,What kinds of objects are used by RBAC to manage permissions in Kubernetes?,Can a Role be used to define permissions that apply across an entire Kubernetes cluster?,How can administrators dynamically configure access policies in Kubernetes?,What is the purpose of affinity in Kubernetes?,What are the two types of affinity mentioned in the text?,How does node affinity differ from pod-to-pod affinity?,What determines whether the affinity rules are required or preferred?,How does the scheduler use the affinity rules to decide pod placement?,What was the role of the deprecated Kubernetes API known as PodSecurityPolicy?,In which version of Kubernetes was PodSecurityPolicy removed?,What can be used as alternatives to PodSecurityPolicy after its removal in Kubernetes?,When was PodSecurityPolicy deprecated in Kubernetes?,What is Pod Security Admission in the context of Kubernetes?,What is the main purpose of setting a Pod Disruption Budget in Kubernetes?,How does a Pod Disruption Budget impact voluntary pod evictions?,Can Pod Disruption Budgets prevent involuntary disruptions to pods?,Do involuntary pod disruptions affect the limits set by a Pod Disruption Budget?,What is the main purpose of a ReplicaSet in a Kubernetes cluster?,How do workload objects utilize ReplicaSets in a Kubernetes environment?,What does a ReplicaSet aim to ensure within a cluster according to its specifications?,What is the primary role of a Pod in Kubernetes?,How many primary containers does a Pod typically run?,What additional features can sidecar containers provide in a Pod?,Who usually manages Pods within a Kubernetes environment?,What is the purpose of having multiple replicas of a pod in a Kubernetes cluster?,How does defining the number of replicas in a Deployment or ReplicaSet affect a Kubernetes cluster?,What benefits do replicas provide in terms of application availability and scalability?,In what ways do replicas contribute to the load balancing and update processes in Kubernetes?,How does Kubernetes handle a situation where the number of running instances differs from the specified number of replicas?,What does a resource represent in the Kubernetes type system?,How is an operation on objects depicted in Kubernetes resources?,What does each resource signify on the Kubernetes API server?,How is the schema for objects or operations defined in the Kubernetes API server resources?,What activities can members participate in through GitHub teams in the Kubernetes community?,What happens to a member's pull requests in terms of testing procedures?,What is expected of a member in the Kubernetes community in terms of contribution?,What is the primary function of the Kubernetes cloud controller manager?,How does the cloud controller manager benefit the development pace between cloud providers and the Kubernetes project?,What is the role of the cloud controller manager in managing components that interact with cloud platforms versus cluster-only components?,What technology does Docker Engine use to achieve operating-system-level virtualization?,Which Linux kernel features are utilized by Docker for resource isolation?,How does Docker differ from traditional virtual machines in terms of resource management and overhead?,What is a workload in the context of Kubernetes?,What are some core objects in Kubernetes related to workloads?,Can you give an example of how different parts of a workload might be distributed in Kubernetes?,What is the purpose of using CEL in Kubernetes?,How can CEL expressions be utilized with dynamic admission control in Kubernetes?,In what way can CEL be applied to dynamic resource allocation in Kubernetes?,What resources are essential for running containers within Kubernetes?,How do containers connect to a network in Kubernetes?,"What is the role of the layer that provides CPU, memory, network, and storage capacities?",What is the role of a service account when processes inside Pods access the cluster?,What happens if you do not specify a service account when creating a Pod?,What service account is automatically assigned to a Pod if none is specified during its creation?,What does a Job in Kubernetes do?,How does a Job ensure the specified number of Pods successfully complete?,What does the Job do as Pods complete their tasks?,What are Group Version Resources (GVRs) used for in Kubernetes?,How do GVRs help when APIs change?,What does the term 'resource' refer to in the context of GVRs?,Why might a GVR not refer to a specific namespace?,How do GVRs contribute to distinguishing different Kubernetes objects?,What role does a proxy server play in communication between a client and a remote service in computing?,How does kube-proxy function as part of a Kubernetes cluster?,What are the two modes kube-proxy can operate in and how do they differ in terms of resource usage?,Why might someone choose to run kube-proxy in hybrid mode if their operating system supports it?,"How does the communication process work between a client, proxy, and actual server?",What is the role of the control plane component in scheduling newly created pods in Kubernetes?,What are the factors considered when making scheduling decisions for pods in Kubernetes?,How do affinity and anti-affinity specifications influence the scheduling of pods?,In what ways does data locality impact the scheduling decisions in Kubernetes?,Why might deadlines be an important factor in the scheduling process for Kubernetes workloads?,What is the main purpose of an Ingress in Kubernetes?,What functionalities can an Ingress offer in a Kubernetes cluster?,Does an Ingress handle SSL termination for services in a Kubernetes cluster?,Can an Ingress object provide virtual hosting features for a cluster?,How does an Ingress relate to load balancing in Kubernetes?,What aspects do approvers consider for the holistic acceptance of a Kubernetes code contribution?,How does the role of an approver differ from a code reviewer in the context of Kubernetes?,What was the previous title used for someone with approver status in Kubernetes?,To what extent is an approver's authority limited within the Kubernetes codebase?,What are Kubernetes add-ons and how do they enhance a cluster's functionality?,Where can I find more information about installing add-ons in Kubernetes?,What type of content would I expect to find in the Kubernetes documentation about add-ons?,What types of devices might be attached to a Kubernetes cluster?,Why do attached devices usually require device drivers in Kubernetes?,What is an example of custom hardware that could be attached to a Kubernetes environment?,What are lifecycle hooks in Kubernetes and what purpose do they serve?,What is the PostStart hook and when does it execute?,How does the PreStop hook function and when is it triggered?,Are lifecycle hooks associated with a specific part of container management?,What is the main difference in the timing of execution between PostStart and PreStop hooks?,What actions does the kubelet take when it discovers a static pod in its configuration?,How is a mirror pod related to the Kubernetes API server once created by the kubelet?,What effect does removing a mirror pod from the API server have on the kubelet's operation?,What is the role of application containers in the context of a pod?,Why might someone use an init container in a pod configuration?,"If a pod lacks init containers, what type of containers does it contain?",What does the duration format in Kubernetes represent and where is it derived from?,"How are Kubernetes durations expressed in the API, and what is their structure?",What are the valid time units you can use when specifying a duration in Kubernetes?,"Can a Kubernetes duration consist of multiple time quantities, and how are these quantities interpreted?",Could you give an example of how a duration might be formatted in Kubernetes?,How can you modify the activation status of an API group in Kubernetes?,What is the purpose of having API groups within the Kubernetes API?,Where can you find more detailed information on API groups and versioning in Kubernetes?,What is the purpose of using selectors in Kubernetes?,How do selectors help when querying lists of resources in Kubernetes?,What role do labels play in the use of selectors for filtering resources?,What is the purpose of using CIDR in Kubernetes networking?,How does CIDR facilitate IP address allocation in Kubernetes?,Is CIDR applicable to both IPv4 and IPv6 in Kubernetes?,What role does a subnet mask play in CIDR notation within Kubernetes?,What is the primary protocol used by Kubernetes for communication with the Container Runtime?,Which interface defines the gRPC protocol used for communication between node components and the Container Runtime in Kubernetes?,Where can I find more details about node components in the Kubernetes architecture?,What is the purpose of the Container Storage Interface (CSI) in Kubernetes?,How do custom storage plugins differ from traditional plugins in Kubernetes in terms of integration?,What is a necessary step before utilizing a CSI driver from a storage provider in a Kubernetes cluster?,Where can you find more detailed information on specific CSI drivers available for use with Kubernetes?,What is the purpose of the layer mentioned in the context of Kubernetes?,What types of applications operate within this layer?,How are the applications described in the text prepared for operation in this layer?,What are the main components included in a StorageClass that help manage storage types?,How can users request a specific storage type in Kubernetes?,"What roles do 'provisioner', 'parameters', and 'reclaimPolicy' fields play in a StorageClass?",What is the main purpose of a SIG in Kubernetes?,How do SIGs maintain governance within their groups?,Where can I find a list of current SIGs and Working Groups in Kubernetes?,What interests might members of a SIG share?,Can SIGs have their own methods of contribution and communication apart from the governance guidelines?,What is the primary purpose of a PersistentVolume in a Kubernetes cluster?,How do PersistentVolumes abstract the details of storage provision and consumption?,When would you use PersistentVolumeClaims instead of directly using PersistentVolumes?,What are the differences between static provisioning and dynamic provisioning in the context of Kubernetes storage?,What command can you use to list all images running in a Kubernetes cluster across all namespaces?,How can you retrieve the names and images of all pods specifically in the 'default' namespace?,"What is the correct command to list all images in a cluster, excluding a specific image named 'registry.k8s.io/coredns:1.6.2'?","How would you display all metadata fields for pods in every namespace, regardless of their specific names?",What is the purpose of using the '-o=jsonpath' option in 'kubectl get pods'?,How would you retrieve only the name of the first pod using the jsonpath option?,"If you wanted to get the metadata name and status capacity of all pods, what would the jsonpath query look like?","In a Windows environment, how should a JSONPath template with spaces be quoted in a 'kubectl' command?",What is the difference in output when getting pod information with '-o json' compared to '-o=jsonpath'?,What are the main components of a Kubernetes cluster?,How does Kubernetes ensure that applications are running as desired?,What role do nodes play in a Kubernetes environment?,Can you explain the concept of a pod in Kubernetes?,How does Kubernetes handle scaling of applications?,What is the primary function of the kubectl command?,Where can I find more detailed information about the kubectl tool?,What is the basic syntax for using kubectl with flags?,What are the main components of a Kubernetes cluster?,How does a Kubernetes Pod differ from a Node?,What role does the API server play in a Kubernetes cluster?,What is the primary purpose of Kubernetes in managing containerized applications?,How does Kubernetes ensure high availability for applications?,Can you explain the role of a pod within Kubernetes?,What mechanisms does Kubernetes use to scale applications automatically?,How does Kubernetes handle resource allocation for different applications?,What are the basic components of a Kubernetes cluster?,How does Kubernetes manage containerized applications across a cluster?,What role does the Kubernetes API server play in the cluster architecture?,Can you explain how Kubernetes uses controllers and schedulers?,What is the purpose of a pod in Kubernetes?,What command would you use to update the service account in a cluster role binding?,How can you update a role binding to include multiple users and a group?,What does the --dry-run=client option do when updating rolebinding subjects?,What happens to an existing selector on a resource when a new selector is set?,What are the character and length requirements for setting a selector?,Which resource type currently supports setting selectors with this method?,How does specifying the --resource-version affect the update process for selectors?,"In the provided example, what purpose does the --dry-run=client option serve?",What are the possible resources where environment variables can be managed in Kubernetes?,How can you update the environment variables of a specific deployment to include a new key-value pair?,What command would you use to list all environment variables for a specific deployment?,How can environment variables be imported from a config map with a specified prefix using kubectl?,What is the command to remove an environment variable from all deployment configurations while targeting a specific container?,What command is used to update the service account of a deployment?,Which resources can have their service account updated using the given command?,How can you simulate updating a deployment's service account without sending the command to the API server?,What happens if a request is not specified for a compute resource like CPU or memory?,How can you set both limits and requests for CPU and memory on a deployment's container using kubectl?,What command would you use to remove resource requests for a deployment's containers?,How can you update resource limits for a container in a YAML file locally without applying changes to the server?,What Kubernetes command would you use to update the image of a specific container in a deployment to a new version?,How can you update the image of all containers in a specific daemonset to a specified image?,Which flag would allow you to update the container image from a file without connecting to the server?,"Can you update multiple resource types, like deployments and replication controllers, at the same time using a single command?",Is it possible to specify multiple containers and their respective images in one command when updating a resource?,What is the purpose of the 'kubectl set' command in Kubernetes?,How does the 'kubectl set SUBCOMMAND' syntax help in managing application resources?,What type of changes can be made to application resources using 'kubectl set'?,What command would you use to start a pod with a specific name and image?,How can you expose a specific port on a pod using the kubectl run command?,What is the purpose of using the --dry-run option when running a pod?,How do you set multiple environment variables when starting a pod with the kubectl run command?,What does the --overrides option achieve when starting a pod?,What command can be used to print all supported API resources on a Kubernetes server?,How can you print the supported API resources along with additional details?,Which command would you use to sort the list of supported API resources by name?,How do you display only the API resources that are namespaced?,"How can you list API resources from a specific API group, such as 'rbac.authorization.k8s.io'?",What are annotations in Kubernetes and how do they differ from labels?,What happens if I try to add an annotation that already exists and how can it be resolved?,How can you update an annotation on multiple Kubernetes resources at once?,What is the purpose of the --resource-version flag in the context of modifying annotations?,How do you remove an annotation from a Kubernetes resource?,What is the default output destination when running the 'kubectl cluster-info dump' command without specifying an output directory?,"How can you include log data for pods in the cluster information dump, and where are these logs organized?",Which flag would you use if you want to include information from all namespaces in the cluster dump?,How can you specify a particular directory location to save the cluster dump files?,"If you need to dump information from specific namespaces only, which flag allows you to do that?",What command is used to display the addresses of the control plane and services in Kubernetes?,Which label is associated with the services whose addresses are displayed by the 'kubectl cluster-info' command?,What command can be used to debug and diagnose cluster issues in Kubernetes?,"In the context of 'kubectl cluster-info', what is the purpose of using flags?",What type of information does 'kubectl cluster-info' output provide?,What is the purpose of the 'kubectl explain' command in Kubernetes?,How can you retrieve information about all fields of a resource using 'kubectl explain'?,What does the API version parameter do in the 'kubectl explain' command?,How is information about each field retrieved and formatted from the server?,What command would you use to get the explanation of a specific field within a Kubernetes resource?,How can you describe resources in Kubernetes using their name prefix or label selector?,What is the command to get a detailed description of all resources of a particular type using kubectl?,"If you wanted to describe all pods created by a specific replication controller, which command would you use?",How does kubectl handle a request if there is no exact match for the TYPE and NAME_PREFIX provided?,What command can you use to describe a resource using a configuration file in Kubernetes?,What is the default editor used for editing resources if the KUBE_EDITOR or EDITOR environment variables are not set?,How can you specify to edit a resource in JSON format instead of the default YAML?,What should you do if another editor changes a resource on the server while you are editing it?,"Can you edit multiple resources at once with the edit command, and how are changes applied in such cases?",How can you specify to use Windows line endings when editing a resource?,How can you retrieve logs from a pod that has multiple containers?,What option should be used to stream logs from a pod and continue logging even if errors occur?,How can you obtain logs from a specific pod but limit the log file size to a certain number of bytes?,What command would you use to get logs from all containers in pods that have a specific label?,How can you fetch logs from a job named 'hello'?,What does the 'kubectl create namespace' command do in Kubernetes?,How can you create a namespace called 'my-namespace' using kubectl?,What options are available for the --dry-run flag when creating a namespace?,How can you request a service account token for a specific namespace using kubectl?,What command would you use to get a token with a custom expiration time for a service account?,How can you request a token with a specific audience using the kubectl command?,"What is the purpose of binding a token to a Secret object, and how is it specified in the command?",How do you request a token bound to a Secret object with a specific UID?,What command would you use to create an ingress that directs all requests to '/path' to a specific service and specifies an Ingress Class?,How can you create an ingress that uses TLS with a specified secret and a path type of Prefix?,What is the command to create an ingress with multiple paths for the same host 'foo.com'?,How do you create an ingress with two specific annotations and a default Ingress Class?,In what way can you create a default backend for an ingress using the specified command syntax?,What command would you use to create a deployment named 'my-dep' that uses the busybox image and specifies a command to run?,How can you create a deployment with a specific number of replicas using the nginx image?,What is the command for creating a deployment that uses multiple container images?,"Is it possible to specify a port to be exposed when creating a deployment, and if so, how?",What are the basic requirements when using 'kubectl create deployment' to set up a deployment?,What is the purpose of using the command 'kubectl create service loadbalancer NAME' in Kubernetes?,How do you specify the ports when creating a LoadBalancer service using kubectl?,What is the effect of using the '--dry-run' flag when creating a LoadBalancer service?,Can you provide an example of creating a LoadBalancer service with a specific name and port forwarding?,What does the parameter '--tcp=port:targetPort' represent in the command to create a LoadBalancer service?,What is the purpose of creating an ExternalName service in Kubernetes?,How does an ExternalName service differ from a regular service in Kubernetes?,What command would you use to create an ExternalName service named 'my-ns' that points to 'bar.com'?,What do the options --dry-run=server|client|none specify when creating an ExternalName service?,What is an example of when you might use an ExternalName service in Kubernetes?,What is the purpose of a docker-registry type secret in Kubernetes?,How is a generic type secret referred to in Kubernetes terminology?,What does a tls type secret contain in Kubernetes?,What is the purpose of the '--value' option when creating a priority class?,How would you set a priority class as the global default priority?,What does the '--preemption-policy' option do when creating a priority class?,How can you specify a dry run when creating a priority class?,"Can two priority classes have the same value, and what happens if they do?",What is the purpose of a pod disruption budget in Kubernetes?,How do you specify which pods a disruption budget applies to?,What does the '--min-available' option define when creating a pod disruption budget?,What would happen if the specified number or percentage of available pods is not met according to the pod disruption budget?,How can you create a pod disruption budget for an app with a specific label?,What is the default behavior for the key and value when creating a config map from a file?,How does Kubernetes handle directory entries that are not regular files when creating a config map from a directory?,Can you specify custom keys instead of using the file basenames when creating a config map from files?,How can you create a config map using literal key-value pairs?,What command can you use to create a config map from environment variable files?,What is the general command syntax for creating a service in Kubernetes using the specified subcommand?,How can you ensure the output of a service creation is in JSON format?,What is the role of flags when using the kubectl create service command?,What is the purpose of the 'kubectl create clusterrolebinding' command in Kubernetes?,How can a user create a cluster role binding for multiple users and a group using the cluster-admin role?,What does the '--dry-run' option do in the 'kubectl create clusterrolebinding' command?,What is the default behavior for naming the key when creating a secret from a file?,How does Kubernetes handle directory entries that are not regular files when creating a secret from a directory?,"Can you specify custom keys when creating a secret from files, and if so, how?",What is the purpose of the '--dry-run' option in the 'kubectl create secret' command?,How can you create a Kubernetes secret using a combination of files and literal values?,What command would you use to set hard limits on resources like CPU and memory for a new quota named 'my-quota'?,How can you specify scopes when creating a resource quota in Kubernetes?,What is the purpose of the '--dry-run' option in the 'kubectl create quota' command?,How would you create a resource quota with a specific scope of 'BestEffort' for a  'best-effort' quota?,What are some of the resources you can set limits on when creating a resource quota in Kubernetes?,What prerequisites are needed before creating a TLS secret using a public/private key pair?,What file format should the public key certificate be encoded in when creating a TLS secret?,How do you create a TLS secret named 'tls-secret' using kubectl?,What is the purpose of the '--dry-run' option in the 'kubectl create secret' command?,What paths should be specified in the kubectl command to create a TLS secret?,What is the purpose of specifying a schedule in the cron job command?,How can you create a cron job with a specific command to execute?,What is the function of the image parameter when creating a cron job?,How often will a cron job scheduled with '*/1 * * * *' execute?,What does the '--' signify in the cron job creation command?,What is the basic command structure to create a job in Kubernetes?,How can you specify an image for the job you are creating?,What command would you use if you want to create a job based on an existing cron job?,"Can you specify a particular command for the job to execute and if so, how?",What does the `--from` flag do when creating a job?,What is the purpose of the '--tcp' flag when creating a ClusterIP service?,How can you specify a ClusterIP service to run in headless mode?,What does the '--dry-run' option do when creating a service clusterip?,"What command would you use to create a role that allows getting, watching, and listing pods in Kubernetes?",How can you specify multiple resource names for a role in Kubernetes?,How do you create a role in Kubernetes with actions specified for a subresource?,What is the primary purpose of using a role binding in Kubernetes?,How can you specify multiple users and groups when creating a role binding?,What is the difference between using --role=NAME and --clusterrole=NAME when creating a role binding?,How can you create a role binding specifically for a service account in Kubernetes?,What is the function of the --dry-run option in the role binding command?,What is the purpose of creating a Dockercfg secret in Kubernetes?,How can you authenticate to a Docker registry using the Docker command line?,What information is necessary to create a docker-registry secret using kubectl?,Is the email address mandatory when authenticating to a Docker registry or creating a Docker secret?,How can you create a Kubernetes secret from an existing .docker/config.json file?,What does the command 'kubectl create service nodeport NAME' do in a Kubernetes context?,"In the given example command, what is the significance of '--tcp=5678:8080'?",What is the purpose of using the '--dry-run' option when creating a NodePort service?,What name is given to the new NodePort service in the example provided?,How can you specify a target port when creating a NodePort service using kubectl?,What is the purpose of using 'kubectl create serviceaccount' command in Kubernetes?,How can you create a service account with a specific name in Kubernetes?,What does the '--dry-run' option do when creating a service account in Kubernetes?,Provide an example command to create a service account named 'my-service-account'.,What are the possible values for the '--dry-run' option when creating a service account?,What is the purpose of specifying multiple verbs in the cluster role creation command?,How can you create a cluster role that allows accessing specific pod resources by name?,What does specifying a NonResourceURL in the cluster role entail?,How does specifying an AggregationRule when creating a cluster role affect its functionality?,What is the significance of specifying an API Group when creating a cluster role?,What file formats are accepted for creating a resource with kubectl?,How can you create a pod using information from a file?,What command allows you to create a pod using data passed through stdin?,How can you edit a YAML file in JSON format before creating a resource?,What command should I use to display the supported API versions in Kubernetes?,In what format are the supported API versions displayed when using the appropriate command?,"Is there a JSON output format available for listing supported API versions, and if so, what is it?",How can you execute a command in a specific container within a pod?,What is the purpose of using the '-c' option with 'kubectl exec'?,How do you use 'kubectl exec' to open an interactive shell in a container?,What should you be careful about when running a command with flags using 'kubectl exec'?,How do you execute a command in a pod from a deployment or service using 'kubectl exec'?,What is the format required for a label key and value when updating a resource in Kubernetes?,How does the --overwrite flag affect the behavior when updating labels on a resource?,What happens if you use the --resource-version flag while updating labels on a resource?,Can you update all pods in a namespace with a specific label in one command? How?,How can you remove a specific label from a pod without using the --overwrite flag?,"What command would you use to list all pods with detailed information, such as the node name, in Kubernetes?",How can you specify which attributes of a fetched resource should be displayed using a template?,Which command is used to list all deployments in the 'v1' version of the 'apps' API group in JSON format?,"If you want to list resources in a specific namespace, how would you include the namespace in the command?",How can you display the phase value of a specific pod using the kubectl get command?,What are the different arguments you can specify when deleting resources in Kubernetes?,"What does the --grace-period flag do when deleting resources, and when might it be ignored?","What is the risk of force deleting a pod, especially in relation to shared storage or remote APIs?",How can you delete resources from a directory containing a kustomization.yaml file?,What is a potential consequence of deleting a resource if someone else submits an update at the same time?,What command can you use to view the resource usage of pods in Kubernetes?,"If you want to view the resource consumption of a specific pod by its name, what syntax would you use?",How can you display metrics for pods that are selected using a specific label?,What might cause a delay in the availability of resource usage metrics for newly created pods?,How do you view the resource usage metrics for all pods within a specific namespace in Kubernetes?,What is the purpose of the 'kubectl top node' command?,How can you display the resource usage for all nodes using kubectl?,What specific information can you view by using 'kubectl top node NODE_NAME'?,What is the primary function of the 'kubectl top' command in Kubernetes?,Why might the resource metrics provided by Kubernetes differ from those offered by OS tools like 'top'?,What prerequisites must be met for the 'kubectl top' command to operate correctly?,In what situations would a Kubernetes user choose to use the 'kubectl top' command?,Why is the 'kubectl top' command not suitable for high-accuracy reporting or alerting in Kubernetes environments?,What is the purpose of specifying preconditions with the scale command in Kubernetes?,How can you scale a replica set named 'foo' to 3 replicas using kubectl?,What command would you use to scale a Kubernetes resource specified in a file named 'foo.yaml'?,How can you ensure that a deployment is only scaled if its current replica count is a specific number?,"Can you scale multiple replication controllers at once and, if so, how?",What are the components of a taint in Kubernetes and how are they expressed?,What are the rules for formatting the key and value of a taint?,What taint effects are allowed in Kubernetes and what do they signify?,How can a taint be removed from a node if it matches a specific key and effect?,Can taints be applied to resources other than nodes in Kubernetes?,How do you perform a rollback to the previous deployment in Kubernetes?,"What is the command to roll back a daemonset to a specific revision, such as revision 3?",What does the --dry-run=server flag do when performing a rollback on a deployment?,What is the effect of resuming a paused resource in Kubernetes?,Which type of Kubernetes resource supports being resumed after being paused?,What is the specific kubectl command to resume a paused deployment named 'nginx'?,What is the purpose of the 'kubectl rollout restart RESOURCE' command?,How can you restart all deployments within a specific namespace using kubectl?,What command would you use to restart a specific deployment named 'nginx'?,How do you restart a daemon set using kubectl?,"How can you restart deployments that have a specific label, such as 'app=nginx'?",What is the purpose of the 'rollout status' command in Kubernetes?,How can you avoid waiting for the rollout to complete using the 'rollout status' command?,What does 'rollout status' do if a new rollout occurs while it's still monitoring the previous one?,How can you watch a specific revision of a rollout and terminate if it gets replaced by another?,What command would you use to check the rollout status of a deployment named 'nginx'?,What does it mean for a Kubernetes resource to be paused?,Which resources support the pause functionality in Kubernetes?,How can a paused resource be resumed in Kubernetes?,Will a paused deployment continue its current operations?,What is the effect of updating a paused deployment?,What are the valid resource types that can be managed using the rollout command in Kubernetes?,How can you roll back to a previous deployment using the rollout command?,What is the kubectl command to check the rollout status of a daemonset called 'foo'?,How would you restart a specific deployment called 'abc' using the rollout command?,"If you have multiple deployments labeled 'app=nginx', what is the command to restart all of them using kubectl?",What command will display the rollout history of a specific deployment in Kubernetes?,How can you view the details for a specific revision of a daemonset?,Which command is used to check the rollout history of Kubernetes resources?,What does the 'kubectl apply set-last-applied -f FILENAME' command do?,How can you update the last-applied-configuration for multiple files within a directory?,What happens if the last-applied-configuration annotation does not exist when using 'kubectl apply set-last-applied'?,What happens if an error occurs while trying to update a resource using the edit-last-applied command?,How can a user specify that the edits should be made in JSON format rather than the default YAML?,What are the steps to take if another editor changes a resource on the server while you are editing it?,What does the --windows-line-endings flag do when using the edit-last-applied command?,How can a user choose which editor to use when running the edit-last-applied command in Kubernetes?,What is the default output format for viewing last-applied-configuration annotations?,How can you change the output format when viewing last-applied-configuration annotations?,What command would you use to view the last-applied-configuration annotations for a specific deployment by name?,How do you view the last-applied-configuration annotations from a file in JSON format?,What options are available for specifying which annotations to view when using the kubectl apply view-last-applied command?,What are the file formats accepted for applying configurations to a Kubernetes resource?,What should you do before using the 'apply' command to ensure the resource is properly configured?,How can you apply a configuration stored in a specific directory that includes kustomization.yaml?,"What is the purpose of the --prune flag when applying configurations, and what caution is advised?",How can you apply a configuration that matches specific labels and delete others that do not match?
Label,Label,Label,Label,Label,Probe,Probe,Probe,LimitRange,LimitRange,LimitRange,LimitRange,Dynamic Resource Allocation,Dynamic Resource Allocation,Dynamic Resource Allocation,Platform Developer,Platform Developer,Platform Developer,Reviewer,Reviewer,Reviewer,Reviewer,Reviewer,CustomResourceDefinition,CustomResourceDefinition,CustomResourceDefinition,Upstream (disambiguation),Upstream (disambiguation),Upstream (disambiguation),Upstream (disambiguation),Upstream (disambiguation),Service Catalog,Service Catalog,Service Catalog,Admission Controller,Admission Controller,Admission Controller,Admission Controller,Admission Controller,Cluster Operations,Cluster Operations,Cluster Operations,Cluster Operations,Cluster Operations,Persistent Volume Claim,Persistent Volume Claim,Persistent Volume Claim,Persistent Volume Claim,Persistent Volume Claim,Contributor,Contributor,Contributor,Contributor,Contributor,Endpoints,Endpoints,Endpoints,Immutable Infrastructure,Immutable Infrastructure,Immutable Infrastructure,Immutable Infrastructure,Immutable Infrastructure,ConfigMap,ConfigMap,ConfigMap,Extensions,Extensions,Extensions,etcd,etcd,etcd,HostAliases,HostAliases,HostAliases,HostAliases,HostAliases,Container network interface (CNI),Container network interface (CNI),Container network interface (CNI),Sidecar Container,Sidecar Container,Sidecar Container,DeviceClass,DeviceClass,DeviceClass,DeviceClass,API server,API server,API server,API server,API server,Volume,Volume,Volume,ResourceClaim,ResourceClaim,ResourceClaim,Node-pressure eviction,Node-pressure eviction,Node-pressure eviction,Node-pressure eviction,Node-pressure eviction,Preemption,Preemption,Preemption,Container,Container,Container,Container,Container,Cluster,Cluster,Cluster,Aggregation Layer,Aggregation Layer,Aggregation Layer,Master,Master,Master,Code Contributor,Code Contributor,Code Contributor,kube-controller-manager,kube-controller-manager,kube-controller-manager,CRI-O,CRI-O,CRI-O,CRI-O,CRI-O,Secret,Secret,Secret,Secret,Secret,Downward API,Downward API,Downward API,Downward API,Downward API,Cloud Native Computing Foundation (CNCF),Cloud Native Computing Foundation (CNCF),Cloud Native Computing Foundation (CNCF),Cloud Native Computing Foundation (CNCF),Cloud Native Computing Foundation (CNCF),Application Architect,Application Architect,Application Architect,Application Architect,Application Architect,kOps (Kubernetes Operations),kOps (Kubernetes Operations),kOps (Kubernetes Operations),kOps (Kubernetes Operations),kOps (Kubernetes Operations),Container Environment Variables,Container Environment Variables,Container Environment Variables,Downstream (disambiguation),Downstream (disambiguation),Downstream (disambiguation),Downstream (disambiguation),Network Policy,Network Policy,Network Policy,Network Policy,Network Policy,Pod Priority,Pod Priority,Pod Priority,Mixed Version Proxy (MVP),Mixed Version Proxy (MVP),Mixed Version Proxy (MVP),Mixed Version Proxy (MVP),Mixed Version Proxy (MVP),Image,Image,Image,Image,Image,Disruption,Disruption,Disruption,Disruption,Disruption,Device Plugin,Device Plugin,Device Plugin,Device Plugin,Device Plugin,Shuffle-sharding,Shuffle-sharding,Shuffle-sharding,Shuffle-sharding,Shuffle-sharding,Toleration,Toleration,Toleration,Toleration,Toleration,UID,UID,UID,UID,UID,Pod Disruption,Pod Disruption,Pod Disruption,Horizontal Pod Autoscaler,Horizontal Pod Autoscaler,Horizontal Pod Autoscaler,Cloud Provider,Cloud Provider,Cloud Provider,Cloud Provider,Cloud Provider,Kubernetes API,Kubernetes API,Kubernetes API,Kubernetes API,Kubernetes API,Cluster Operator,Cluster Operator,Cluster Operator,Control Plane,Control Plane,Control Plane,kube-proxy,kube-proxy,kube-proxy,kube-proxy,kube-proxy,JSON Web Token (JWT),JSON Web Token (JWT),JSON Web Token (JWT),Kubectl,Kubectl,Kubectl,Pod Lifecycle,Pod Lifecycle,Pod Lifecycle,Pod Lifecycle,Pod Lifecycle,CronJob,CronJob,CronJob,CronJob,CronJob,Quantity,Quantity,Quantity,Quantity,Quantity,Application Developer,Application Developer,Application Developer,Dockershim,Dockershim,Dockershim,sysctl,sysctl,sysctl,Node,Node,Node,Watch,Watch,Watch,Watch,ReplicationController,ReplicationController,ReplicationController,PriorityClass,PriorityClass,PriorityClass,PriorityClass,PriorityClass,FlexVolume,FlexVolume,FlexVolume,FlexVolume,FlexVolume,DaemonSet,DaemonSet,DaemonSet,Developer (disambiguation),Developer (disambiguation),Developer (disambiguation),Feature gate,Feature gate,Feature gate,Feature gate,Feature gate,Operator pattern,Operator pattern,Operator pattern,Operator pattern,Operator pattern,Drain,Drain,Drain,Drain,Drain,Name,Name,Name,Name,EndpointSlice,EndpointSlice,EndpointSlice,Namespace,Namespace,Namespace,Namespace,Namespace,ResourceSlice,ResourceSlice,ResourceSlice,ResourceSlice,ResourceSlice,Helm Chart,Helm Chart,Helm Chart,Helm Chart,Helm Chart,QoS Class,QoS Class,QoS Class,QoS Class,QoS Class,PodTemplate,PodTemplate,PodTemplate,PodTemplate,Controller,Controller,Controller,Controller,Controller,Resource (infrastructure),Resource (infrastructure),Resource (infrastructure),Resource (infrastructure),Kubeadm,Kubeadm,Kubeadm,Istio,Istio,Istio,Istio,Istio,containerd,containerd,containerd,Kubelet,Kubelet,Kubelet,Kubelet,Kubelet,Garbage Collection,Garbage Collection,Garbage Collection,Garbage Collection,Garbage Collection,Dynamic Volume Provisioning,Dynamic Volume Provisioning,Dynamic Volume Provisioning,cAdvisor,cAdvisor,cAdvisor,cAdvisor,cAdvisor,Object,Object,Object,Object,Object,Ephemeral Container,Ephemeral Container,Ephemeral Container,Ephemeral Container,Ephemeral Container,Volume Plugin,Volume Plugin,Volume Plugin,ResourceClaimTemplate,ResourceClaimTemplate,ResourceClaimTemplate,ResourceClaimTemplate,ResourceClaimTemplate,Manifest,Manifest,Manifest,Finalizer,Finalizer,Finalizer,Finalizer,Finalizer,cgroup (control group),cgroup (control group),cgroup (control group),Deployment,Deployment,Deployment,Certificate,Certificate,Certificate,Glossary,Glossary,Glossary,Glossary,Glossary,WG (working group),WG (working group),WG (working group),Logging,Logging,Logging,Logging,Logging,CLA (Contributor License Agreement),CLA (Contributor License Agreement),CLA (Contributor License Agreement),Cluster Infrastructure,Cluster Infrastructure,Cluster Infrastructure,Taint,Taint,Taint,Managed Service,Managed Service,Managed Service,Static Pod,Static Pod,Static Pod,StatefulSet,StatefulSet,StatefulSet,StatefulSet,ResourceQuota,ResourceQuota,ResourceQuota,user namespace,user namespace,user namespace,user namespace,user namespace,Init Container,Init Container,Init Container,Init Container,Gateway API,Gateway API,Gateway API,Gateway API,Gateway API,Event,Event,Event,Event,Event,Annotation,Annotation,Annotation,Annotation,Annotation,API-initiated eviction,API-initiated eviction,API-initiated eviction,API-initiated eviction,Service,Service,Service,Service,Service,Security Context,Security Context,Security Context,Security Context,Security Context,Eviction,Eviction,Eviction,Minikube,Minikube,Minikube,Spec,Spec,Spec,Spec,Spec,Cluster Architect,Cluster Architect,Cluster Architect,Cluster Architect,Cluster Architect,Container Runtime,Container Runtime,Container Runtime,RBAC (Role-Based Access Control),RBAC (Role-Based Access Control),RBAC (Role-Based Access Control),RBAC (Role-Based Access Control),RBAC (Role-Based Access Control),Affinity,Affinity,Affinity,Affinity,Affinity,Pod Security Policy,Pod Security Policy,Pod Security Policy,Pod Security Policy,Pod Security Policy,Pod Disruption Budget,Pod Disruption Budget,Pod Disruption Budget,Pod Disruption Budget,ReplicaSet,ReplicaSet,ReplicaSet,Pod,Pod,Pod,Pod,Replica,Replica,Replica,Replica,Replica,API resource,API resource,API resource,API resource,Member,Member,Member,Cloud Controller Manager,Cloud Controller Manager,Cloud Controller Manager,Docker,Docker,Docker,Workload,Workload,Workload,Common Expression Language,Common Expression Language,Common Expression Language,Data Plane,Data Plane,Data Plane,ServiceAccount,ServiceAccount,ServiceAccount,Job,Job,Job,Group Version Resource,Group Version Resource,Group Version Resource,Group Version Resource,Group Version Resource,Proxy,Proxy,Proxy,Proxy,Proxy,kube-scheduler,kube-scheduler,kube-scheduler,kube-scheduler,kube-scheduler,Ingress,Ingress,Ingress,Ingress,Ingress,Approver,Approver,Approver,Approver,Add-ons,Add-ons,Add-ons,Device,Device,Device,Container Lifecycle Hooks,Container Lifecycle Hooks,Container Lifecycle Hooks,Container Lifecycle Hooks,Container Lifecycle Hooks,Mirror Pod,Mirror Pod,Mirror Pod,App Container,App Container,App Container,Duration,Duration,Duration,Duration,Duration,API Group,API Group,API Group,Selector,Selector,Selector,CIDR,CIDR,CIDR,CIDR,Container Runtime Interface (CRI),Container Runtime Interface (CRI),Container Runtime Interface (CRI),Container Storage Interface (CSI),Container Storage Interface (CSI),Container Storage Interface (CSI),Container Storage Interface (CSI),Applications,Applications,Applications,Storage Class,Storage Class,Storage Class,SIG (special interest group),SIG (special interest group),SIG (special interest group),SIG (special interest group),SIG (special interest group),Persistent Volume,Persistent Volume,Persistent Volume,Persistent Volume,kubectl Quick Reference,kubectl Quick Reference,kubectl Quick Reference,kubectl Quick Reference,JSONPath Support,JSONPath Support,JSONPath Support,JSONPath Support,JSONPath Support,kubectl Commands,kubectl Commands,kubectl Commands,kubectl Commands,kubectl Commands,kubectl,kubectl,kubectl,Introduction to kubectl,Introduction to kubectl,Introduction to kubectl,Kubectl user preferences (kuberc),Kubectl user preferences (kuberc),Kubectl user preferences (kuberc),Kubectl user preferences (kuberc),Kubectl user preferences (kuberc),kubectl reference,kubectl reference,kubectl reference,kubectl reference,kubectl reference,kubectl set subject,kubectl set subject,kubectl set subject,kubectl set selector,kubectl set selector,kubectl set selector,kubectl set selector,kubectl set selector,kubectl set env,kubectl set env,kubectl set env,kubectl set env,kubectl set env,kubectl set serviceaccount,kubectl set serviceaccount,kubectl set serviceaccount,kubectl set resources,kubectl set resources,kubectl set resources,kubectl set resources,kubectl set image,kubectl set image,kubectl set image,kubectl set image,kubectl set image,kubectl set,kubectl set,kubectl set,kubectl run,kubectl run,kubectl run,kubectl run,kubectl run,kubectl api-resources,kubectl api-resources,kubectl api-resources,kubectl api-resources,kubectl api-resources,kubectl annotate,kubectl annotate,kubectl annotate,kubectl annotate,kubectl annotate,kubectl cluster-info dump,kubectl cluster-info dump,kubectl cluster-info dump,kubectl cluster-info dump,kubectl cluster-info dump,kubectl cluster-info,kubectl cluster-info,kubectl cluster-info,kubectl cluster-info,kubectl cluster-info,kubectl explain,kubectl explain,kubectl explain,kubectl explain,kubectl explain,kubectl describe,kubectl describe,kubectl describe,kubectl describe,kubectl describe,kubectl edit,kubectl edit,kubectl edit,kubectl edit,kubectl edit,kubectl logs,kubectl logs,kubectl logs,kubectl logs,kubectl logs,kubectl create namespace,kubectl create namespace,kubectl create namespace,kubectl create token,kubectl create token,kubectl create token,kubectl create token,kubectl create token,kubectl create ingress,kubectl create ingress,kubectl create ingress,kubectl create ingress,kubectl create ingress,kubectl create deployment,kubectl create deployment,kubectl create deployment,kubectl create deployment,kubectl create deployment,kubectl create service loadbalancer,kubectl create service loadbalancer,kubectl create service loadbalancer,kubectl create service loadbalancer,kubectl create service loadbalancer,kubectl create service externalname,kubectl create service externalname,kubectl create service externalname,kubectl create service externalname,kubectl create service externalname,kubectl create secret,kubectl create secret,kubectl create secret,kubectl create priorityclass,kubectl create priorityclass,kubectl create priorityclass,kubectl create priorityclass,kubectl create priorityclass,kubectl create poddisruptionbudget,kubectl create poddisruptionbudget,kubectl create poddisruptionbudget,kubectl create poddisruptionbudget,kubectl create poddisruptionbudget,kubectl create configmap,kubectl create configmap,kubectl create configmap,kubectl create configmap,kubectl create configmap,kubectl create service,kubectl create service,kubectl create service,kubectl create clusterrolebinding,kubectl create clusterrolebinding,kubectl create clusterrolebinding,kubectl create secret generic,kubectl create secret generic,kubectl create secret generic,kubectl create secret generic,kubectl create secret generic,kubectl create quota,kubectl create quota,kubectl create quota,kubectl create quota,kubectl create quota,kubectl create secret tls,kubectl create secret tls,kubectl create secret tls,kubectl create secret tls,kubectl create secret tls,kubectl create cronjob,kubectl create cronjob,kubectl create cronjob,kubectl create cronjob,kubectl create cronjob,kubectl create job,kubectl create job,kubectl create job,kubectl create job,kubectl create job,kubectl create service clusterip,kubectl create service clusterip,kubectl create service clusterip,kubectl create role,kubectl create role,kubectl create role,kubectl create rolebinding,kubectl create rolebinding,kubectl create rolebinding,kubectl create rolebinding,kubectl create rolebinding,kubectl create secret docker-registry,kubectl create secret docker-registry,kubectl create secret docker-registry,kubectl create secret docker-registry,kubectl create secret docker-registry,kubectl create service nodeport,kubectl create service nodeport,kubectl create service nodeport,kubectl create service nodeport,kubectl create service nodeport,kubectl create serviceaccount,kubectl create serviceaccount,kubectl create serviceaccount,kubectl create serviceaccount,kubectl create serviceaccount,kubectl create clusterrole,kubectl create clusterrole,kubectl create clusterrole,kubectl create clusterrole,kubectl create clusterrole,kubectl create,kubectl create,kubectl create,kubectl create,kubectl api-versions,kubectl api-versions,kubectl api-versions,kubectl exec,kubectl exec,kubectl exec,kubectl exec,kubectl exec,kubectl label,kubectl label,kubectl label,kubectl label,kubectl label,kubectl get,kubectl get,kubectl get,kubectl get,kubectl get,kubectl delete,kubectl delete,kubectl delete,kubectl delete,kubectl delete,kubectl top pod,kubectl top pod,kubectl top pod,kubectl top pod,kubectl top pod,kubectl top node,kubectl top node,kubectl top node,kubectl top,kubectl top,kubectl top,kubectl top,kubectl top,kubectl scale,kubectl scale,kubectl scale,kubectl scale,kubectl scale,kubectl taint,kubectl taint,kubectl taint,kubectl taint,kubectl taint,kubectl rollout undo,kubectl rollout undo,kubectl rollout undo,kubectl rollout resume,kubectl rollout resume,kubectl rollout resume,kubectl rollout restart,kubectl rollout restart,kubectl rollout restart,kubectl rollout restart,kubectl rollout restart,kubectl rollout status,kubectl rollout status,kubectl rollout status,kubectl rollout status,kubectl rollout status,kubectl rollout pause,kubectl rollout pause,kubectl rollout pause,kubectl rollout pause,kubectl rollout pause,kubectl rollout,kubectl rollout,kubectl rollout,kubectl rollout,kubectl rollout,kubectl rollout history,kubectl rollout history,kubectl rollout history,kubectl apply set-last-applied,kubectl apply set-last-applied,kubectl apply set-last-applied,kubectl apply edit-last-applied,kubectl apply edit-last-applied,kubectl apply edit-last-applied,kubectl apply edit-last-applied,kubectl apply edit-last-applied,kubectl apply view-last-applied,kubectl apply view-last-applied,kubectl apply view-last-applied,kubectl apply view-last-applied,kubectl apply view-last-applied,kubectl apply,kubectl apply,kubectl apply,kubectl apply,kubectl apply
